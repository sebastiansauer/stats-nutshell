# Inference

```{r}
#| include: false
library(tidyverse)
library(knitr)
```

![](img/stern.png){width="5%"}




## R packages needed for this chapter

Make sure to run the following code chunks in order to start the neccessary R packages needed in this chapter.



```{r}
#| message: false
#| warning: false
library(easystats)  # make stats easy again
library(tidyverse)  # data wrangling
```




## What is it?

### Generalizing from a sample to a population

Statistical inference, according to @gelman_regression_2021, chap. 1.1, faces the challenge of *generalizing* from the particular to the general.

In more details, this amounts to generalizing from ...

1.  a sample to a population
2.  a treatment to a control group (i.e., causal inference)
3.  observed measurement to the underlying ("latent") construct of interest

::: callout-important
Statistical inference is concerned with making general claims from particular data using mathematical tools.
:::

### Population and sample

We want to have an estimate of some population value, for example the proportion of `A`.

However, all we have is a subset, a sample of the populuation. Hence, we need to *infer* from the sample to the popluation. We do so by generalizing from the sample to the population, see Figure @fig-pop-sample.

::: {#fig-pop-sample layout-ncol="2"}
![Population](img/pvoll.png){#fig-pop}

![Sample](img/psti.png){#fig-sample}

Population vs. sample (Image credit: Karsten Luebke)
:::

### What's not inference?

Consider fig. @fig-desk-inf which epitomizes the difference between *descriptive* and *inferential* statistics.

```{r desk-vs-ino-crop-plot}
#| fig-cap: "The difference between description and inference"
#| label: fig-desk-inf
#| echo: false
#| out-width: "50%"
knitr::include_graphics("img/desk_vs_inf-crop.png")
```

### When size helps

Larger samples allow for more precise estimations (ceteris paribus).

```{r plot-estimate-gif}
#| include: !expr knitr::is_html_output()
#| fig-cap: "Sample size in motion, Image credit: Karsten Luebke"
#| echo: false

knitr::include_graphics("img/Estimate.gif")
```


<!-- ![Sample size in motion, Image credit: Karsten Luebke](img/Estimate.gif) -->






### Inference in a nutshell

Let's look at a concrete example.

Here's a research question:

>   Are the auction prices (`total_pr`) of *new* Mariokart games higher than the auction prices for *used* ones (variable `cond`ition)?


Of cource, we would like to know the answer to the research question with respect to the population, not just to the sample at hand. 
The sample at hand is known without any uncertainty.
To the contrary, the exact value in the population is unknown.
We use statistics to *estimate* it.

For a quick example, first load the Mariokart data set (see @lst-mariokart)

```{r}
mariokart <- read.csv(  "https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv")
```

Now, let's do *INFERENCE STATISTICS*! Voila:


```{r}
my_inference_model <- lm(total_pr ~ cond, data = mariokart)
parameters(my_inference_model) |> print_md()
```

The model estimates the "effect" of the "used" condition of approx. 6.6 Dollar.
Used games cost on average about 6.6 Dollar *less* compared to new ones, according to this model.


The value at `95% CI` shows us the 95% confidence interval (CI) (Frequentist flavor). 
The CI is an indication of the precision with which we estimate the parameter of interest. 
Values within the CI can be dubbed "plausible" values of the parameter of interest; values outside are "implausible". 
Put shortly, the model thinks that the true effect in the population is somewhere between approx. -15 and +2 Dollar.

We also learn that zero is included in the CI of `cond`: 
That means that zero is plausible value for the effect of `cond`.
Therefore, we cannot exclude the null/zero value/hypothesis.



## What inference flavors are available?

Typically, when one hears "inference" one thinks of p-values and null hypothesis testing. 
Those procedures are examples of the school of *Frequentist statistics*.

However, there's a second flavor of statistics to be mentioned here: *Bayesian statistics*.

### Frequentist inference

Frequentism is *not* concerned about the probability of your research hypothesis.

Frequentism is all about controlling the *long-term error*. For illustration, suppose you are the CEO of a factory producing screws, and many of them. 
As the boss, you are not so much interested if a particular scree is in order (or faulty). 
Rather you are interested that the overall, 
long-term error rate of your production is low. 
One may add that your goal might not the minimize the long-term error, b
ut to control it to a certain level - it may be to expensive to produce super high quality screws. 
Some decent, but cheap screws, might be more profitable.

### Bayes inference

Bayes inference is concerned about the probability of your research hypothesis.

It simply redistributes your beliefs based on new data (evidence) you observe, 
see Figure @fig-belief-update.

```{mermaid, include=knitr::is_html_output()}
%%| label: fig-belief-update
%%| fig-cap: Bayesian belief updating
%%| echo: false
flowchart LR
  A(prior belief) --> B(new data) --> C(posterior belief)


```

In more detail, the posterior belief is formalized as the posterior probability. The Likelihood is the probability of the data given some hypothesis. 
The normalizing constant serves to give us a number between zero and one.

$$\overbrace{\Pr(\color{blue}{H}|\color{green}{D})}^\text{posterior probability} = \overbrace{Pr(\color{blue}{H})}^\text{prior} \frac{\overbrace{Pr(\color{green}{D}|\color{blue}{H})}^\text{likelihood}}{\underbrace{Pr(\color{green}{D})}_{\text{normalizing constant}}}$$

In practice, the posterior probability of your hypothesis is, 
the average of your prior and the Likelihood of your data.

![Prior-Likelihood-Posterior](img/prior-l-post.png){width="70%"}


Can you see that the posterior is some average of prior and likelihood?


Check out this [great video on Bayes Theorem by 3b1b](https://youtu.be/HZGCoVF3YvM).

## But which one should I consume?

PRO Frequentist:

-   Your supervisor and reviewers will be more familiar with it
-   The technical overhead is simpler compared to Bayes

PRO Bayes:

-   You'll probably want to have a posterior probability of your hypothesis
-   You may appear as a cool kid and an early adoptor of emering statistical methods

::: callout-tip
You'll learn that the technical setup used for doing Bayes statistics is quite similar to doing frequentist statistics. Stay tuned.
:::





## Lab

Consider your (most pressing) research question. Assess whether it is more accessible via Frequentist or via Bayesian statistics. Explain your reasoning.








## Comment from xkcd

```{r}
#| echo: false
#| fig-align: center
#| out-width: "50%"

knitr::include_graphics("img/frequentists_vs_bayesians_2x.png")
```

[Quelle](https://xkcd.com/1132/)

## p-value

The p-value has been used as the pivotal criterion to decide about whether or not a research hypothesis were to be "accepted" 
(a term forbidden in frequentist and Popperian langauge) or to be rejected. 
However, more recently, it is advised to use the p-value only as *one* indicator *among multiple*
; see @wasserstein_asas_2016

::: callout-important
The p-value is defined as the probability of obtaining the observed data (or more extreme) under the assumption of no effect.
:::

Figure @fig-pvalue visualizes the p-value.

```{r p-value-plot}
#| fig-align: "center"
#| fig-cap: "Visualization of the p-value"
#| label: fig-pvalue
#| echo: false
#| out-width: "50%"
ggplot(NULL, aes(c(-3,3))) +
  geom_area(stat = "function", fun = dnorm, fill = "grey80", xlim = c(-3, 1.960)) +
  geom_area(stat = "function", fun = dnorm, fill = "firebrick", xlim = c(1.96, 3)) + 
  theme_void() +
  labs(title = "The p-value as the (size of the) red area under the curve") +
  geom_vline(xintercept = 1.96, linetype = "dashed") +
   annotate("label", x = 1.96, y = 0.1, label = "observed effect")
```

## Some confusion remains about the p-value

![Source: from ImgFlip Meme Generator](img/6m29tz.jpeg){width=25%}



## Exercises

>    üë®‚Äçüè´ Check-out all exercises from [Datenwerk](https://datenwerk.netlify.app/) with the tag [inference](https://datenwerk.netlify.app/#category=inference).
For Bayesian inference, check out the tag [bayes](https://datenwerk.netlify.app/#category=bayes) on the same website.


For example,

- [punktschaetzer-reicht-nicht](https://datenwerk.netlify.app/posts/punktschaetzer-reicht-nicht/punktschaetzer-reicht-nicht)
- [wikipedia](https://datenwerk.netlify.app/posts/wikipedia/)
- [nullhyp-beispiel](https://datenwerk.netlify.app/posts/nullhyp-beispiel/nullhyp-beispiel)


## Case studies



- [flights-delay-simplified](https://datenwerk.netlify.app/posts/flights-delay-simplified/)


## Going further

@goodman_dirty_2008 provides an entertaining overview on typical misconceptions of the p-value [full text](https://www.ohri.ca//newsroom/seminars/SeminarUploads/1829/Suggested%20Reading%20-%20Nov%203,%202014.pdf).
@poldrack_statistical_2022 provides a fresh, 
accessible and sound introduction to statistical inference; in addition @cetinkaya-rundel_introduction_2021 is a worthwhile treatise.

