[
  {
    "objectID": "goals.html#overview",
    "href": "goals.html#overview",
    "title": "1  Goals in statistics",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nMany stories to be told. Here’s one, on the goals pursued in statistics (and related fields), see Figure Figure 1.1.\n\n\n\n\n\nflowchart LR\n  A{Goals} --> B(describe)\n  A --> C(predict)\n  A --> D(explain)\n  B --> E(distribution)\n  B --> F(assocation)\n  B --> G(extrapolation)\n  C --> H(point estimate)\n  C --> I(interval)\n  D --> J(causal inference)\n  D --> K(population)\n  D --> L(latent construct)\n\n\n\n\n\n\nFigure 1.1: A taxonomy of statistical goals\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that “goals” do not exist in the world. We make them up in our heads. Hence, they have no ontological existence, they are epistemological beasts. This entails that we are free to devise goals as we wish, provided we can convince ourselves and other souls of the utility of our creativity."
  },
  {
    "objectID": "goals.html#further-reading",
    "href": "goals.html#further-reading",
    "title": "1  Goals in statistics",
    "section": "1.2 Further reading",
    "text": "1.2 Further reading\nHernán, Hsu, and Healy (2019) distinguish:\nHernán et al. (2019) distinguish:\n\nDescription: “How can women aged 60–80 years with stroke history be partitioned in classes defined by their characteristics?”\nPrediction: “What is the probability of having a stroke next year for women with certain characteristics?”\nCausal inference: “Will starting a statin reduce, on average, the risk of stroke in women with certain characteristics?”\n\nGelman, Hill, and Vehtari (2021), chap. 1.1 proposes three “challenges” of statistical inference."
  },
  {
    "objectID": "goals.html#if-nothing-else-helps",
    "href": "goals.html#if-nothing-else-helps",
    "title": "1  Goals in statistics",
    "section": "1.3 If nothing else helps",
    "text": "1.3 If nothing else helps\nStay calm and behold the infinity.\n\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge: Cambridge University Press.\n\n\nHernán, Miguel A., John Hsu, and Brian Healy. 2019. “A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks.” Chance 32 (1): 42–49. https://doi.org/10.1080/09332480.2019.1579578."
  },
  {
    "objectID": "inference.html#what-is-it",
    "href": "inference.html#what-is-it",
    "title": "4  Inference",
    "section": "4.1 What is it?",
    "text": "4.1 What is it?\nStatistical inference, according to Gelman, Hill, and Vehtari (2021), chap. 1.1, faces the challenge of generalizing from the particular to the general.\nIn more details, this amounts to generalizing from …\n\na sample to a population\na treatment to a control group (i.e., causal inference)\nobserved measurement to the underlying (“latent”) construct of interest\n\n\n\n\n\n\n\nImportant\n\n\n\nStatistical inference is concerned with making general claims from particular data using mathematical tools."
  },
  {
    "objectID": "inference.html#population-and-sample",
    "href": "inference.html#population-and-sample",
    "title": "4  Inference",
    "section": "4.2 Population and sample",
    "text": "4.2 Population and sample\nWe want to have an estimate of some population value, for example the proportion of A.\nHowever, all we have is a subset, a sample of the populuation. Hence, we need to infer from the sample to the popluation. We do so by generalizing from the sample to the population, see Figure Figure 4.1.\n\n\n\n\n\n\n\n(a) Population\n\n\n\n\n\n\n\n(b) Sample\n\n\n\n\nFigure 4.1: Population vs. sample (Image credit: Karsten Luebke)"
  },
  {
    "objectID": "inference.html#whats-not-inference",
    "href": "inference.html#whats-not-inference",
    "title": "4  Inference",
    "section": "4.3 What’s not inference?",
    "text": "4.3 What’s not inference?\nConsider fig. Figure 4.2 which epitomizes the difference between descriptive and inferential statistics.\n\n\n\n\n\nFigure 4.2: The difference between description and inference"
  },
  {
    "objectID": "inference.html#when-size-helps",
    "href": "inference.html#when-size-helps",
    "title": "4  Inference",
    "section": "4.4 When size helps",
    "text": "4.4 When size helps\nLarger samples allow for more precise estimations (ceteris paribus).\n\n\n\nSample size in motion, Image credit: Karsten Luebke"
  },
  {
    "objectID": "inference.html#what-flavors-are-available",
    "href": "inference.html#what-flavors-are-available",
    "title": "4  Inference",
    "section": "4.5 What flavors are available?",
    "text": "4.5 What flavors are available?\nTypically, when one hears “inference” one thinks of p-values and null hypothesis testing. Those procedures are examples of the school of Frequentist statistics.\nHowever, there’s a second flavor of statistics to be mentioned here: Bayesian statistics.\n\n4.5.1 Frequentist inference\nFrequentism is not concerned about the probability of your research hypothesis.\nFrequentism is all about controling the long-term error. For illustration, suppose you are the CEO of a factory producing screws, and many of them. As the boss, you are not so much interested if a particular scree is in order (or faulty). Rather you are interested that the overall, long-term error rate of your production is low. One may add that your goal might not the minimize the long-term error, but to control it to a certain level - it may be to expensive to produce super high quality screws. Some decent, but cheap screws, might be more profitable.\n\n\n4.5.2 Bayes inference\nBayes inference is concerned about the probability of your research hypothesis.\nIt simply redestributes your beliefs based on new data (evidence) you observe, see Figure (bfig-elief-update?):\n\n\n\n\n\nflowchart LR\n  A(prior belief) --> B(new data) --> C(posterior belief)\n\n\n\n\n\n\nFigure 4.3: Bayesian belief updating\n\n\n\n\nIn more detail, the posterior belief is formalized as the posterior probability. The Likelihood is the probability of the data given some hypothesis. The normalizing constant serves to give us a number between zero and one.\n\\[\\overbrace{\\Pr(\\color{blue}{H}|\\color{green}{D})}^\\text{posterior probability} = \\overbrace{Pr(\\color{blue}{H})}^\\text{prior} \\frac{\\overbrace{Pr(\\color{green}{D}|\\color{blue}{H})}^\\text{likelihood}}{\\underbrace{Pr(\\color{green}{D})}_{\\text{normalizing constant}}}\\]\nIn practice, the posterior probability of your hypothesis is, the average of your prior and the Likelihood of your data.\n\n\n\nPrior-Likelihood-Posterior"
  },
  {
    "objectID": "inference.html#but-which-one-should-i-consume",
    "href": "inference.html#but-which-one-should-i-consume",
    "title": "4  Inference",
    "section": "4.6 But which one should I consume?",
    "text": "4.6 But which one should I consume?\nPRO Frequentist:\n\nYour supervisor and reviewers will be more familiar with it\nThe technical overhead is simpler compared to Bayes\n\nPRO Bayes:\n\nYou’ll probably want to have a posterior probability of your hypothesis\nYou may appear as a cool kid and an early adoptor of emering statistical methods\n\n\n\n\n\n\n\nTip\n\n\n\nYou’ll learn that the technical setup used for doing Bayes statistics is quite similar to doing frequentist statistics. Stay tuned."
  },
  {
    "objectID": "inference.html#comment-from-xkcd",
    "href": "inference.html#comment-from-xkcd",
    "title": "4  Inference",
    "section": "4.7 Comment from xkcd",
    "text": "4.7 Comment from xkcd\n\n\n\n\n\n\n\n\n\nQuelle"
  },
  {
    "objectID": "inference.html#p-value",
    "href": "inference.html#p-value",
    "title": "4  Inference",
    "section": "4.8 p-value",
    "text": "4.8 p-value\nThe p-value has been used as the pivotal criterion to decide about whether or not a research hypothesis were to be “accepted” (a term forbidden in frequentist and Popperian langauge) or to be rejected. However, more recently, it is advised to use the p-value only as one indicator among multiple; see Wasserstein and Lazar (2016) and Wasserstein, Schirm, and Lazar (2019).\n\n\n\n\n\n\nImportant\n\n\n\nThe p-value is defined as the probability of obtaining the observed data (or more extreme) under the assumption of no effect.\n\n\nFigure Figure 4.4 visualizes the p-value.\n\n\n\n\n\nFigure 4.4: Visualization of the p-value"
  },
  {
    "objectID": "inference.html#some-confusion-remains-about-the-p-value",
    "href": "inference.html#some-confusion-remains-about-the-p-value",
    "title": "4  Inference",
    "section": "4.9 Some confusion remains about the p-value",
    "text": "4.9 Some confusion remains about the p-value\n\n\nfrom Imgflip Meme Generator\n\nGoodman (2008) provides an entertaining overview on typical misconceptions of the p-value full text.\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge: Cambridge University Press.\n\n\nGoodman, Steven. 2008. “A Dirty Dozen: Twelve p-Value Misconceptions.” Seminars in Hematology, Interpretation of quantitative research, 45 (3): 135–40. https://doi.org/10.1053/j.seminhematol.2008.04.003.\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA’s Statement on p-Values: Context, Process, and Purpose.” The American Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108.\n\n\nWasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. 2019. “Moving to a World Beyond ‘ p</i> p < 0.05’.” The American Statistician 73 (March): 1–19. https://doi.org/10.1080/00031305.2019.1583913."
  },
  {
    "objectID": "regression1.html#whats-modelling",
    "href": "regression1.html#whats-modelling",
    "title": "5  Modelling and regression",
    "section": "5.1 What’s modelling?",
    "text": "5.1 What’s modelling?\nRead this great introduction by modelling by Russel Poldrack. Actually, the whole book is nice Poldrack (2022)."
  },
  {
    "objectID": "regression1.html#regression-as-the-umbrella-tool-for-modelling",
    "href": "regression1.html#regression-as-the-umbrella-tool-for-modelling",
    "title": "5  Modelling and regression",
    "section": "5.2 Regression as the umbrella tool for modelling",
    "text": "5.2 Regression as the umbrella tool for modelling\n\n\n\nOne regression\n\n\nAlternatively, venture into the forest of statistical tests as oultined eg here, at Uni Muenster.\nYou may want to ponder on this image of a decision tree of which test to choose, see Figure Figure 5.1.\n\n\n\nFigure 5.1: Choose your test carefully\n\n\n\n5.2.1 Common statistical tests are linear models\nAs Jonas Kristoffer Lindeløv tells us, we can formulate most statistical tests as a linear model, ie., a regression.\n\n\n\nCommon statistical tests as linear models\n\n\n\n\n5.2.2 How to find the regression line\nIn the simplest case, regression analyses can be interpreted geometrically as a line in a 2D coordinate system, see Figre Figure 5.2.\n\n\n\nFigure 5.2: Least Square Regression\n\n\nPut simple, we are looking for the line which is in the “middle of the points”. More precisely, we place the line such that the squared distances from the line to the points is minimal, see Figre Figure 5.2.\nConsider Figure Figure 5.3, from this source by Roback and Legler (2021). It visualizes not only the notorious regression line, but also sheds light on regression assumptions, particularly on the error distribution.\n\n\n\nFigure 5.3: Regression and some of its assumptions\n\n\n\n\n5.2.3 The linear model\nHere’s the canonical form of the linear model.\nConsider a model with \\(k\\) predictors:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k + \\epsilon\\]\n\n\n5.2.4 Algebraic derivation\nFor the mathematical inclined, check out this derivation of the simple case regression model. Note that the article is written in German, but your browser can effortlessly translate into English. Here’s a similar English article from StackExchange."
  },
  {
    "objectID": "regression1.html#r-packages-needed",
    "href": "regression1.html#r-packages-needed",
    "title": "5  Modelling and regression",
    "section": "5.3 R-packages needed",
    "text": "5.3 R-packages needed\nFor this chapter, the following R packages are needed.\n\nlibrary(rstanarm)\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nPlease use the `linewidth` argument instead.\n\nlibrary(tidyverse)\nlibrary(easystats)"
  },
  {
    "objectID": "regression1.html#in-all-its-glory",
    "href": "regression1.html#in-all-its-glory",
    "title": "5  Modelling and regression",
    "section": "5.4 In all its glory",
    "text": "5.4 In all its glory"
  },
  {
    "objectID": "regression1.html#first-model-one-metric-predictor",
    "href": "regression1.html#first-model-one-metric-predictor",
    "title": "5  Modelling and regression",
    "section": "5.5 First model: one metric predictor",
    "text": "5.5 First model: one metric predictor\nFirst, let’s load some data:\n\ndata(mtcars)\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\n5.5.1 Frequentist\nDefine and fit the model:\n\nlm1_freq <- lm(mpg ~ hp, data = mtcars)\n\nGet the parameter values:\n\nparameters(lm1_freq)\n\nParameter   | Coefficient |   SE |         95% CI | t(30) |      p\n------------------------------------------------------------------\n(Intercept) |       30.10 | 1.63 | [26.76, 33.44] | 18.42 | < .001\nhp          |       -0.07 | 0.01 | [-0.09, -0.05] | -6.74 | < .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nPlot the model parameters:\n\nplot(parameters(lm1_freq))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nPlease use `linewidth` instead.\n\n\n\n\n\n\n\n5.5.2 Bayesian\n\nlm1_bayes <- stan_glm(mpg ~ hp, data = mtcars)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000601 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 6.01 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.046955 seconds (Warm-up)\nChain 1:                0.046687 seconds (Sampling)\nChain 1:                0.093642 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.050986 seconds (Warm-up)\nChain 2:                0.048418 seconds (Sampling)\nChain 2:                0.099404 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.9e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.050064 seconds (Warm-up)\nChain 3:                0.052421 seconds (Sampling)\nChain 3:                0.102485 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.8e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.05014 seconds (Warm-up)\nChain 4:                0.046328 seconds (Sampling)\nChain 4:                0.096468 seconds (Total)\nChain 4: \n\n\nActually, we want to suppress some overly verbose output, using refresh = 0:\n\nlm1_bayes <- stan_glm(mpg ~ hp, data = mtcars, refresh = 0)\n\nGet the parameter values:\n\nparameters(lm1_bayes)\n\nParameter   | Median |         95% CI |   pd | % in ROPE |  Rhat |     ESS |                   Prior\n----------------------------------------------------------------------------------------------------\n(Intercept) |  30.04 | [26.90, 33.42] | 100% |        0% | 1.001 | 3055.00 | Normal (20.09 +- 15.07)\nhp          |  -0.07 | [-0.09, -0.05] | 100% |      100% | 1.001 | 3100.00 |   Normal (0.00 +- 0.22)\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n\n\nPlot the model parameters:\n\nplot(parameters(lm1_bayes))\n\n\n\n\n\n\n5.5.3 Model performance\n\nr2(lm1_freq)\n\n# R2 for Linear Regression\n       R2: 0.602\n  adj. R2: 0.589\n\n\n\nr2(lm1_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.586 (95% CI [0.389, 0.757])\n\n\n\n\n5.5.4 Model check\n\ncheck_model(lm1_freq)\n\n\n\n\n\ncheck_model(lm1_bayes)\n\n\n\n\n\n\n5.5.5 Get some predictions\n\nlm1_pred <- estimate_relation(lm1_freq)\nlm1_pred\n\nModel-based Expectation\n\nhp     | Predicted |   SE |         95% CI\n------------------------------------------\n52.00  |     26.55 | 1.18 | [24.15, 28.95]\n83.44  |     24.41 | 0.94 | [22.49, 26.32]\n114.89 |     22.26 | 0.75 | [20.72, 23.80]\n146.33 |     20.11 | 0.68 | [18.72, 21.51]\n177.78 |     17.97 | 0.75 | [16.43, 19.50]\n209.22 |     15.82 | 0.93 | [13.92, 17.73]\n240.67 |     13.68 | 1.17 | [11.29, 16.07]\n272.11 |     11.53 | 1.44 | [ 8.59, 14.48]\n303.56 |      9.39 | 1.73 | [ 5.86, 12.92]\n335.00 |      7.24 | 2.02 | [ 3.11, 11.38]\n\nVariable predicted: mpg\nPredictors modulated: hp\n\n\nMore details on the above function can be found on the respective page at the easystats site.\n\n\n5.5.6 Plot the model\n\nplot(lm1_pred)"
  },
  {
    "objectID": "regression1.html#more-of-this",
    "href": "regression1.html#more-of-this",
    "title": "5  Modelling and regression",
    "section": "5.6 More of this",
    "text": "5.6 More of this\nMore technical details for gauging model performance and model quality, can be found on the site of the R package “performance at the easystats site."
  },
  {
    "objectID": "regression1.html#bayes-members-only",
    "href": "regression1.html#bayes-members-only",
    "title": "5  Modelling and regression",
    "section": "5.7 Bayes-members only",
    "text": "5.7 Bayes-members only\nBayes statistics provide a distribution as the result of the analysis, the posterior distribution, which provides us with quite some luxury.\nAs the posterior distribution manifests itself by a number of samples, we can easily filter and manipulate this sample distribution in order to ask some interesing questions.\nSee:\n\nlm1_bayes %>% \n  as_tibble() %>% \n  head()\n\n# A tibble: 6 × 3\n  `(Intercept)`      hp sigma\n          <dbl>   <dbl> <dbl>\n1          33.2 -0.0792  4.02\n2          32.6 -0.0815  3.48\n3          29.0 -0.0621  3.55\n4          31.2 -0.0742  3.60\n5          29.8 -0.0675  4.09\n6          32.2 -0.0824  3.65\n\n\n\n5.7.1 Asking for probabilites\nWhat’s the probability that the effect of hp is negative?\n\nlm1_bayes %>% \n  as_tibble() %>% \n  count(hp < 0)\n\n# A tibble: 1 × 2\n  `hp < 0`     n\n  <lgl>    <int>\n1 TRUE      4000\n\n\nFeel free to ask similar questions!\n\n\n5.7.2 Asking for quantiles\nWith a given probability of, say 90%, how large is the effect of hp?\n\nlm1_bayes %>% \n  as_tibble() %>% \n  summarise(q_90 = quantile(hp, .9))\n\n# A tibble: 1 × 1\n     q_90\n    <dbl>\n1 -0.0553\n\n\nWhat’s the smallest 95% percent interval for the effect of hp?\n\nhdi(lm1_bayes)\n\nHighest Density Interval\n\nParameter   |        95% HDI\n----------------------------\n(Intercept) | [26.95, 33.45]\nhp          | [-0.09, -0.05]\n\n\nIn case you prefer 89% intervals (I do!):\n\nhdi(lm1_bayes, ci = .89)\n\nHighest Density Interval\n\nParameter   |        89% HDI\n----------------------------\n(Intercept) | [27.47, 32.72]\nhp          | [-0.08, -0.05]"
  },
  {
    "objectID": "regression1.html#multiple-metric-predictors",
    "href": "regression1.html#multiple-metric-predictors",
    "title": "5  Modelling and regression",
    "section": "5.8 Multiple metric predictors",
    "text": "5.8 Multiple metric predictors\nAssume we have a theory that dictates that fuel economy is a (causal) function of horse power and engine displacement.\n\nlm2_freq <- lm(mpg ~ hp + disp, data = mtcars)\nparameters(lm2_freq)\n\nParameter   | Coefficient |       SE |         95% CI | t(29) |      p\n----------------------------------------------------------------------\n(Intercept) |       30.74 |     1.33 | [28.01, 33.46] | 23.08 | < .001\nhp          |       -0.02 |     0.01 | [-0.05,  0.00] | -1.86 | 0.074 \ndisp        |       -0.03 | 7.40e-03 | [-0.05, -0.02] | -4.10 | < .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nSimilarly for Bayes inference:\n\nlm2_bayes <- stan_glm(mpg ~ hp + disp, data = mtcars)\n\nResults\n\nparameters(lm2_bayes)\n\nParameter   | Median |         95% CI |     pd | % in ROPE |  Rhat |     ESS |                   Prior\n------------------------------------------------------------------------------------------------------\n(Intercept) |  30.72 | [27.90, 33.49] |   100% |        0% | 1.000 | 4915.00 | Normal (20.09 +- 15.07)\nhp          |  -0.02 | [-0.05,  0.00] | 96.05% |      100% | 1.001 | 2046.00 |   Normal (0.00 +- 0.22)\ndisp        |  -0.03 | [-0.05, -0.02] | 99.95% |      100% | 1.000 | 1985.00 |   Normal (0.00 +- 0.12)\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n\nplot(parameters(lm2_bayes))\n\n\n\nr2(lm2_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.728 (95% CI [0.570, 0.840])\n\n\nDepending on the value of disp the prediction of mpg from hp will vary:\n\nlm2_pred <- estimate_relation(lm2_freq)\nplot(lm2_pred)"
  },
  {
    "objectID": "regression1.html#one-nominal-predictor",
    "href": "regression1.html#one-nominal-predictor",
    "title": "5  Modelling and regression",
    "section": "5.9 One nominal predictor",
    "text": "5.9 One nominal predictor\n\nmtcars2 <-\n  mtcars %>% \n  mutate(am_f = factor(am))\n\nlm3a <- lm(mpg ~ am_f, data = mtcars2)\nparameters(lm3a)\n\nParameter   | Coefficient |   SE |         95% CI | t(30) |      p\n------------------------------------------------------------------\n(Intercept) |       17.15 | 1.12 | [14.85, 19.44] | 15.25 | < .001\nam f [1]    |        7.24 | 1.76 | [ 3.64, 10.85] |  4.11 | < .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\n\nlm3a_means <- estimate_means(lm3a, at = \"am_f\")\nlm3a_means \n\nEstimated Marginal Means\n\nam_f |  Mean |   SE |         95% CI\n------------------------------------\n0    | 17.15 | 1.12 | [14.85, 19.44]\n1    | 24.39 | 1.36 | [21.62, 27.17]\n\nMarginal means estimated at am_f\n\n\n\nplot(lm3a_means)\n\n\n\n\nNote that we should have converted am to a factor variable before fitting the model. Otherwise, the plot won’t work.\nHere’s a more hand-crafted version of the last plot, se. Fig. (lm3a-means?)\n\nggplot(mtcars2) +\n  aes(x = am_f, y = mpg) +\n  geom_violin() +\n  geom_jitter(width = .1, alpha = .5) +\n  geom_pointrange(data = lm3a_means,\n                  color = \"orange\",\n                  aes(ymin = CI_low, ymax = CI_high, y = Mean)) +\n  geom_line(data = lm3a_means, aes(y = Mean, group = 1))\n\n\n\n\nMeans per level of am"
  },
  {
    "objectID": "regression1.html#one-metric-and-one-nominal-predictor",
    "href": "regression1.html#one-metric-and-one-nominal-predictor",
    "title": "5  Modelling and regression",
    "section": "5.10 One metric and one nominal predictor",
    "text": "5.10 One metric and one nominal predictor\n\nmtcars2 <-\n  mtcars %>% \n  mutate(cyl = factor(cyl))\n\nlm4 <- lm(mpg ~ hp + cyl, data = mtcars2)\nparameters(lm4)\n\nParameter   | Coefficient |   SE |          95% CI | t(28) |      p\n-------------------------------------------------------------------\n(Intercept) |       28.65 | 1.59 | [ 25.40, 31.90] | 18.04 | < .001\nhp          |       -0.02 | 0.02 | [ -0.06,  0.01] | -1.56 | 0.130 \ncyl [6]     |       -5.97 | 1.64 | [ -9.33, -2.61] | -3.64 | 0.001 \ncyl [8]     |       -8.52 | 2.33 | [-13.29, -3.76] | -3.66 | 0.001 \n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\n\nlm4_pred <- estimate_relation(lm4)\nplot(lm4_pred)"
  },
  {
    "objectID": "regression1.html#what-about-correlation",
    "href": "regression1.html#what-about-correlation",
    "title": "5  Modelling and regression",
    "section": "5.11 What about correlation?",
    "text": "5.11 What about correlation?\nCorrelation is really a close cousin to regression. In fact, regression with standardized variables amounts to correlation.\nLet’s get the correlation matrix of the variables in involved in lm4.\n\nlm4_corr <- \n  mtcars %>% \n  select(mpg, hp, disp) %>% \n  correlation()\n\nlm4_corr\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |     r |         95% CI | t(30) |         p\n--------------------------------------------------------------------\nmpg        |         hp | -0.78 | [-0.89, -0.59] | -6.74 | < .001***\nmpg        |       disp | -0.85 | [-0.92, -0.71] | -8.75 | < .001***\nhp         |       disp |  0.79 | [ 0.61,  0.89] |  7.08 | < .001***\n\np-value adjustment method: Holm (1979)\nObservations: 32\n\n\n\nplot(summary(lm4_corr))"
  },
  {
    "objectID": "regression1.html#exercises",
    "href": "regression1.html#exercises",
    "title": "5  Modelling and regression",
    "section": "5.12 Exercises",
    "text": "5.12 Exercises\n\nmtcars simple 1\nmtcars simple 2\nmtcars simple 3"
  },
  {
    "objectID": "regression1.html#lab",
    "href": "regression1.html#lab",
    "title": "5  Modelling and regression",
    "section": "5.13 Lab",
    "text": "5.13 Lab\nGet your own data, and build a simple model reflecting your research hypothesis. If you are lacking data (or hypothesis) get something close to it."
  },
  {
    "objectID": "regression1.html#further-reading",
    "href": "regression1.html#further-reading",
    "title": "5  Modelling and regression",
    "section": "5.14 Further reading",
    "text": "5.14 Further reading\nRoback and Legler (2021) provide and more than introductory account of regression while being accessible. A recent but still classic book (if this is possible) is the book by Gelman, Hill, and Vehtari (2021)."
  },
  {
    "objectID": "regression1.html#debrief",
    "href": "regression1.html#debrief",
    "title": "5  Modelling and regression",
    "section": "5.15 Debrief",
    "text": "5.15 Debrief\n\n\n\nScience. Via Giphy.\n\n\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge: Cambridge University Press.\n\n\nPoldrack, Russell. 2022. Statistical Thinking for the 21st Century. https://statsthinking21.github.io/statsthinking21-core-site/index.html.\n\n\nRoback, Paul, and Julie Legler. 2021. Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in. 1st ed. Chapman and Hall Texts in Statistical Science. Boca Raton: CRC Press."
  },
  {
    "objectID": "regression2.html#r-packages-needed",
    "href": "regression2.html#r-packages-needed",
    "title": "6  More Regression",
    "section": "6.1 R-packages needed",
    "text": "6.1 R-packages needed\nFor this chapter, the following R packages are needed.\n\nlibrary(tidyverse)\nlibrary(easystats)"
  },
  {
    "objectID": "regression2.html#multiplicative-associations",
    "href": "regression2.html#multiplicative-associations",
    "title": "6  More Regression",
    "section": "6.2 Multiplicative associations",
    "text": "6.2 Multiplicative associations\n\n6.2.1 The Log-Y model\nConsider again the linear model, in a simple form:\n\\[\\hat{y} = \\beta_0 + \\beta_1 x_1 +  \\ldots + b_kx_k +\\] Surprisingly, we can use this linear model to describe multiplicative assocations:\n\\(\\hat{y} = e^{b_0 + b_1x_1 + b_2x_2 + \\ldots + b_kx_k}\\)\n(I wrote b instead of \\(\\beta\\) just to show that both has its meaning, but are separate things.)\nExponentiate both sides to get:\n\\(log (\\hat{y}) = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_kx_k\\)\nFor simplicity, let’s drop the subscripts in the following without loss of generality and keep it short:\n\\(y = e^{x}\\), with \\(\\approx 2.71...\\)\nExponentiate both sides to get:\n\\(log(y) = x\\)\nThis association is called multiplicative, because if x increases by 1, y increased by a constant factor.\n\n\n\n\n\n\nNote\n\n\n\nThe logarithm is not defined for negative (input) values. And \\(log(0) = -\\infty\\).\n\n\nA side-effect of modelling log_y instead of y is that the distribution shape of the outcome variable changes. This can be useful in times.\n\n\n6.2.2 Exercise\n\nEffect of education on income\nEffect of log-y transformation on the distribution, an example\n\n\n\n\n\n\n\nNote\n\n\n\nThe exercises are written in German Language. Don’t fret. Browsers are able to translate websites instantaneously. Alternatively, go to sites such as Google Translate and enter the URL of the website to be translated. Also check out the webstor of your favorite browser to get an extention such as this one for Google Chrome.\n\n\n\n\n6.2.3 Visualizing Log Transformation\nCheck out this post for an example of a log-y regression visualized.\nThis post puts some more weight to the argument that a log-y transformation is useful (if you want to model multiplicative relations).\n\n\n6.2.4 Further reading\nCheck out this great essay by Kenneth Benoit on different log-variants in regression. Also Gelman, Hill, and Vehtari (2021), chapter 12 (and others), is useful."
  },
  {
    "objectID": "regression2.html#interaction",
    "href": "regression2.html#interaction",
    "title": "6  More Regression",
    "section": "6.3 Interaction",
    "text": "6.3 Interaction\n\n6.3.1 Multiple predictors, no interaction\nRegression analyses can be used with more than one predictor, see Figure Figure 6.1.\n\n\n\n\n\nflowchart LR\nX --> Y1\n\nX1 --> Y2\nX2 --> Y2\n\n\n\n\n\nFigure 6.1: One predictor (X) vs. two predictors (X1, X2)\n\n\n\n\nA different perspective is given by Figure Figure 6.2, where a 3D account of a regression is given. 3D means to input variables, and (which is always the case) one output variable.\n\n\n\n\n\n\nNote\n\n\n\nNote that the slope in linear in both axis (X1 and X2).\n\n\n\n\n\nFigure 6.2: Multiple Regression. Image source: Cfbaf, Public Domain\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf the slope for one predictor is the same for all values of the other predictor, then we say that no interaction is taking place.\n\n\nHere’s a visualization of a 3D regression plane (not line) without interaction: constant slope in one axis, see Figure ?fig-3dregr2.\n\n\n\n\n\nFigure 6.3: ?(caption)\n\n\n\n\n\n\n\nFigure 6.4: ?(caption)\n\n\n\n\n\n\n\nFigure 6.5: ?(caption)\n\n\n\n\nNote that the slope in each predictor axis equals 1, boringly. Hence the according 2D plots are boring, too, see Figure (3d-2dregr?).\nFor the sake of an example, consider this linear model:\n\\(mpg \\sim hp + disp\\)\nOr, in more regression like terms:\n\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\), where x1 is hp and x2 is disp in the mtcars dataset.\nIn R terms:\n\nlm3d <- lm(mpg ~ hp + disp, data = mtcars)\n\nThe 3D plot is shown in Figure Figure 6.6.\n\n\n\n\n\nFigure 6.6: mpg ~ hp + disp\n\n\n\n\nHere are the two corresponding 2d (1 predictor) regression models:\n\nlm1 <- lm(mpg ~ hp, data = mtcars)\nplot(estimate_relation(lm1))\n\n\n\nlm2 <- lm(mpg ~ disp, data = mtcars)\nplot(estimate_relation(lm2))\n\n\n\n\nCheckout this post for a visually slightly more appealing 3d regression plane.\n\n\n6.3.2 Interaction\nFor interaction to happen we relax the assumption that the slope of predictor 1 must be constant for all values of predictor 2.\nIn R, we specify an interaction model like this:\n\nlm3d_interact <- lm(mpg ~ hp + disp + hp:disp, data = mtcars)\n\nThe symbol hp:disp can be read as “the interaction effect of hp and disp”.\nHere’s a visual account, see Figure Figure 6.7.\n\n\n\n\n\nFigure 6.7: mpg ~ hp + disp\n\n\n\n\nCompare Figure 6.7 and Figure 6.6.\nIn Figure 6.7 you’ll see that the lines along the Y axis are not parallel anymore. Similarly, the lines along the X axis are not parallel anymore.\n\n\n\n\n\n\nImportant\n\n\n\nIf the regression lines (indicating different values of one predictor) are not parallel, we say that an interaction effect is taking place.\n\n\nHowever, the difference or change between two adjacent values (lines) is constant. This value is the size the regression effect.\n\n\n6.3.3 Interaction made simple\nIf you find that two sophisticated, consider the following simple case.\nFirst, we mutate am to be a factor variable, in order to make things simpler (without loss of generality).\n\nmtcars2 <-\n  mtcars %>% \n  mutate(am_f = factor(am))\n\nNow we use this new variable for a simple regression model:\n\nlm_interact_simple <- lm(mpg ~ disp + am_f + disp:am_f, data = mtcars2)\n\nHere’s the plot, Figure Figure 6.8.\n\nplot(estimate_relation(lm_interact_simple))\n\n\n\n\nFigure 6.8: A simple interaction model\n\n\n\n\nIn this picture, we see that the two regression lines are not parallel, and hence there is evidence of an interaction effect.\nThe interaction effect amounts to the difference in slops in Figure Figure 6.8.\nOne might be inclined to interpret Figure Figure 6.8 as an 3D image, where the one (reddish) line is in the foreground and the blueish line in the background (or vice versa, as you like). Given a 3D image (and hence 2 predictors), we are where we started further above.\nFor completeness, here are the parameters of the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(28)\np\n\n\n\n\n(Intercept)\n25.16\n1.93\n(21.21, 29.10)\n13.07\n< .001\n\n\ndisp\n-0.03\n6.22e-03\n(-0.04, -0.01)\n-4.44\n< .001\n\n\nam f (1)\n7.71\n2.50\n(2.58, 12.84)\n3.08\n0.005\n\n\ndisp * am f (1)\n-0.03\n0.01\n(-0.05, -7.99e-03)\n-2.75\n0.010\n\n\n\n\n\n\n\n6.3.4 Centering variables\nThe effect of of am_f must be interpreted when disp is zero, which does not make much sense.\nTherefore it simplifies the interpretation of regression coefficients to center all input variables, by subtrating the mean value (“demeaning” or “centering”):\n\\[x' = x - \\bar{x}\\] In R, this can be achieved e.g,. in this way:\n\nmtcars3 <- \nmtcars2 %>% \n  mutate(disp_c = disp - mean(disp))\n\n\nlm_interact_simple2 <- lm(mpg ~ disp_c + am_f + disp_c:am_f, data = mtcars3)\nparameters(lm_interact_simple2)\n\nParameter         | Coefficient |       SE |         95% CI | t(28) |      p\n----------------------------------------------------------------------------\n(Intercept)       |       18.79 |     0.76 | [17.23, 20.36] | 24.63 | < .001\ndisp c            |       -0.03 | 6.22e-03 | [-0.04, -0.01] | -4.44 | < .001\nam f [1]          |        0.45 |     1.39 | [-2.40,  3.30] |  0.32 | 0.748 \ndisp c * am f [1] |       -0.03 |     0.01 | [-0.05, -0.01] | -2.75 | 0.010 \n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation."
  },
  {
    "objectID": "regression2.html#predictor-relevance",
    "href": "regression2.html#predictor-relevance",
    "title": "6  More Regression",
    "section": "6.4 Predictor relevance",
    "text": "6.4 Predictor relevance\nGiven a model, we might want to know which predictor has the strongest association with the outcome?\nIn order to answer this question, all predictor must have the same scale. Otherwise the importance of a predictor would increase by 1000, if we multiply each of the observations’ values by the same factor. However, this multiplication should not change the relevance of a predictor.\nA simple solution is to standardize all predictors to the same scale (sd=1).\n\nmtcars4 <-\n  mtcars %>% \n  standardize(select = c(\"disp\", \"hp\", \"cyl\"))\n\nBy the way, “standardizing” centers the variable by default to a mean value of zero (by demeaning).\nSee:\n\nhead(mtcars4$disp)\n\n[1] -0.57061982 -0.57061982 -0.99018209  0.22009369  1.04308123 -0.04616698\n\nhead(mtcars$disp)\n\n[1] 160 160 108 258 360 225\n\n\nHere’s the SD:\n\nsd(mtcars4$disp)\n\n[1] 1\n\nsd(mtcars$disp)\n\n[1] 123.9387\n\n\nAnd here’s the mean value:\n\nmean(mtcars4$disp)\n\n[1] -9.084937e-17\n\nmean(mtcars$disp)\n\n[1] 230.7219\n\n\nNow we are in a position to decide which predictor is more important:\n\nm <- lm(mpg ~ disp + hp + cyl, data = mtcars4)\nparameters(m)\n\nParameter   | Coefficient |   SE |         95% CI | t(28) |      p\n------------------------------------------------------------------\n(Intercept) |       20.09 | 0.54 | [18.98, 21.20] | 37.20 | < .001\ndisp        |       -2.33 | 1.29 | [-4.98,  0.31] | -1.81 | 0.081 \nhp          |       -1.01 | 1.00 | [-3.06,  1.05] | -1.00 | 0.325 \ncyl         |       -2.19 | 1.42 | [-5.11,  0.72] | -1.54 | 0.135 \n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation."
  },
  {
    "objectID": "regression2.html#exercises",
    "href": "regression2.html#exercises",
    "title": "6  More Regression",
    "section": "6.5 Exercises",
    "text": "6.5 Exercises\n\nPredictor relevance\nAdjusting\nAdjusting 2\nInterpreting Regression coefficients"
  },
  {
    "objectID": "regression2.html#lab",
    "href": "regression2.html#lab",
    "title": "6  More Regression",
    "section": "6.6 Lab",
    "text": "6.6 Lab\nGet your own data, and build a simple model reflecting your research hypothesis based on the topics covered in this chapter. If you are lacking data (or hypothesis) get something close to it."
  },
  {
    "objectID": "regression2.html#glimpse-on-parameter-estimation",
    "href": "regression2.html#glimpse-on-parameter-estimation",
    "title": "6  More Regression",
    "section": "6.7 Glimpse on parameter estimation",
    "text": "6.7 Glimpse on parameter estimation\nAn elegant yet simple explanation of the math of parameter estimation can be found at “go data driven”. A similar approach is presented here.\nHere’s the essence of a geometric interpretation of the least square method, see Figure Figure 6.9.\n\n\n\nFigure 6.9: Geometric interpretation of the least square method. Source: Oleg Alexandrov on Wikimedia"
  },
  {
    "objectID": "regression2.html#further-reading-1",
    "href": "regression2.html#further-reading-1",
    "title": "6  More Regression",
    "section": "6.8 Further Reading",
    "text": "6.8 Further Reading\nMathematical foundations can be found in Deisenroth, Faisal, and Ong (2020). Here’s a collection of online resources tapping into statistics and machine learning.\n\n\n\n\nDeisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020. Mathematics for Machine Learning. Cambridge ; New York, NY: Cambridge University Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge: Cambridge University Press."
  },
  {
    "objectID": "causality.html#intro-to-causality",
    "href": "causality.html#intro-to-causality",
    "title": "7  Causality",
    "section": "7.1 Intro to causality",
    "text": "7.1 Intro to causality\nCheck out this talk."
  },
  {
    "objectID": "causality.html#further-reading",
    "href": "causality.html#further-reading",
    "title": "7  Causality",
    "section": "7.2 Further reading",
    "text": "7.2 Further reading\nRohrer (2018) provides an accessible introduction to causal inference. Slightly more advanced is the introduction by one of the leading figures of the Field, Judea Pearl, Pearl, Glymour, and Jewell (2016).\n\n\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. Chichester, West Sussex: Wiley.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Deisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020.\nMathematics for Machine Learning. Cambridge ; New York,\nNY: Cambridge University Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and\nOther Stories. Analytical Methods for Social Research. Cambridge:\nCambridge University Press.\n\n\nGoodman, Steven. 2008. “A Dirty Dozen: Twelve p-Value\nMisconceptions.” Seminars in Hematology, Interpretation\nof quantitative research, 45 (3): 135–40. https://doi.org/10.1053/j.seminhematol.2008.04.003.\n\n\nHernán, Miguel A., John Hsu, and Brian Healy. 2019. “A Second\nChance to Get Causal Inference Right: A Classification of Data Science\nTasks.” Chance 32 (1): 42–49. https://doi.org/10.1080/09332480.2019.1579578.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal\nInference in Statistics: A Primer. Chichester, West Sussex: Wiley.\n\n\nPoldrack, Russell. 2022. Statistical Thinking for the 21st\nCentury. https://statsthinking21.github.io/statsthinking21-core-site/index.html.\n\n\nRoback, Paul, and Julie Legler. 2021. Beyond Multiple Linear\nRegression: Applied Generalized Linear Models and Multilevel Models\nin. 1st ed. Chapman and Hall Texts in Statistical Science. Boca\nRaton: CRC Press.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and\nCausation: Graphical Causal Models for Observational Data.”\nAdvances in Methods and Practices in Psychological Science 1\n(1): 27–42. https://doi.org/10.1177/2515245917745629.\n\n\nSauer, Sebastian. 2019. Moderne Datenanalyse Mit r: Daten Einlesen,\nAufbereiten, Visualisieren Und Modellieren. 1. Auflage 2019.\nFOM-Edition. Wiesbaden: Springer. https://www.springer.com/de/book/9783658215866.\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The\nASA’s Statement on p-Values: Context, Process, and\nPurpose.” The American Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108.\n\n\nWasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. 2019.\n“Moving to a World Beyond ‘\np</i> p <\n0.05’.” The American Statistician 73 (March):\n1–19. https://doi.org/10.1080/00031305.2019.1583913."
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "2  Basics",
    "section": "",
    "text": "Check out chapter 1 in ModernDive for an accessible introduction to getting started with R and RStudio.\nPlease also note that R and RStudio should be installed upfront (see also in ModernDive)."
  },
  {
    "objectID": "basics.html#data-import",
    "href": "basics.html#data-import",
    "title": "2  Basics",
    "section": "2.2 Data import",
    "text": "2.2 Data import\nCheck out chapter 4 in ModernDive on how to import data into RStudio and for some basic concepts about “tidy data”.\nSpoiler: There’s a button in RStudio in the “Environment” Pane saying “Import Dataset”. Just click it, and things should work out."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "stats-nutshell",
    "section": "",
    "text": "Desktop version, not the server↩︎\npossibly you need also a Fortran compiler, but maybe that’s optional↩︎"
  }
]