[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "stats-nutshell",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "stats-nutshell",
    "section": "Welcome!",
    "text": "Welcome!\nThis is an introductory course on statistical modelling. Welcome!\nThe focus of this course is on how to specify a theoretical idea (possibly vague) in a testable statistical model."
  },
  {
    "objectID": "index.html#please-read-me",
    "href": "index.html#please-read-me",
    "title": "stats-nutshell",
    "section": "Please read me",
    "text": "Please read me\nIn order to benefit as much as possible from this course, it is necessary for you to read this preface information. Yoda agrees (s. Figure 1).\n\n\n\nFigure 1: Yoda finds you should read the manual"
  },
  {
    "objectID": "index.html#pdf-version",
    "href": "index.html#pdf-version",
    "title": "stats-nutshell",
    "section": "PDF-Version",
    "text": "PDF-Version\nUse the print button of your browser to print the html page into a PDF page."
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "stats-nutshell",
    "section": "Course description",
    "text": "Course description\nAnalyzing research data can broadly be classified in three parts: explorative data analysis, modeling (including inference), and visualization. Either part is pivotal in its own right, but it can be argued that modeling is at the core of the scientific endeavor. However, in practice, modeling, visualization, and data exploration is heavily intertwined, so that three parts may be recognized (as individual entities) but not usefully separated from each other. This idea provides the rationale of this course: Data exploration, data visualization and data modeling is discussed as an integrated framework.\nThe focus is on practical data analysis; theoretical concepts are, where mentioned, second class citizens due to time constraints and the didactic aims of the course.\nFor example, statistical inference – such as p-values and confidence intervals – are not more than touched briefly, as the instructor believes that modeling, not inference, is of prime importance for the auditorium.\nWe will use the R environment for all computations (freely available). Please bring your own Laptop with R and RStudio installed (installation guides are provided). Data and R code will be provided."
  },
  {
    "objectID": "index.html#were-on-a-crash-course",
    "href": "index.html#were-on-a-crash-course",
    "title": "stats-nutshell",
    "section": "We’re on a crash course",
    "text": "We’re on a crash course\nThe course is set-up as a “crash course” which indicates that we’ll rather try to cover a breadth of steps rather than digging deep at certain particular points. The rationale of this approach is that before digging deep, it is necessary to gain an overview of the territory. In addition, if one particular topic is not of interest to a given student (perhaps to difficult/simple), not much time is lost.\nBe warned! Compare this crash course to a dancing crash course right before your wedding: A lot can be achieved by such a course in some instances, or rather, the worst consequences (of not knowing how to dance) may be fenced off, but one should not expect to be a dancing queen (king) thereafter."
  },
  {
    "objectID": "index.html#more-on-modelling",
    "href": "index.html#more-on-modelling",
    "title": "stats-nutshell",
    "section": "More on modelling",
    "text": "More on modelling\nModels and modeling are of pivotal importance in many sciences, not only for providing an explanation of nature en miniature (theoretical models), but also for gauging how closely the empirical data at hand match the theoretical model. Translating a theoretical model into statistical language is called statistical modeling and provides the guiding principle in this introductory course. Regression models will be presented as a lingua franca of statistical modeling, and we will learn that many empirical questions can (comfortably) be analyzed using a regression framework. Depending on the background and aims of the participants (and time permitting), we will shed light on some standard topics such as model comparison, classification models, and typical pitfalls. Given a more advanced auditorium, we may want to explore how causal and non-causal associations can be translated and tested using simple linear statistical models. Foundational ideas of statistical modeling will be accompanied by short examples and case studies to facilitate transfer and practical application after the course."
  },
  {
    "objectID": "index.html#course-prerequisites",
    "href": "index.html#course-prerequisites",
    "title": "stats-nutshell",
    "section": "Course prerequisites",
    "text": "Course prerequisites\nBasic computer usage knowledge is needed (downloading materials from the internet, operating a PC, etc). Basic R knowledge is needed. Basic knowledge of statistical concepts (such as descriptive statistics) is needed. Willingness to learn is essential."
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "stats-nutshell",
    "section": "Learning objectives",
    "text": "Learning objectives\nUpon successful completion of this course, students should be able to:\n\nselect the right statistical visualization for a variety of data contexts\n“crunch” or “wrangle” data\nexplain what statistical modeling means\nformulate basic statistical models\ndifferentiate between predictive and explanatory modeling\napply the methods to own datasets"
  },
  {
    "objectID": "index.html#course-literature",
    "href": "index.html#course-literature",
    "title": "stats-nutshell",
    "section": "Course Literature",
    "text": "Course Literature\nThis course builds on the freely available e-book ModernDive. Each topic is paralleled by an accompanying chapter from ModernDive. A hard copy can be purchased here. The book is for sale in print here."
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "stats-nutshell",
    "section": "Course logistics",
    "text": "Course logistics\nThis course can be presented as a one-day seminar or split-up in two or more blocks.\nThe course can be held in English or German.\nPlease bring your own computer and read the notes regarding course logistics in advance. Note that some upfront preparation is needed from the learners.\nR and RStudio1 will be needed throughout the course. Please make sure that the IT is running. In case of technical difficulties with R feel free to use RStudio Cloud; free plans are available.\nAll learning materials (such as literature, code, data) will be provided in electronic format."
  },
  {
    "objectID": "index.html#upfront-student-preparation",
    "href": "index.html#upfront-student-preparation",
    "title": "stats-nutshell",
    "section": "UPFRONT student preparation",
    "text": "UPFRONT student preparation\n\nInstall R and RStudio, see ModernDive Chap. 1.1. In case you have your R running on your system, please make sure that you’re uptodate. If outdated, download and install the most recent versions of the software. Similarly, hit the “Update” button in RStudio’s “Packages” tab to update your packages if you have not done so for a couple of months.\nSign-in at RStudio Cloud. It’s super helpful because I as the techer can provide you with an environment where all R stuff is ready to use (packages installed etc).\nInstall the necessary R packages as used in the book chapters covered in this course (see the sections on “Needed packages” in each chapter). If in doubt, see here the instructions on how to install R packages. Here’s the actual list on the R packages we’ll need.\nStudents new to R are advised to learn the basics, see ModernDive, Chap 1.2 - 1.5\nBring your own laptop\nMake sure your internet connection is stable and your loudspeaker/headset is working; a webcam is helpful.\nStudents are advised to review the course materials after each session.\nI recommend that you carefully check the course description to make sure the course fits your needs (not too advanced/basic)."
  },
  {
    "objectID": "index.html#didactic-outline",
    "href": "index.html#didactic-outline",
    "title": "stats-nutshell",
    "section": "Didactic outline",
    "text": "Didactic outline\nThis course can rather be considered a workshop in the sense that the instructor uses a dialogue-based approach to teaching and that there are numerous exercises during the course. Instead of providing long talks to the students, the instructor feels obligated to engage students in back-and-forth conversations. Similarly, the presentation of a large number of Powerpoint slide is avoided. Instead, a thorough course literature is available (free online), so that students will have no barrier in diving deeply into the materials and ideas presented. However, during class it is more important to transmit the pivotal ideas; details need to be read and worked by the students individually after (and before) the course. As an alternative to presenting a lot of text on slides, in this course there will be a (electronic) whiteboard where concepts are developed dynamically and in pace of the teaching conversation thereby adjusting the “dose” of new thoughts to the actual pace of the instruction."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "stats-nutshell",
    "section": "Schedule",
    "text": "Schedule\n\nOverview on topics covered\n\nData Visualization using the grammar of graphics and ggplot2\nData Wrangling based on the tidyverse in R\nBasic concepts of statistical modelling\nPrimer on causal inference\nIntroduction to regression analysis\n\n\n\nBlock 1: Explorative Data Analysis\n\nVisualization\n\nData visualization, see ModernDive Chap. 2, and get the R code here\n\nExploring common types of statistical diagrams, the “5NG”\nDiscussing when (not) to use diagrams see Anscombe’s Quartett, and when to use which one\nBuilding elegant graphics in R\n\n\n\n\nData Wrangling\n\nData wrangling, see ModernDive Chap. 3, and get the R code here\n\nA taxonomy of typical data operations\nHow to perform common data operations with R\nSummarizing data (aka computing descriptive statistics)\n\n\n\n\nExercises / Case study\n\nExercises\n\nExercises on life expectancy.\nCase study on the visualization of flight delays\nAdvanced case study on one hit wonders\nVisualization covid cases\nCase study on nominal data: Survival on the Titanic\nInspiration for own project: Visualize Covid-19 cases from this source.\n\n\n\n\n\nBlock 2: Statistical Modelling: Basic\n\nTheory\n\nBasics of modelling, see ModernDive Chap. 5.0, and get the R code here\n\nWhat is modelling?\nBasic terminology\nPrediction vs. explanation\n\nSome thoughts on causal inference, see ModernDive Chap. 5.3.1\nRegression with one numerical predictor, see ModernDive Chap. 5.1\nRegression with one categorical predictor, see ModernDive Chap. 5.2\nAssessing model fit (using (adjusted) \\(R^2\\)), see ModernDive Chap. 5.3.2\nFor some tips and tricks on typical issues, see ModernDive tips and tricks\n\n\n\nCase study\n\nExercises/Case studies:\n\nPrices of Boston houses, first part\nModeling movie succes, first part\n\n\n\n\n\nBlock 3: Statistical Modelling: Multiple Regression and interaction\n\nTheory\n\nSlightly more advanced topics on linear regression such as multiple regression and interaction, see ModernDive Chap. 6, and get the R code here\nOne numerical and one categorical predictor, see ModernDive Chap. 6.1\nTwo numerical predictors, see ModernDive Chap. 6.2\nSimpson’s paradox and more on causal inference, see ModernDive Chap. 6.3.3\n\n\n\nCase study\n\nExercises/Case studies:\n\nPrices of Boston houses, second part\nModeling movie succes, second part\nModeling flight delays\n\n\n\n\n\nBlock 4: Project coaching\n\nThis session is dedicated to work on real projects brought in by the students.\nIn addition, open questions regarding the presented concepts are being discussed."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "stats-nutshell",
    "section": "Instructor",
    "text": "Instructor\nSebastian Sauer works as a professor at Ansbach university, teaching statistics and related stuff. Analyzing data to answer questions related to social phenomena is one of his major interests. He is trying to help raising the methological (and particularly statistical) skills in the sciences (ie., scientists). The programming language “R” is one of his favorite tools. He sees himself as a learner, and is particularly interested learning more on quantitative approaches to understand nature. Open Science is a hot topic to him. He hopes to contribute to pressing social problems such as populism by bringing in his statistical and psychological know-how. He writes a blog which serves as a sketchpad for stuff in his mind (not immune to thought updates) at https://data-se.netlify.app/. Sebastian is the author of “Moderne Datenanalyse mit R” (Sauer 2019). His publication list is available on Google Scholar."
  },
  {
    "objectID": "index.html#contact-me",
    "href": "index.html#contact-me",
    "title": "stats-nutshell",
    "section": "Contact me",
    "text": "Contact me\nFeel free to contact me via email at sebastiansauer1@gmail.com."
  },
  {
    "objectID": "index.html#assessment-and-grades",
    "href": "index.html#assessment-and-grades",
    "title": "stats-nutshell",
    "section": "Assessment and grades",
    "text": "Assessment and grades\nThere is no assessment, there are no grades!"
  },
  {
    "objectID": "index.html#talk-to-me",
    "href": "index.html#talk-to-me",
    "title": "stats-nutshell",
    "section": "Talk to me",
    "text": "Talk to me\nIt’s my goal to make this an excellent course and a stimulating and enjoyable experience for all of us. So that I can find out if this is happening, I encourage feedback—be it positive or negative—on all aspects of the course at any time. For example, if something I’m doing is making it difficult for you to learn, then let me know before it’s too late; if you particularly enjoyed something we did in class, say so so that we can do it again."
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "stats-nutshell",
    "section": "Course materials",
    "text": "Course materials\nMost of the materials as presented below is made available through the course book ModernDive. Please check the relevant chapters of the book before the course to make sure you have all materials available."
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "stats-nutshell",
    "section": "Licence",
    "text": "Licence\nThis is permissive work, see the licence here.\nThe author is Sebastian Sauer.\nCheck out the Github repo of this project."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "stats-nutshell",
    "section": "Resources",
    "text": "Resources\n\nRecommendations\n\nRStudio Cheatsheets, particularly on data wrangling, and data vizualization\nBook R for Data Science as a handy reference or a serious text book.\nTidy Tuesday video series\nPost your open question on Stack Overflow.\nFollow #rstats hashtag on Twitter.\n\nFor students willing to learn more and go deeper (than the concepts explored in the present course), this book on regression modelling, and this book on statistical learning are recommended. For German folks, check out my book on modern data analysis.\nSuggested literature for deepening the analytic skills include Statistical Rethinking. For an introduction to graphical causal models, check out Julia Rohrer’s paper. For a more in-depth journey, consider reading this book. While I wholeheartedly recommend such books, we will not be able to discuss many of the ideas presented therein in class (in this course) due to time constraints.\n\n\nR Packages\nAll R packages are accessible through the course book; please consult the relevant chapters. Please install all R packages used before the course. Here’s a tutorial on how to install R packages.\nThe most important R packages for this course are:\n\ntidyverse\neasystats\n\nThe following packages are useful for data access (but not strictly mandatory):\n\ngapminder\nnycflights13\nfivethirtyeight\nskimr\nISLR\n\nFor the Bayes models you’ll need some extra software (free, save and stable), but somewhat more hassle to install. Using Bayes in this course is optional. You don’t miss a lot if you don’t use it.\n\nrstanarm\n\nFor the R package {rstanarm} to run, you’ll need to install RStan. On Windows, this amounts to installing RTools. On Mac, you’ll need to install the XCode CLI2.\nIn sum, follow the instructions on the RStan website. It’s unfortunately a bit complicated.\n\n\nData\nAll data are accessible through the course book; please consult the relevant chapters.\n\n\nLabs (case studies)\nPractical data analysis skills can be practiced using these labs; in addition Chapter 11 provides two cases studies. Note that such content may be used as homework.\nThere are a lot of case studies scattered on the internet.\n\n\nSketching causal models\nDagitty is great tool for sketching causal graphs (DAGs), it can be usd in your browser or as R package. Here’s an example of a collider bias. Check out this post for an intuitive explanation.\n\n\nGerman introductary course\nReaders who speak German may check out this Blitzkurs into data analysis using R."
  },
  {
    "objectID": "index.html#where-are-the-slides",
    "href": "index.html#where-are-the-slides",
    "title": "stats-nutshell",
    "section": "Where are the slides?",
    "text": "Where are the slides?\nThere are none. I feel that slides are not optimal for learning. In class, slides can be detrimental if they are too wordy because that distracts from that the dialogue with the instructor, and I hold this very dialogue as essential. Outside of class, slides are neither helpful. Instead, a good book is much more beneficial, because in a book, there’s enough room to patiently explain in sufficient details, an endeavor which is impossible for a slide deck.\nTo underline my messages to you, dear learners, I will use some sketches on a virtual whiteboard, some interactive apps, live coding, and some (pre-prepared) diagrams. That’s a bit similar to what happens at Khan Academy. You might have noticed that many courses at Coursera follow a similar approach.\nI readily confess that this approach is novel to many learners in these days, learners who are accustomed to hundreds of Powerpoint slides. Please be open and I think you will appreciate this didactic style."
  },
  {
    "objectID": "index.html#technical-details",
    "href": "index.html#technical-details",
    "title": "stats-nutshell",
    "section": "Technical Details",
    "text": "Technical Details\nLast update: 2023-09-13 08:33:20\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Berlin\n date     2023-09-12\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.2.0)\n colorout    * 1.2-2   2022-06-13 [1] local\n digest        0.6.33  2023-07-07 [1] CRAN (R 4.2.0)\n evaluate      0.21    2023-05-05 [1] CRAN (R 4.2.0)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.2.0)\n htmltools     0.5.6   2023-08-10 [1] CRAN (R 4.2.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.2.0)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.2.0)\n knitr         1.44    2023-09-11 [1] CRAN (R 4.2.1)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.2.0)\n rmarkdown     2.24    2023-08-14 [1] CRAN (R 4.2.0)\n rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.2.0)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n xfun          0.40    2023-08-09 [1] CRAN (R 4.2.0)\n\n [1] /Users/sebastiansaueruser/Rlibs\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\nSauer, Sebastian. 2019. Moderne Datenanalyse Mit r: Daten Einlesen, Aufbereiten, Visualisieren Und Modellieren. 1. Auflage 2019. FOM-Edition. Wiesbaden: Springer. https://www.springer.com/de/book/9783658215866."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "stats-nutshell",
    "section": "",
    "text": "Desktop version, not the server↩︎\npossibly you need also a Fortran compiler, but maybe that’s optional↩︎"
  },
  {
    "objectID": "goals.html#overview",
    "href": "goals.html#overview",
    "title": "1  Goals in statistics",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nMany stories to be told. Here’s one, on the goals pursued in statistics (and related fields), see Figure Figure 1.1.\n\n\n\n\n\nflowchart LR\n  A{Goals} --&gt; B(describe)\n  A --&gt; C(predict)\n  A --&gt; D(explain)\n  B --&gt; E(distribution)\n  B --&gt; F(assocation)\n  B --&gt; G(extrapolation)\n  C --&gt; H(point estimate)\n  C --&gt; I(interval)\n  D --&gt; J(causal inference)\n  D --&gt; K(population)\n  D --&gt; L(latent construct)\n\n\n\nFigure 1.1: A taxonomy of statistical goals\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that “goals” do not exist in the world. We make them up in our heads. Hence, they have no ontological existence, they are epistemological beasts. This entails that we are free to devise goals as we wish, provided we can convince ourselves and other souls of the utility of our creativity."
  },
  {
    "objectID": "goals.html#lab",
    "href": "goals.html#lab",
    "title": "1  Goals in statistics",
    "section": "1.2 Lab",
    "text": "1.2 Lab\nMatch your (most pressing) research goal to the nomenclature for scientific goals as shown in Figure 1.1. Explain your reasoning.\nNext, put three research themes or studies you particularly like to this nomenclature and explain your reasoning."
  },
  {
    "objectID": "goals.html#further-reading",
    "href": "goals.html#further-reading",
    "title": "1  Goals in statistics",
    "section": "1.3 Further reading",
    "text": "1.3 Further reading\nHernán, Hsu, and Healy (2019) distinguish:\n\nDescription: “How can women aged 60–80 years with stroke history be partitioned in classes defined by their characteristics?”\nPrediction: “What is the probability of having a stroke next year for women with certain characteristics?”\nCausal inference: “Will starting a statin reduce, on average, the risk of stroke in women with certain characteristics?”\n\nGelman, Hill, and Vehtari (2021), chap. 1.1 proposes three “challenges” of statistical inference."
  },
  {
    "objectID": "goals.html#if-nothing-else-helps",
    "href": "goals.html#if-nothing-else-helps",
    "title": "1  Goals in statistics",
    "section": "1.4 If nothing else helps",
    "text": "1.4 If nothing else helps\nStay calm and behold the infinity.\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge: Cambridge University Press.\n\n\nHernán, Miguel A., John Hsu, and Brian Healy. 2019. “A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks.” Chance 32 (1): 42–49. https://doi.org/10.1080/09332480.2019.1579578."
  },
  {
    "objectID": "basics.html#a-framework-for-problem-solving",
    "href": "basics.html#a-framework-for-problem-solving",
    "title": "2  Basics",
    "section": "2.1 A framework for problem solving",
    "text": "2.1 A framework for problem solving\n\n2.1.1 PPDAC\nThe PPDAC Model is a methodological framework (aka a model) for applying the scientific method to any analytical or research question, or at least it is applicable to quite a few (MacKay and Oldford 2000). It is not meant to be a rigid sequence, but rather a cycle that may turn a number of rounds like a spiral. Statistician Chris Wild puts the PPDAC cycle in the following figure, see Figure Figure 2.1. In this short essay, he summaries his ideas on how to use the PPDAC as a tool for data analysis in problem solving.\n\n\n\nFigure 2.1: PPDAC cycle. Image source: Chris Wild\n\n\nWickham and Grolemund (see Figure Figure 3.1 in Section 3.3) provide a suggestion of the parts of the statistical analyses, that is the “Analysis” step in the PPDAC.\n\n\n2.1.2 Fundamental issues in data analysis\nWild and Pfannkuch (1999) further note that variation is one of the essential characteristics of data. They discern to types of variation however, see Figure Figure 2.2.\n\n\n\nFigure 2.2: Two types of variartion. Image source: Chris Wild\n\n\nWild and Pfannkuch (1999) give a more systematic overview on how a quantitative research question - applied or basic - can be tackled and conceived. For example, in their paper the authors enumarate some dispositions that researcher should embrace in order to fruitfully engage in empirical research:\n\nScepticism\nImagination\nCuriosity\nOpennness\nA propensity to seek deeper menaing\nBeing logical\nEngagement\nPerseverance"
  },
  {
    "objectID": "basics.html#r-basics",
    "href": "basics.html#r-basics",
    "title": "2  Basics",
    "section": "2.2 R Basics",
    "text": "2.2 R Basics\nCheck out chapter 1 in ModernDive for an accessible introduction to getting started with R and RStudio.\nPlease also note that R and RStudio should be installed before starting (this course).\nIn addition, your R packages should be updtodate, according to Arnold Schwarzenegger (s. Figure 2.3).\n\n\n\nFigure 2.3: Keep your R packages uptodate, or risk being an outdated model, Arnie says"
  },
  {
    "objectID": "basics.html#initial-quiz",
    "href": "basics.html#initial-quiz",
    "title": "2  Basics",
    "section": "2.3 Initial quiz",
    "text": "2.3 Initial quiz\nTo get an idea whether you have digested some R basics, consider the following quiz.\n\nExercise 2.1 (Define a variable) Define in R the variable age and assign the value 42.1\n\n\nExercise 2.2 (Define a variable as a string) Define in R the variable name and assign the value me.2\n\n\nExercise 2.3 (Define a variable by another variable) Define in R the variable name and assign the variable age.3\n\n\nExercise 2.4 (Call a function) Ask R what today’s date() is, that is, call a function.4\n\n\nExercise 2.5 (Define a vector) Define in R a vector x with the values 1,2,3 .5\n\n\nExercise 2.6 (Vector wise computation) Square each value in the vector x.6\n\n\nExercise 2.7 (Vector wise computation 2) Square each value in the vector x and sum up the values.7\n\n\nExercise 2.8 (Vector wise computation 3) Square each value in the vector x, sum up the values, and divide by 3.8\n\n\nExercise 2.9 (Compute the variance) Compute the variance of x using basic arithmetic.910\n\n\nExercise 2.10 (Work with NA) Define the vector y with the values 1,2,NA. Compute the mean. Explain the results.11"
  },
  {
    "objectID": "basics.html#data-import",
    "href": "basics.html#data-import",
    "title": "2  Basics",
    "section": "2.4 Data import",
    "text": "2.4 Data import\nCheck out chapter 4 in ModernDive on how to import data into RStudio and for some basic concepts about “tidy data”.\nSpoiler: There’s a button in RStudio in the “Environment” Pane saying “Import Dataset”. Just click it, and things should work out.\n\n\n\n\n\n\nNote\n\n\n\nI strongly advice working with Projects in RStudio, as it makes working with file paths a lot easier."
  },
  {
    "objectID": "basics.html#sec-blitz-data",
    "href": "basics.html#sec-blitz-data",
    "title": "2  Basics",
    "section": "2.5 Blitz start with data",
    "text": "2.5 Blitz start with data\nTo blitz start with data, type the following in R:\n\ndata(mtcars)\n\nAnd the data set mtcars will be available.\nTo get help for the data set, type help(mtcars)\nA bit more advanced, but it’s a nice data set, try the Palmer Penguins data set:\n\nd &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\n\nhead(d)  # see the first few rows, the \"head\" of the table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrownames\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\n\n2007\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n6\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\nHere’s some documentation (code book) for this data set."
  },
  {
    "objectID": "basics.html#lab",
    "href": "basics.html#lab",
    "title": "2  Basics",
    "section": "2.6 Lab",
    "text": "2.6 Lab\nImport your research data into R."
  },
  {
    "objectID": "basics.html#more-data-set",
    "href": "basics.html#more-data-set",
    "title": "2  Basics",
    "section": "2.7 More data set",
    "text": "2.7 More data set\nCheck out this curated list of data sets useful for learning and practicing your data skills."
  },
  {
    "objectID": "basics.html#literature",
    "href": "basics.html#literature",
    "title": "2  Basics",
    "section": "2.8 Literature",
    "text": "2.8 Literature\nWild and Pfannkuch (1999) discuss the thougnht processes involved in statistical problem solving seen from a broad perspective. Ismay and Kim (2020) is a helpful start into the first steps in R.\n\n\n\n\nIsmay, Chester, and Albert Young-Sun Kim. 2020. Statistical Inference via Data Science: A ModernDive into r and the Tidyverse. Chapman & Hall/CRC the r Series. Boca Raton: CRC Press / Taylor & Francis Group. https://moderndive.com/.\n\n\nMacKay, R. J., and R. W. Oldford. 2000. “Scientific Method, Statistical Method and the Speed of Light.” Statistical Science 15 (3): 254–78. https://doi.org/10.1214/ss/1009212817.\n\n\nWild, Chris J, and Maxine Pfannkuch. 1999. “Statistical Thinking in Empirical Enquiry.” International Statistical Review 67 (3): 223–48."
  },
  {
    "objectID": "basics.html#footnotes",
    "href": "basics.html#footnotes",
    "title": "2  Basics",
    "section": "",
    "text": "age &lt;- 42, spaces are optional but useful↩︎\nage &lt;- \"me\"↩︎\nage &lt;- age↩︎\ndate()↩︎\nx &lt;- c(1, 2, 3)↩︎\nx^2↩︎\nsum(x^2)↩︎\nmean(x^2)↩︎\nsum(x^2)↩︎\n\nx &lt;- c(1, 2, 3)\n\nsum((x - mean(x))^2) / (length(x)-1)\n\n[1] 1\n\n# compare: \nvar(x)  \n\n[1] 1\n\n\n↩︎\ny &lt;- c(1, 2, NA); mean(y) NA (not available, ie., missing) is contagious in R: If there’s a missing element, R will assume that something has gone wrong and will raise a red flag, i.e, give you a NA back.↩︎"
  },
  {
    "objectID": "EDA.html#r-packages-needed-for-this-chapter",
    "href": "EDA.html#r-packages-needed-for-this-chapter",
    "title": "3  Exploratory Data Analysis",
    "section": "3.1 R packages needed for this chapter",
    "text": "3.1 R packages needed for this chapter\n\nlibrary(easystats)  # make stats easy again\nlibrary(tidyverse)  # data wrangling\nlibrary(tableone)  # tables, optional\nlibrary(rio)  # import/export data, eg., to excel\nlibrary(ggpubr)  # simple data visualization\nlibrary(ggstatsplot)  # data visualization ornamented with statistics"
  },
  {
    "objectID": "EDA.html#whats-eda",
    "href": "EDA.html#whats-eda",
    "title": "3  Exploratory Data Analysis",
    "section": "3.2 What’s EDA?",
    "text": "3.2 What’s EDA?\nExploratory Data Analysis (EDA) is a procedure to scrutinize a dataset at hand in order learn about it. EDA comprises descriptive statistics, data visualization and data transformation techniques (such as dimension reduction).\nIt’s not so mathematical deep as modelling, but in practice it’s really important.\nThere’s this famous saying:\n\nIn Data Science, 80% of time spent prepare data, 20% of time spent complain about the need to prepare data.\n\nEDA can roughly be said to comprise the following parts:\n\nImporting (and exporting) data\nData cleansing (such as deal with missing data etc)\nData transformation or “wrangling” (such as long to wide format)\nComputing desriptive statistics (such as the notorious mean)\nAnalyzing distributions (is it normal?)\nFinding patterns in data (aka data mining)\nMore complex data transformation techniques (such as factor analysis)"
  },
  {
    "objectID": "EDA.html#sec-data-journey",
    "href": "EDA.html#sec-data-journey",
    "title": "3  Exploratory Data Analysis",
    "section": "3.3 Data journey",
    "text": "3.3 Data journey\nWickham and Grolemund (2016) present a visual sketch of what could be called the “data journey”, i.e., the steps we are taking in order to learn from data, seen from an hands-on angle, see Figure 3.1.\n\n\n\nFigure 3.1: The data journey"
  },
  {
    "objectID": "EDA.html#blitz-data",
    "href": "EDA.html#blitz-data",
    "title": "3  Exploratory Data Analysis",
    "section": "3.4 Blitz data",
    "text": "3.4 Blitz data\nSee Section 2.5 for some data sets suitable to get going."
  },
  {
    "objectID": "EDA.html#data-cleansing",
    "href": "EDA.html#data-cleansing",
    "title": "3  Exploratory Data Analysis",
    "section": "3.5 Data cleansing",
    "text": "3.5 Data cleansing\nThe R package {janitor} provides some nice stuff for data cleansing. Check out this case study."
  },
  {
    "objectID": "EDA.html#convenience-functions",
    "href": "EDA.html#convenience-functions",
    "title": "3  Exploratory Data Analysis",
    "section": "3.6 Convenience functions",
    "text": "3.6 Convenience functions\nThere a quite a few functions (residing in some packages) that help you doing EDA from a helicoptor point of view. In other words, you do not have to pay attention to nitty-gritty details, the function will do that for you. This is approach is, well, convenient, but of course comes at a price. You will not have a great amount of choice and influence on the way the data is analyzed and presented.\n\n3.6.1 Data Explorer\nThere are many systems and approaches to explore data. One particular interesting system is the R-package DataExplorer.\n\n\n\nR-package DataExplorer\n\n\nCheck it out on its Githup page.\n\n\n3.6.2 vtree\nA bit similar to {DataExplorer}, the R package {vtree} helps to explore visually datasets.\n\n\n\nvtree is used to generate variable trees, like the one above.\n\n\n\n\n3.6.3 The easystats way\nThere are some packages, such as {easystats}, which provide comfortable access to basic statistics:\n\nlibrary(easystats)  # once per session\ndescribe_distribution(mtcars)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nMin\nMax\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nmpg\n20.090625\n6.0269481\n7.52500\n10.400\n33.900\n0.6723771\n-0.0220063\n32\n0\n\n\ncyl\n6.187500\n1.7859216\n4.00000\n4.000\n8.000\n-0.1922609\n-1.7627939\n32\n0\n\n\ndisp\n230.721875\n123.9386938\n221.52500\n71.100\n472.000\n0.4202331\n-1.0675234\n32\n0\n\n\nhp\n146.687500\n68.5628685\n84.50000\n52.000\n335.000\n0.7994067\n0.2752116\n32\n0\n\n\ndrat\n3.596563\n0.5346787\n0.84000\n2.760\n4.930\n0.2927802\n-0.4504325\n32\n0\n\n\nwt\n3.217250\n0.9784574\n1.18625\n1.513\n5.424\n0.4659161\n0.4165947\n32\n0\n\n\nqsec\n17.848750\n1.7869432\n2.02250\n14.500\n22.900\n0.4063466\n0.8649307\n32\n0\n\n\nvs\n0.437500\n0.5040161\n1.00000\n0.000\n1.000\n0.2645418\n-2.0632731\n32\n0\n\n\nam\n0.406250\n0.4989909\n1.00000\n0.000\n1.000\n0.4008089\n-1.9665503\n32\n0\n\n\ngear\n3.687500\n0.7378041\n1.00000\n3.000\n5.000\n0.5823086\n-0.8952916\n32\n0\n\n\ncarb\n2.812500\n1.6152000\n2.00000\n1.000\n8.000\n1.1570911\n2.0200593\n32\n0\n\n\n\n\n\n\ndescribe_distribution provides us with an overview on typical descriptive summaries.\nFor nominal variables, consider data_tabulate:\n\ndata_tabulate(mtcars, select = c(\"am\", \"vs\"))\n\nam (am) &lt;numeric&gt;\n# total N=32 valid N=32\n\nValue |  N | Raw % | Valid % | Cumulative %\n------+----+-------+---------+-------------\n0     | 19 | 59.38 |   59.38 |        59.38\n1     | 13 | 40.62 |   40.62 |       100.00\n&lt;NA&gt;  |  0 |  0.00 |    &lt;NA&gt; |         &lt;NA&gt;\n\nvs (vs) &lt;numeric&gt;\n# total N=32 valid N=32\n\nValue |  N | Raw % | Valid % | Cumulative %\n------+----+-------+---------+-------------\n0     | 18 | 56.25 |   56.25 |        56.25\n1     | 14 | 43.75 |   43.75 |       100.00\n&lt;NA&gt;  |  0 |  0.00 |    &lt;NA&gt; |         &lt;NA&gt;\n\n\nWe can also get grouped tabulations, which amounts to something similar to a contingency table:\n\nmtcars %&gt;% \n  group_by(am) %&gt;% \n  data_tabulate(select = \"vs\", collapse = TRUE)\n\n# Frequency Table\n\nVariable |  Group | Value |  N | Raw % | Valid % | Cumulative %\n---------+--------+-------+----+-------+---------+-------------\nvs       | am (0) |     0 | 12 | 63.16 |   63.16 |        63.16\n         |        |     1 |  7 | 36.84 |   36.84 |       100.00\n         |        |  &lt;NA&gt; |  0 |  0.00 |    &lt;NA&gt; |         &lt;NA&gt;\n---------+--------+-------+----+-------+---------+-------------\nvs       | am (1) |     0 |  6 | 46.15 |   46.15 |        46.15\n         |        |     1 |  7 | 53.85 |   53.85 |       100.00\n         |        |  &lt;NA&gt; |  0 |  0.00 |    &lt;NA&gt; |         &lt;NA&gt;\n---------------------------------------------------------------\n\n\n\nCheckout the function reference of your favorite package in order to learn what’s on the shelf. For example, here’s the function reference site of datawizard, one of the packages in the easystats ecosystem.\n\n\n\n3.6.4 TableOne\nThe R package {tableOne} provides something like the typical “Table One” in many papers.\nFrom the homepage:\n\nThe tableone package is an R package that eases the construction of “Table 1”, i.e., patient baseline characteristics table commonly found in biomedical research papers. The packages can summarize both continuous and categorical variables mixed within one table. Categorical variables can be summarized as counts and/or percentages. Continuous variables can be summarized in the “normal” way (means and standard deviations) or “nonnormal” way (medians and interquartile ranges).\n\n\npenguins &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\n\nIt’s quite simple to use:\n\nCreateTableOne(data = penguins)\n\n                               \n                                Overall         \n  n                                 344         \n  rownames (mean (SD))           172.50 (99.45) \n  species (%)                                   \n     Adelie                         152 (44.2)  \n     Chinstrap                       68 (19.8)  \n     Gentoo                         124 (36.0)  \n  island (%)                                    \n     Biscoe                         168 (48.8)  \n     Dream                          124 (36.0)  \n     Torgersen                       52 (15.1)  \n  bill_length_mm (mean (SD))      43.92 (5.46)  \n  bill_depth_mm (mean (SD))       17.15 (1.97)  \n  flipper_length_mm (mean (SD))  200.92 (14.06) \n  body_mass_g (mean (SD))       4201.75 (801.95)\n  sex (%)                                       \n                                     11 ( 3.2)  \n     female                         165 (48.0)  \n     male                           168 (48.8)  \n  year (mean (SD))              2008.03 (0.82)  \n\n\nTo get more detailled results, use the summary method:\n\ntab1 &lt;- CreateTableOne(data = penguins)\nsummary(tab1)\n\n\n     ### Summary of continuous variables ###\n\nstrata: Overall\n                    n miss p.miss mean    sd median  p25  p75  min  max  skew\nrownames          344    0    0.0  172  99.4    172   87  258    1  344  0.00\nbill_length_mm    344    2    0.6   44   5.5     44   39   48   32   60  0.05\nbill_depth_mm     344    2    0.6   17   2.0     17   16   19   13   22 -0.14\nflipper_length_mm 344    2    0.6  201  14.1    197  190  213  172  231  0.35\nbody_mass_g       344    2    0.6 4202 802.0   4050 3550 4750 2700 6300  0.47\nyear              344    0    0.0 2008   0.8   2008 2007 2009 2007 2009 -0.05\n                  kurt\nrownames          -1.2\nbill_length_mm    -0.9\nbill_depth_mm     -0.9\nflipper_length_mm -1.0\nbody_mass_g       -0.7\nyear              -1.5\n\n=======================================================================================\n\n     ### Summary of categorical variables ### \n\nstrata: Overall\n     var   n miss p.miss     level freq percent cum.percent\n species 344    0    0.0    Adelie  152    44.2        44.2\n                         Chinstrap   68    19.8        64.0\n                            Gentoo  124    36.0       100.0\n                                                           \n  island 344    0    0.0    Biscoe  168    48.8        48.8\n                             Dream  124    36.0        84.9\n                         Torgersen   52    15.1       100.0\n                                                           \n     sex 344    0    0.0             11     3.2         3.2\n                            female  165    48.0        51.2\n                              male  168    48.8       100.0"
  },
  {
    "objectID": "EDA.html#lab",
    "href": "EDA.html#lab",
    "title": "3  Exploratory Data Analysis",
    "section": "3.7 Lab",
    "text": "3.7 Lab\nTake your research data and prepare it using (at least) one of the “convenience” functions for data cleansing presented above."
  },
  {
    "objectID": "EDA.html#tidyverse",
    "href": "EDA.html#tidyverse",
    "title": "3  Exploratory Data Analysis",
    "section": "3.8 Tidyverse",
    "text": "3.8 Tidyverse\n\n3.8.1 Intro to the tidyverse\nThe Tidyverse is probably the R thing with the most publicity. And it’s great. It’s a philosophy baken into an array of R packages. Perhaps central is the idea that a lot of little lego pieces, if fitting nicely together, provides a simple yet flexibel and thus powerful machinery.\nThere’s a lot of introctory material to the tidyverse around for instance, so I’m not repeating that here.\n\n\n3.8.2 More advanced tidyverse\n\n3.8.2.1 Repeat a function over many columns\nAt times, we would like to compute the same functions for many variables, ie columns for tidyverse applications.\nLet’s load the penguins data for illustration.\n\nd &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\n\nhead(d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrownames\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n6\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\nSay, we would like to compute the mean value for each numeric variable in the data set:\n\nd %&gt;% \n  summarise(across(bill_length_mm:body_mass_g, mean, na.rm = TRUE))\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\n43.92193\n17.15117\n200.9152\n4201.754\n\n\n\n\n\n\nSynonymously, we could write:\n\nd %&gt;% \n  summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)))\n\n\n\n\n\n\n\n\n\n\n\n\n\nrownames\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nyear\n\n\n\n\n172.5\n43.92193\n17.15117\n200.9152\n4201.754\n2008.029\n\n\n\n\n\n\nSay, we would like to compute the z-value of each numeric variable.\nAddmittedly, easystats makes it quite simple:\n\nd %&gt;% \n  standardise(select = is.numeric) %&gt;% \n  head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrownames\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n-1.724511\nAdelie\nTorgersen\n-0.8832047\n0.7843001\n-1.4162715\n-0.5633167\nmale\n-1.257484\n\n\n-1.714456\nAdelie\nTorgersen\n-0.8099390\n0.1260033\n-1.0606961\n-0.5009690\nfemale\n-1.257484\n\n\n-1.704400\nAdelie\nTorgersen\n-0.6634077\n0.4298326\n-0.4206603\n-1.1867934\nfemale\n-1.257484\n\n\n-1.694345\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n-1.257484\n\n\n-1.684289\nAdelie\nTorgersen\n-1.3227986\n1.0881294\n-0.5628905\n-0.9374027\nfemale\n-1.257484\n\n\n-1.674234\nAdelie\nTorgersen\n-0.8465718\n1.7464261\n-0.7762357\n-0.6880121\nmale\n-1.257484\n\n\n\n\n\n\nSee the help page of standardise for mor details on how to select variables and on more options.\nBut for the purpose of illustration, let’s do it with more simple means, i.e. tidyverse only.\n\nd %&gt;% \n  transmute(across(bill_length_mm:body_mass_g, \n                .fns = ~ {(.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)},\n                .names = \"{.col}_z\")) %&gt;% \n  head()\n\n\n\n\n\nbill_length_mm_z\nbill_depth_mm_z\nflipper_length_mm_z\nbody_mass_g_z\n\n\n\n\n-0.8832047\n0.7843001\n-1.4162715\n-0.5633167\n\n\n-0.8099390\n0.1260033\n-1.0606961\n-0.5009690\n\n\n-0.6634077\n0.4298326\n-0.4206603\n-1.1867934\n\n\nNA\nNA\nNA\nNA\n\n\n-1.3227986\n1.0881294\n-0.5628905\n-0.9374027\n\n\n-0.8465718\n1.7464261\n-0.7762357\n-0.6880121\n\n\n\n\n\n\nIt’s maybe more succint to put the z-value computation in its function, and then just apply this function:\n\nz_stand &lt;- function(x){\n  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\n}\n\n\nd2 &lt;-\nd %&gt;% \n  mutate(across(bill_length_mm:body_mass_g, \n                .fns = z_stand))\n  \nd2 %&gt;% \n  glimpse()\n\nRows: 344\nColumns: 9\n$ rownames          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ bill_length_mm    &lt;dbl&gt; -0.8832047, -0.8099390, -0.6634077, NA, -1.3227986, …\n$ bill_depth_mm     &lt;dbl&gt; 0.78430007, 0.12600328, 0.42983257, NA, 1.08812936, …\n$ flipper_length_mm &lt;dbl&gt; -1.4162715, -1.0606961, -0.4206603, NA, -0.5628905, …\n$ body_mass_g       &lt;dbl&gt; -0.563316704, -0.500969030, -1.186793445, NA, -0.937…\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"f…\n$ year              &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n3.8.2.2 Rowwise operations\nFor technical reasons, it’s a bit cumbersome in (base) R to compute rowwise operations. The thing is, R’s dataframes are organized as vectors of columns so it’s much easier to do stuff columnwise.\nHowever, since recently, computing rowwise operations with the tidyverse has become simpler. Consider the following example. Say we would like to know the highest z-value for each variable we just computed, that is the highest values per individual, ie., by row in the data frame.\n\nd2 %&gt;% \n  drop_na() %&gt;% \n  rowwise() %&gt;% \n  mutate(max_z = max(c(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g))) %&gt;% \n  head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrownames\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\nmax_z\n\n\n\n\n1\nAdelie\nTorgersen\n-0.8832047\n0.7843001\n-1.4162715\n-0.5633167\nmale\n2007\n0.7843001\n\n\n2\nAdelie\nTorgersen\n-0.8099390\n0.1260033\n-1.0606961\n-0.5009690\nfemale\n2007\n0.1260033\n\n\n3\nAdelie\nTorgersen\n-0.6634077\n0.4298326\n-0.4206603\n-1.1867934\nfemale\n2007\n0.4298326\n\n\n5\nAdelie\nTorgersen\n-1.3227986\n1.0881294\n-0.5628905\n-0.9374027\nfemale\n2007\n1.0881294\n\n\n6\nAdelie\nTorgersen\n-0.8465718\n1.7464261\n-0.7762357\n-0.6880121\nmale\n2007\n1.7464261\n\n\n7\nAdelie\nTorgersen\n-0.9198375\n0.3285561\n-1.4162715\n-0.7191859\nfemale\n2007\n0.3285561"
  },
  {
    "objectID": "EDA.html#data-visualization",
    "href": "EDA.html#data-visualization",
    "title": "3  Exploratory Data Analysis",
    "section": "3.9 Data visualization",
    "text": "3.9 Data visualization\nThe star on the R visualization sky is called ggplot. It probably is one of the most advanced statistical visualization package on the planet.\nHowever, for a quick start, there are some nice wrappers on ggplot, levering the beauty of ggplot but making the making simple(r).\nggpubr is a simple yet powerful way to visualize data.\nIn addtion, consider [ggstatsplot])(https://indrajeetpatil.github.io/ggstatsplot/). Let’s use the mtcars dataset for a quick demonstration. For example, assume you would like to compare two group of cars, automatic vs. manual shifting, in terms of fuel economy. Here we go, ?fig-ggstatsplot1.\n\nggbetweenstats(\n  data = mtcars,\n  x = am,\n  y = mpg)\n\n\n\n\nFigure 3.2: ggstatsplot baut auf ggplot2 auf, ist aber einfacher zu bedienen"
  },
  {
    "objectID": "EDA.html#exporting-to-office",
    "href": "EDA.html#exporting-to-office",
    "title": "3  Exploratory Data Analysis",
    "section": "3.10 Exporting to Office",
    "text": "3.10 Exporting to Office\n\n3.10.1 Excel\nThe most straightforward approach is to convince your EDA function to produce a data frame. Data frames can be written as CSV or XLSX to disk, and then easily imported to office packages.\neasystats and tidyverse are two examples where this happens.\n\ndf1 &lt;- describe_distribution(mtcars)\n\nNow, df1 is a data frame:\n\nstr(df1)\n\nClasses 'parameters_distribution', 'see_parameters_distribution' and 'data.frame':  11 obs. of  10 variables:\n $ Variable : chr  \"mpg\" \"cyl\" \"disp\" \"hp\" ...\n $ Mean     : num  20.09 6.19 230.72 146.69 3.6 ...\n $ SD       : num  6.027 1.786 123.939 68.563 0.535 ...\n $ IQR      : num  7.53 4 221.53 84.5 0.84 ...\n $ Min      : num  10.4 4 71.1 52 2.76 ...\n $ Max      : num  33.9 8 472 335 4.93 ...\n $ Skewness : num  0.672 -0.192 0.42 0.799 0.293 ...\n $ Kurtosis : num  -0.022 -1.763 -1.068 0.275 -0.45 ...\n $ n        : int  32 32 32 32 32 32 32 32 32 32 ...\n $ n_Missing: int  0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"object_name\")= chr \"mtcars\"\n - attr(*, \"threshold\")= num 0.1\n\n\nLet’s export as XLSX (via the R package rio) and as CSV:\n\nexport(df1, file = \"df1.xlsx\")\nexport(df1, file = \"df1.csv\")\n\nFor exporting csv files we could also use write_csv() from the tidyverse or write.csv from base R.\n\ndf2 &lt;- data_tabulate(mtcars$am)\nstr(df2)\n\nClasses 'dw_data_tabulate' and 'data.frame':    3 obs. of  6 variables:\n $ Variable    : chr  \"mtcars$am\" \"mtcars$am\" \"mtcars$am\"\n $ Value       : Factor w/ 2 levels \"0\",\"1\": 1 2 NA\n $ N           : int  19 13 0\n $ Raw %       : num  59.4 40.6 0\n $ Valid %     : num  59.4 40.6 NA\n $ Cumulative %: num  59.4 100 NA\n - attr(*, \"type\")= chr \"numeric\"\n - attr(*, \"object\")= chr \"mtcars$am\"\n - attr(*, \"duplicate_varnames\")= logi [1:3] FALSE TRUE TRUE\n - attr(*, \"total_n\")= int 32\n - attr(*, \"valid_n\")= int 32\n\n\n\nexport(df2, file = \"df2.csv\")\n\nNote that if we use data_tabulate like this:\n\ndf3 &lt;- data_tabulate(mtcars, select = c(\"am\", \"vs\"))\n\ndf3 %&gt;% str()\n\nList of 2\n $ :Classes 'dw_data_tabulate' and 'data.frame':    3 obs. of  6 variables:\n  ..$ Variable    : chr [1:3] \"am\" \"am\" \"am\"\n  ..$ Value       : Factor w/ 2 levels \"0\",\"1\": 1 2 NA\n  ..$ N           : int [1:3] 19 13 0\n  ..$ Raw %       : num [1:3] 59.4 40.6 0\n  ..$ Valid %     : num [1:3] 59.4 40.6 NA\n  ..$ Cumulative %: num [1:3] 59.4 100 NA\n  ..- attr(*, \"type\")= chr \"numeric\"\n  ..- attr(*, \"varname\")= chr \"am\"\n  ..- attr(*, \"object\")= chr \"am\"\n  ..- attr(*, \"duplicate_varnames\")= logi [1:3] FALSE TRUE TRUE\n  ..- attr(*, \"total_n\")= int 32\n  ..- attr(*, \"valid_n\")= int 32\n $ :Classes 'dw_data_tabulate' and 'data.frame':    3 obs. of  6 variables:\n  ..$ Variable    : chr [1:3] \"vs\" \"vs\" \"vs\"\n  ..$ Value       : Factor w/ 2 levels \"0\",\"1\": 1 2 NA\n  ..$ N           : int [1:3] 18 14 0\n  ..$ Raw %       : num [1:3] 56.2 43.8 0\n  ..$ Valid %     : num [1:3] 56.2 43.8 NA\n  ..$ Cumulative %: num [1:3] 56.2 100 NA\n  ..- attr(*, \"type\")= chr \"numeric\"\n  ..- attr(*, \"varname\")= chr \"vs\"\n  ..- attr(*, \"object\")= chr \"vs\"\n  ..- attr(*, \"duplicate_varnames\")= logi [1:3] FALSE TRUE TRUE\n  ..- attr(*, \"total_n\")= int 32\n  ..- attr(*, \"valid_n\")= int 32\n - attr(*, \"class\")= chr [1:2] \"dw_data_tabulates\" \"list\"\n - attr(*, \"collapse\")= logi FALSE\n\n\nWe’ll get a list of two data frames.\nTo export either, we need to access each list list element:\n\nexport(df3[[1]], file = \"df3.csv\")\n\n\n\n3.10.2 Word\nObviously, once your data has arriven as a spreadsheet in Excel (or similar software packages), it’s no big deal to get it into Word processors as well.\nIn addition, one user gave the following recommendation on StackOverflow:\n\nAnother possible solution: The above strategy did not work for me when I had a similar issue, but it was resolved once I knitted the table1 object and opened the html in browser to copy the html table and successfully paste into word. Doing it within RStudio viewer would not work for me for some reason.\n\nLastly, there are options to export directly to Word or Powerpoint. The R package flextable provides functions for that purpose:\n\nlibrary(flextable)\nmy_flex_tab &lt;- flextable(penguins)\n\nsave_as_docx(\"Table 1 \" = my_flex_tab, path = \"my_tab.docx\")\n\nsave_as_pptx(\"Table 1 \" = my_flex_tab, path = \"my_tab.pptx\")"
  },
  {
    "objectID": "EDA.html#case-studies",
    "href": "EDA.html#case-studies",
    "title": "3  Exploratory Data Analysis",
    "section": "3.11 Case Studies",
    "text": "3.11 Case Studies\n\n3.11.1 Data Wrangling\n\n\n\nR package/dataset palmerpenguins\n\n\nExplore the palmerpenguins dataset, it’s a famous dataset made for learning data analysis.\nThere’s a great interactive course on EDA based on the penguins. Have a look, it’s great!\nGo penguins! Allez!\n\n\n3.11.2 Data visualization\n\nGapminder\nPenguins\nmtcars"
  },
  {
    "objectID": "EDA.html#cheatsheets",
    "href": "EDA.html#cheatsheets",
    "title": "3  Exploratory Data Analysis",
    "section": "3.12 Cheatsheets",
    "text": "3.12 Cheatsheets\nThere are a number of nice cheat sheets available on an array of topics related to EDA, made available by the folks at RStudio.\nConsider this collection:\n\n{dplyr}: data wrangling\n{tidyr}: data preparation\n{ggplot}: data visualization\n{gtsummary}: publication ready tables\n\nSo much great stuff! A bit too much to digest in one go, but definitely worthwhile if you plan to dig deeper in data analysis."
  },
  {
    "objectID": "EDA.html#literature",
    "href": "EDA.html#literature",
    "title": "3  Exploratory Data Analysis",
    "section": "3.13 Literature",
    "text": "3.13 Literature\nWickham and Grolemund (2016) is an highly recommendable resource in order to get a thorough understanding of data analysis using R. Note that this source is focusing on the “how to”, not so much to theoretical foundations. Ismay and Kim (2020) is a gently introduction into many steps on the data journey, including EDA.\n\n\n\n\nIsmay, Chester, and Albert Young-Sun Kim. 2020. Statistical Inference via Data Science: A ModernDive into r and the Tidyverse. Chapman & Hall/CRC the r Series. Boca Raton: CRC Press / Taylor & Francis Group. https://moderndive.com/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Visualize, Model, Transform, Tidy, and Import Data. O’Reilly Media. https://r4ds.had.co.nz/index.html."
  },
  {
    "objectID": "inference.html#what-is-it",
    "href": "inference.html#what-is-it",
    "title": "4  Inference",
    "section": "4.1 What is it?",
    "text": "4.1 What is it?\nStatistical inference, according to Gelman, Hill, and Vehtari (2021), chap. 1.1, faces the challenge of generalizing from the particular to the general.\nIn more details, this amounts to generalizing from …\n\na sample to a population\na treatment to a control group (i.e., causal inference)\nobserved measurement to the underlying (“latent”) construct of interest\n\n\n\n\n\n\n\nImportant\n\n\n\nStatistical inference is concerned with making general claims from particular data using mathematical tools."
  },
  {
    "objectID": "inference.html#population-and-sample",
    "href": "inference.html#population-and-sample",
    "title": "4  Inference",
    "section": "4.2 Population and sample",
    "text": "4.2 Population and sample\nWe want to have an estimate of some population value, for example the proportion of A.\nHowever, all we have is a subset, a sample of the populuation. Hence, we need to infer from the sample to the popluation. We do so by generalizing from the sample to the population, see Figure Figure 4.1.\n\n\n\n\n\n\n\n(a) Population\n\n\n\n\n\n\n\n(b) Sample\n\n\n\n\nFigure 4.1: Population vs. sample (Image credit: Karsten Luebke)"
  },
  {
    "objectID": "inference.html#whats-not-inference",
    "href": "inference.html#whats-not-inference",
    "title": "4  Inference",
    "section": "4.3 What’s not inference?",
    "text": "4.3 What’s not inference?\nConsider fig. Figure 4.2 which epitomizes the difference between descriptive and inferential statistics.\n\n\n\n\n\nFigure 4.2: The difference between description and inference"
  },
  {
    "objectID": "inference.html#when-size-helps",
    "href": "inference.html#when-size-helps",
    "title": "4  Inference",
    "section": "4.4 When size helps",
    "text": "4.4 When size helps\nLarger samples allow for more precise estimations (ceteris paribus).\n\n\n\n\n\nSample size in motion, Image credit: Karsten Luebke"
  },
  {
    "objectID": "inference.html#what-flavors-are-available",
    "href": "inference.html#what-flavors-are-available",
    "title": "4  Inference",
    "section": "4.5 What flavors are available?",
    "text": "4.5 What flavors are available?\nTypically, when one hears “inference” one thinks of p-values and null hypothesis testing. Those procedures are examples of the school of Frequentist statistics.\nHowever, there’s a second flavor of statistics to be mentioned here: Bayesian statistics.\n\n4.5.1 Frequentist inference\nFrequentism is not concerned about the probability of your research hypothesis.\nFrequentism is all about controlling the long-term error. For illustration, suppose you are the CEO of a factory producing screws, and many of them. As the boss, you are not so much interested if a particular scree is in order (or faulty). Rather you are interested that the overall, long-term error rate of your production is low. One may add that your goal might not the minimize the long-term error, b ut to control it to a certain level - it may be to expensive to produce super high quality screws. Some decent, but cheap screws, might be more profitable.\n\n\n4.5.2 Bayes inference\nBayes inference is concerned about the probability of your research hypothesis.\nIt simply redistributes your beliefs based on new data (evidence) you observe, see Figure Figure 4.3.\n\n\n\n\n\nflowchart LR\n  A(prior belief) --&gt; B(new data) --&gt; C(posterior belief)\n\n\n\nFigure 4.3: Bayesian belief updating\n\n\n\n\nIn more detail, the posterior belief is formalized as the posterior probability. The Likelihood is the probability of the data given some hypothesis. The normalizing constant serves to give us a number between zero and one.\n\\[\\overbrace{\\Pr(\\color{blue}{H}|\\color{green}{D})}^\\text{posterior probability} = \\overbrace{Pr(\\color{blue}{H})}^\\text{prior} \\frac{\\overbrace{Pr(\\color{green}{D}|\\color{blue}{H})}^\\text{likelihood}}{\\underbrace{Pr(\\color{green}{D})}_{\\text{normalizing constant}}}\\]\nIn practice, the posterior probability of your hypothesis is, the average of your prior and the Likelihood of your data.\n\n\n\nPrior-Likelihood-Posterior\n\n\nCan you see that the posterior is some average of prior and likelihood?\nCheck out this great video on Bayes Theorem by 3b1b."
  },
  {
    "objectID": "inference.html#but-which-one-should-i-consume",
    "href": "inference.html#but-which-one-should-i-consume",
    "title": "4  Inference",
    "section": "4.6 But which one should I consume?",
    "text": "4.6 But which one should I consume?\nPRO Frequentist:\n\nYour supervisor and reviewers will be more familiar with it\nThe technical overhead is simpler compared to Bayes\n\nPRO Bayes:\n\nYou’ll probably want to have a posterior probability of your hypothesis\nYou may appear as a cool kid and an early adoptor of emering statistical methods\n\n\n\n\n\n\n\nTip\n\n\n\nYou’ll learn that the technical setup used for doing Bayes statistics is quite similar to doing frequentist statistics. Stay tuned."
  },
  {
    "objectID": "inference.html#lab",
    "href": "inference.html#lab",
    "title": "4  Inference",
    "section": "4.7 Lab",
    "text": "4.7 Lab\nConsider your (most pressing) research question. Assess whether it is more accessible via Frequentist or via Bayesian statistics. Explain your reasoning."
  },
  {
    "objectID": "inference.html#comment-from-xkcd",
    "href": "inference.html#comment-from-xkcd",
    "title": "4  Inference",
    "section": "4.8 Comment from xkcd",
    "text": "4.8 Comment from xkcd\n\n\n\n\n\n\n\n\n\nQuelle"
  },
  {
    "objectID": "inference.html#p-value",
    "href": "inference.html#p-value",
    "title": "4  Inference",
    "section": "4.9 p-value",
    "text": "4.9 p-value\nThe p-value has been used as the pivotal criterion to decide about whether or not a research hypothesis were to be “accepted” (a term forbidden in frequentist and Popperian langauge) or to be rejected. However, more recently, it is advised to use the p-value only as one indicator among multiple; see Wasserstein and Lazar (2016) and Wasserstein, Schirm, and Lazar (2019).\n\n\n\n\n\n\nImportant\n\n\n\nThe p-value is defined as the probability of obtaining the observed data (or more extreme) under the assumption of no effect.\n\n\nFigure Figure 4.4 visualizes the p-value.\n\n\n\n\n\nFigure 4.4: Visualization of the p-value"
  },
  {
    "objectID": "inference.html#some-confusion-remains-about-the-p-value",
    "href": "inference.html#some-confusion-remains-about-the-p-value",
    "title": "4  Inference",
    "section": "4.10 Some confusion remains about the p-value",
    "text": "4.10 Some confusion remains about the p-value\n\n\n\nSource: from ImgFlip Meme Generator\n\n\nGoodman (2008) provides an entertaining overview on typical misconceptions of the p-value full text.\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge: Cambridge University Press.\n\n\nGoodman, Steven. 2008. “A Dirty Dozen: Twelve p-Value Misconceptions.” Seminars in Hematology, Interpretation of quantitative research, 45 (3): 135–40. https://doi.org/10.1053/j.seminhematol.2008.04.003.\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA’s Statement on p-Values: Context, Process, and Purpose.” The American Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108.\n\n\nWasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. 2019. “Moving to a World Beyond ‘ p&lt;/i&gt; p &lt; 0.05’.” The American Statistician 73 (March): 1–19. https://doi.org/10.1080/00031305.2019.1583913."
  },
  {
    "objectID": "regression1.html#r-packages-needed-for-this-chapter",
    "href": "regression1.html#r-packages-needed-for-this-chapter",
    "title": "5  Modelling and regression",
    "section": "5.1 R packages needed for this chapter",
    "text": "5.1 R packages needed for this chapter\n\nlibrary(easystats)\nlibrary(tidyverse)\nlibrary(rstanarm)  # optional!"
  },
  {
    "objectID": "regression1.html#whats-modelling",
    "href": "regression1.html#whats-modelling",
    "title": "5  Modelling and regression",
    "section": "5.2 What’s modelling?",
    "text": "5.2 What’s modelling?\nRead this great introduction by modelling by Russel Poldrack. Actually, the whole book is nice Poldrack (2022).\nAn epitome of modelling is this, let’s call it the fundamental modelling equation, a bit grandiose but at the point, see Equation 5.1.\nThe data can be separated in the model’s prediction and the rest (the “error”), i.e., what’s unaccounted for by the model.\n\\[\n\\text{data} = \\text{model} + \\text{error}\n\\tag{5.1}\\]\nA more visual account of our basic modelling equation is depicted in Figure 5.1.\n\n\n\n\n\nflowchart LR\n  X --&gt; Y\n  error --&gt; Y\n\n\nFigure 5.1: A more visual account of our basic modelling equation"
  },
  {
    "objectID": "regression1.html#regression-as-the-umbrella-tool-for-modelling",
    "href": "regression1.html#regression-as-the-umbrella-tool-for-modelling",
    "title": "5  Modelling and regression",
    "section": "5.3 Regression as the umbrella tool for modelling",
    "text": "5.3 Regression as the umbrella tool for modelling\n\n\n\nOne regression\n\n\nSource: Image Flip\nAlternatively, venture into the forest of statistical tests as outlined e.g. here, at Uni Muenster. Proceed at your own peril.\nYou may want to ponder on this image of a decision tree of which test to choose, see Figure Figure 5.2.\n\n\n\nFigure 5.2: Choose your test carefully\n\n\n\n5.3.1 Common statistical tests are linear models\nAs Jonas Kristoffer Lindeløv tells us, we can formulate most statistical tests as a linear model, ie., a regression.\n\n\n\nCommon statistical tests as linear models\n\n\n\n\n5.3.2 How to find the regression line\nIn the simplest case, regression analyses can be interpreted geometrically as a line in a 2D coordinate system, see Figure Figure 5.3.\n\n\n\nFigure 5.3: Least Square Regression\n\n\nSource: Orzetoo, CC-SA, Wikimedia\nPut simple, we are looking for the line which is in the “middle of the points”. More precisely, we place the line such that the squared distances from the line to the points is minimal, see Figre Figure 5.3.\nConsider Figure Figure 5.4, from this source by Roback and Legler (2021). It visualizes not only the notorious regression line, but also sheds light on regression assumptions, particularly on the error distribution.\n\n\n\nFigure 5.4: Regression and some of its assumptions\n\n\nAmong the assumptions of the linear model are:\n\nlinearity of the function\nvariance of \\(y\\) remains constant across range of \\(x\\)\nnormality of residuals\n\n\n\n5.3.3 The linear model\nHere’s the canonical form of the linear model.\nConsider a model with \\(k\\) predictors:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k + \\epsilon\\]\n\n\n5.3.4 Algebraic derivation\nFor the mathematical inclined, check out this derivation of the simple case regression model. Note that the article is written in German, but your browser can effortlessly translate into English. Here’s a similar English article from StackExchange."
  },
  {
    "objectID": "regression1.html#in-all-its-glory",
    "href": "regression1.html#in-all-its-glory",
    "title": "5  Modelling and regression",
    "section": "5.4 In all its glory",
    "text": "5.4 In all its glory\n\n\n\n\n\n\n\n\n\nLet’s depict the residuals, s. Figure 5.5.\n\n\n\n\n\nFigure 5.5: Residuals as deviations from the predicted value"
  },
  {
    "objectID": "regression1.html#first-model-one-metric-predictor",
    "href": "regression1.html#first-model-one-metric-predictor",
    "title": "5  Modelling and regression",
    "section": "5.5 First model: one metric predictor",
    "text": "5.5 First model: one metric predictor\nFirst, let’s load some data:\n\ndata(mtcars)\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\n5.5.1 Frequentist\nDefine and fit the model:\n\nlm1_freq &lt;- lm(mpg ~ hp, data = mtcars)\n\nGet the parameter values:\n\nparameters(lm1_freq)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n30.0988605\n1.6339210\n0.95\n26.7619488\n33.4357723\n18.421246\n30\n0e+00\n\n\nhp\n-0.0682283\n0.0101193\n0.95\n-0.0888947\n-0.0475619\n-6.742388\n30\n2e-07\n\n\n\n\n\n\nPlot the model parameters:\n\nplot(parameters(lm1_freq))\n\n\n\n\n\n\n\n\n\n\n5.5.2 Bayesian\n\nlm1_bayes &lt;- stan_glm(mpg ~ hp, data = mtcars)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 8.9e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.89 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.056403 seconds (Warm-up)\nChain 1:                0.062497 seconds (Sampling)\nChain 1:                0.1189 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.059504 seconds (Warm-up)\nChain 2:                0.052776 seconds (Sampling)\nChain 2:                0.11228 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.5e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.060164 seconds (Warm-up)\nChain 3:                0.055246 seconds (Sampling)\nChain 3:                0.11541 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.058865 seconds (Warm-up)\nChain 4:                0.053954 seconds (Sampling)\nChain 4:                0.112819 seconds (Total)\nChain 4: \n\n\nActually, we want to suppress some overly verbose output of the sampling, so add the argument refresh = 0:\n\nlm1_bayes &lt;- stan_glm(mpg ~ hp, data = mtcars, refresh = 0)\n\nGet the parameter values:\n\nparameters(lm1_bayes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\nCI\nCI_low\nCI_high\npd\nRhat\nESS\nPrior_Distribution\nPrior_Location\nPrior_Scale\n\n\n\n\n(Intercept)\n30.01923\n0.95\n26.7517738\n33.2576098\n1\n0.9993350\n3839.400\nnormal\n20.09062\n15.0673701\n\n\nhp\n-0.06770\n0.95\n-0.0884016\n-0.0470858\n1\n0.9995484\n3453.765\nnormal\n0.00000\n0.2197599\n\n\n\n\n\n\nPlot the model parameters:\n\nplot(parameters(lm1_bayes))\n\n\n\n\n\n\n\n\n\n\n5.5.3 Model performance\n\nr2(lm1_freq)\n\n# R2 for Linear Regression\n       R2: 0.602\n  adj. R2: 0.589\n\n\n\nr2(lm1_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.584 (95% CI [0.370, 0.745])\n\n\n\n\n5.5.4 Model check\nHere’s a bunch of typical model checks in the Frequentist sense.\n\ncheck_model(lm1_freq)\n\n\n\n\n\n\n\n\nAnd here are some Bayesian flavored model checks.\n\ncheck_model(lm1_bayes)\n\n\n\n\n\n\n\n\n\n\n5.5.5 Get some predictions\n\nlm1_pred &lt;- estimate_relation(lm1_freq)\nlm1_pred\n\n\n\n\n\nhp\nPredicted\nSE\nCI_low\nCI_high\n\n\n\n\n52.00000\n26.550990\n1.1766139\n24.148024\n28.95396\n\n\n83.44444\n24.405590\n0.9358933\n22.494241\n26.31694\n\n\n114.88889\n22.260189\n0.7548971\n20.718484\n23.80190\n\n\n146.33333\n20.114789\n0.6828911\n18.720139\n21.50944\n\n\n177.77778\n17.969389\n0.7518697\n16.433866\n19.50491\n\n\n209.22222\n15.823989\n0.9310065\n13.922620\n17.72536\n\n\n240.66667\n13.678588\n1.1707841\n11.287528\n16.06965\n\n\n272.11111\n11.533188\n1.4412478\n8.589767\n14.47661\n\n\n303.55556\n9.387788\n1.7280486\n5.858642\n12.91693\n\n\n335.00000\n7.242387\n2.0242544\n3.108308\n11.37647\n\n\n\n\n\n\nMore details on the above function can be found on the respective page at the easystats site.\n\n\n5.5.6 Plot the model\n\nplot(lm1_pred)"
  },
  {
    "objectID": "regression1.html#more-of-this",
    "href": "regression1.html#more-of-this",
    "title": "5  Modelling and regression",
    "section": "5.6 More of this",
    "text": "5.6 More of this\nMore technical details for gauging model performance and model quality, can be found on the site of the R package “performance at the easystats site."
  },
  {
    "objectID": "regression1.html#lab",
    "href": "regression1.html#lab",
    "title": "5  Modelling and regression",
    "section": "5.7 Lab",
    "text": "5.7 Lab\nRun a simple regression on your own research data. Present the results. Did you encounter any glitches?"
  },
  {
    "objectID": "regression1.html#bayes-members-only",
    "href": "regression1.html#bayes-members-only",
    "title": "5  Modelling and regression",
    "section": "5.8 Bayes-members only",
    "text": "5.8 Bayes-members only\nBayes statistics provide a distribution as the result of the analysis, the posterior distribution, which provides us with quite some luxury.\nAs the posterior distribution manifests itself by a number of samples, we can easily filter and manipulate this sample distribution in order to ask some interesing questions.\nSee\n\nlm1_bayes_tibble &lt;- as_tibble(lm1_bayes)  # cast as a tibble (table)\n\nhead(lm1_bayes_tibble)  # show the first few rows\n\n\n\n\n\n(Intercept)\nhp\nsigma\n\n\n\n\n31.13917\n-0.0711852\n3.197711\n\n\n28.75411\n-0.0636610\n4.671171\n\n\n29.39286\n-0.0665289\n4.114371\n\n\n29.96161\n-0.0651294\n4.548393\n\n\n31.62700\n-0.0701204\n3.804601\n\n\n28.14868\n-0.0626511\n4.102010\n\n\n\n\n\n\n\n5.8.1 Asking for probabilites\nWhat’s the probability that the effect of hp is negative?\n\nlm1_bayes_tibble %&gt;% \n  count(hp &lt; 0)\n\n\n\n\n\nhp &lt; 0\nn\n\n\n\n\nTRUE\n4000\n\n\n\n\n\n\nFeel free to ask similar questions!\n\n\n5.8.2 Asking for quantiles\nWith a given probability of, say 90%, how large is the effect of hp?\n\nquantile(lm1_bayes_tibble$hp, .9)\n\n        90% \n-0.05448312 \n\n\nWhat’s the smallest 95% percent interval for the effect of hp?\n\nhdi(lm1_bayes)\n\n\n\n\n\nParameter\nCI\nCI_low\nCI_high\nEffects\nComponent\n\n\n\n\n(Intercept)\n0.95\n26.817785\n33.2873498\nfixed\nconditional\n\n\nhp\n0.95\n-0.088532\n-0.0475954\nfixed\nconditional\n\n\n\n\n\n\nIn case you prefer 89% intervals (I do!):\n\nhdi(lm1_bayes, ci = .89)\n\n\n\n\n\nParameter\nCI\nCI_low\nCI_high\nEffects\nComponent\n\n\n\n\n(Intercept)\n0.89\n27.3732054\n32.6197653\nfixed\nconditional\n\n\nhp\n0.89\n-0.0832743\n-0.0491476\nfixed\nconditional\n\n\n\n\n\n\n\n\n5.8.3 Model specification\nIn Bayes statistics, it is customary to specify the model in something like the following way:\n$$\n\\[\\begin{aligned}\n\ny_i &\\sim N(\\mu_i,\\sigma)\\\\\n\\mu_i &= \\beta_0 + \\beta_1 x_i\\\\\n\\beta_0, \\beta_1 &\\sim N(0, 1) \\\\\n\\sigma &\\sim E(1)\n\\end{aligned}\\]\n$$\nIn this specification, \\(N\\) refers to the normal distribution, and \\(E\\) to the exponential distribution. Furthermore, this model assumes that the \\(X\\) and \\(Y\\) are given in standard units.\n\n\n5.8.4 Prediction interval\nA prediction interval answers the following question\n\nHow large is the uncertainty in \\(y\\) associated with a given obersation? What interval of values should I expect for a randomly chosen observation?\n\nFor example, what’s the uncertainty attached to the fuel economy of a car with 100 hp?\n\nestimate_prediction(model = lm1_bayes, \n                    data = tibble(hp = 100) )\n\n\n\n\n\nhp\nPredicted\nSE\nCI_low\nCI_high\n\n\n\n\n100\n23.35222\n4.094458\n15.53018\n31.2594\n\n\n\n\n\n\n\n\n5.8.5 … And more\nWe could even ask intriguing questions such as\n\nGiven the model, and given two random observations, one from the experimental group and one from the control group, what is the probability that observation 1 has a higher value in \\(Y\\) than observation 2 has?\n\nNote that we are not only asking for “typical” observations as predicted by the model but we are also taking into account the uncertainty of the prediction for each group. Hence, this kind of questions is likely to yield more realistic (and less clear-cut) answers than just asking for the typical value. In other words, such analyses draw on the posterior predictive distribution."
  },
  {
    "objectID": "regression1.html#multiple-metric-predictors",
    "href": "regression1.html#multiple-metric-predictors",
    "title": "5  Modelling and regression",
    "section": "5.9 Multiple metric predictors",
    "text": "5.9 Multiple metric predictors\nAssume we have a theory that dictates that fuel economy is a (causal) function of horse power and engine displacement.\n\nlm2_freq &lt;- lm(mpg ~ hp + disp, data = mtcars)\nparameters(lm2_freq)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n30.7359042\n1.3315661\n0.95\n28.0125457\n33.4592628\n23.082522\n29\n0.0000000\n\n\nhp\n-0.0248401\n0.0133855\n0.95\n-0.0522165\n0.0025363\n-1.855746\n29\n0.0736791\n\n\ndisp\n-0.0303463\n0.0074049\n0.95\n-0.0454909\n-0.0152016\n-4.098159\n29\n0.0003063\n\n\n\n\n\n\nSimilarly for Bayes inference:\n\nlm2_bayes &lt;- stan_glm(mpg ~ hp + disp, data = mtcars)\n\nResults\n\nparameters(lm2_bayes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\nCI\nCI_low\nCI_high\npd\nRhat\nESS\nPrior_Distribution\nPrior_Location\nPrior_Scale\n\n\n\n\n(Intercept)\n30.7505061\n0.95\n28.0524917\n33.5248323\n1.00000\n0.9994184\n4416.682\nnormal\n20.09062\n15.0673701\n\n\nhp\n-0.0251499\n0.95\n-0.0526039\n0.0024292\n0.96625\n1.0006908\n2181.434\nnormal\n0.00000\n0.2197599\n\n\ndisp\n-0.0301997\n0.95\n-0.0455587\n-0.0150872\n0.99975\n1.0016436\n2213.671\nnormal\n0.00000\n0.1215712\n\n\n\n\n\nplot(parameters(lm2_bayes))\n\n\n\n\n\n\n\nr2(lm2_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.731 (95% CI [0.573, 0.839])\n\n\nDepending on the value of disp the prediction of mpg from hp will vary:\n\nlm2_pred &lt;- estimate_relation(lm2_freq)\nplot(lm2_pred)"
  },
  {
    "objectID": "regression1.html#one-nominal-predictor",
    "href": "regression1.html#one-nominal-predictor",
    "title": "5  Modelling and regression",
    "section": "5.10 One nominal predictor",
    "text": "5.10 One nominal predictor\n\nlm3a &lt;- lm(mpg ~ am, data = mtcars)\nparameters(lm3a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n17.147368\n1.124602\n0.95\n14.85062\n19.44411\n15.247492\n30\n0.000000\n\n\nam\n7.244939\n1.764422\n0.95\n3.64151\n10.84837\n4.106127\n30\n0.000285\n\n\n\n\n\n\n\nlm3a_means &lt;- estimate_means(lm3a, at = \"am = c(0, 1)\")\nlm3a_means \n\n\n\n\n\nam\nMean\nSE\nCI_low\nCI_high\n\n\n\n\n0\n17.14737\n1.124602\n14.85062\n19.44411\n\n\n1\n24.39231\n1.359578\n21.61568\n27.16894\n\n\n\n\n\n\nIf we were not to specify the values of am which we would like to get predictions for, the default of the function would select 10 values, spread across the range of am. For numeric variables, this is usually fine. However, for nominal variables - and am is in fact a nominally scaled variable - we insist that we want predictions for the levels of the variable only, that is for 0 and 1.\nHowever, unfortunately, the plot needs a nominal variable if we are to compare groups. In our case, am is considered a numeric variables, since it consists of numbers only. The plot does not work, malheureusement:\n\nplot(lm3a_means)\n\nError in `rlang::sym()`:\n! Can't convert a character vector to a symbol.\n\n\nWe need to transform am to a factor variable. That’s something like a string. If we hand over a factor() to the plotting function, everything will run smoothly. Computationwise, no big differences:\n\nmtcars2 &lt;-\n  mtcars %&gt;% \n  mutate(am_f = factor(am))\n\nlm3a &lt;- lm(mpg ~ am_f, data = mtcars2)\nparameters(lm3a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n17.147368\n1.124602\n0.95\n14.85062\n19.44411\n15.247492\n30\n0.000000\n\n\nam_f1\n7.244939\n1.764422\n0.95\n3.64151\n10.84837\n4.106127\n30\n0.000285\n\n\n\n\n\n\n\nlm3a_means &lt;- estimate_means(lm3a)\nplot(lm3a_means)\n\n\n\n\n\n\n\n\nNote that we should have converted am to a factor variable before fitting the model. Otherwise, the plot won’t work.\nHere’s a more hand-crafted version of the last plot, see Fig. Figure 5.6.\n\nggplot(mtcars2) +\n  aes(x = am_f, y = mpg) +\n  geom_violin() +\n  geom_jitter(width = .1, alpha = .5) +\n  geom_pointrange(data = lm3a_means,\n                  color = \"orange\",\n                  aes(ymin = CI_low, ymax = CI_high, y = Mean)) +\n  geom_line(data = lm3a_means, aes(y = Mean, group = 1))\n\n\n\n\nFigure 5.6: Means per level of am"
  },
  {
    "objectID": "regression1.html#one-metric-and-one-nominal-predictor",
    "href": "regression1.html#one-metric-and-one-nominal-predictor",
    "title": "5  Modelling and regression",
    "section": "5.11 One metric and one nominal predictor",
    "text": "5.11 One metric and one nominal predictor\n\nmtcars2 &lt;-\n  mtcars %&gt;% \n  mutate(cyl = factor(cyl))\n\nlm4 &lt;- lm(mpg ~ hp + cyl, data = mtcars2)\nparameters(lm4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n28.6501182\n1.5877870\n0.95\n25.3976840\n31.9025524\n18.044056\n28\n0.0000000\n\n\nhp\n-0.0240388\n0.0154079\n0.95\n-0.0556005\n0.0075228\n-1.560163\n28\n0.1299540\n\n\ncyl6\n-5.9676551\n1.6392776\n0.95\n-9.3255631\n-2.6097471\n-3.640418\n28\n0.0010921\n\n\ncyl8\n-8.5208508\n2.3260749\n0.95\n-13.2855993\n-3.7561022\n-3.663188\n28\n0.0010286\n\n\n\n\n\n\n\nlm4_pred &lt;- estimate_relation(lm4)\nplot(lm4_pred)"
  },
  {
    "objectID": "regression1.html#watch-out-for-simpson",
    "href": "regression1.html#watch-out-for-simpson",
    "title": "5  Modelling and regression",
    "section": "5.12 Watch out for Simpson",
    "text": "5.12 Watch out for Simpson\nBeware! Model estimates can swing wildly if you add (or remove) some predictor from your model. See this post for an demonstration."
  },
  {
    "objectID": "regression1.html#what-about-correlation",
    "href": "regression1.html#what-about-correlation",
    "title": "5  Modelling and regression",
    "section": "5.13 What about correlation?",
    "text": "5.13 What about correlation?\nCorrelation is really a close cousin to regression. In fact, regression with standardized variables amounts to correlation.\nLet’s get the correlation matrix of the variables in involved in lm4.\n\nlm4_corr &lt;- \n  mtcars %&gt;% \n  select(mpg, hp, disp) %&gt;% \n  correlation()\n\nlm4_corr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\nt\ndf_error\np\nMethod\nn_Obs\n\n\n\n\nmpg\nhp\n-0.7761684\n0.95\n-0.8852686\n-0.5860994\n-6.742388\n30\n2e-07\nPearson correlation\n32\n\n\nmpg\ndisp\n-0.8475514\n0.95\n-0.9233594\n-0.7081376\n-8.747151\n30\n0e+00\nPearson correlation\n32\n\n\nhp\ndisp\n0.7909486\n0.95\n0.6106794\n0.8932775\n7.080122\n30\n1e-07\nPearson correlation\n32\n\n\n\n\n\n\n\nplot(summary(lm4_corr))"
  },
  {
    "objectID": "regression1.html#exercises",
    "href": "regression1.html#exercises",
    "title": "5  Modelling and regression",
    "section": "5.14 Exercises",
    "text": "5.14 Exercises\n\nmtcars simple 1\nmtcars simple 2\nmtcars simple 3"
  },
  {
    "objectID": "regression1.html#lab-1",
    "href": "regression1.html#lab-1",
    "title": "5  Modelling and regression",
    "section": "5.15 Lab",
    "text": "5.15 Lab\nGet your own data, and build a simple model reflecting your research hypothesis. If you are lacking data (or hypothesis) get something close to it."
  },
  {
    "objectID": "regression1.html#literature",
    "href": "regression1.html#literature",
    "title": "5  Modelling and regression",
    "section": "5.16 Literature",
    "text": "5.16 Literature\nAn accessible treatment of regression is provided by Ismay and Kim (2020).\nRoback and Legler (2021) provide a more than introductory account of regression while being accessible. A recent but already classic book (if this is possible) is the book by Gelman, Hill, and Vehtari (2021). You may also benefit from Poldrack (2022) (open access)."
  },
  {
    "objectID": "regression1.html#debrief",
    "href": "regression1.html#debrief",
    "title": "5  Modelling and regression",
    "section": "5.17 Debrief",
    "text": "5.17 Debrief\nScience!\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge: Cambridge University Press.\n\n\nIsmay, Chester, and Albert Young-Sun Kim. 2020. Statistical Inference via Data Science: A ModernDive into r and the Tidyverse. Chapman & Hall/CRC the r Series. Boca Raton: CRC Press / Taylor & Francis Group. https://moderndive.com/.\n\n\nPoldrack, Russell. 2022. Statistical Thinking for the 21st Century. https://statsthinking21.github.io/statsthinking21-core-site/index.html.\n\n\nRoback, Paul, and Julie Legler. 2021. Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in. 1st ed. Chapman and Hall Texts in Statistical Science. Boca Raton: CRC Press."
  },
  {
    "objectID": "regression2.html#r-packages-needed",
    "href": "regression2.html#r-packages-needed",
    "title": "6  More linear models",
    "section": "6.1 R-packages needed",
    "text": "6.1 R-packages needed"
  },
  {
    "objectID": "regression2.html#r-packages-needed-for-this-chapter",
    "href": "regression2.html#r-packages-needed-for-this-chapter",
    "title": "6  More linear models",
    "section": "6.2 R packages needed for this chapter",
    "text": "6.2 R packages needed for this chapter\n\nlibrary(easystats)\nlibrary(tidyverse)"
  },
  {
    "objectID": "regression2.html#multiplicative-associations",
    "href": "regression2.html#multiplicative-associations",
    "title": "6  More linear models",
    "section": "6.3 Multiplicative associations",
    "text": "6.3 Multiplicative associations\n\n6.3.1 The Log-Y model\nConsider again the linear model, in a simple form:\n\\[\\hat{y} = \\beta_0 + \\beta_1 x_1 +  \\ldots + b_kx_k +\\] Surprisingly, we can use this linear model to describe multiplicative assocations:\n\\(\\hat{y} = e^{b_0 + b_1x_1 + b_2x_2 + \\ldots + b_kx_k}\\)\n(I wrote b instead of \\(\\beta\\) just to show that both has its meaning, but are separate things.)\nExponentiate both sides to get:\n\\(log (\\hat{y}) = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_kx_k\\)\nFor simplicity, let’s drop the subscripts in the following without loss of generality and keep it short:\n\\(y = e^{x}\\), with \\(e \\approx 2.71...\\)\nExponentiate both sides to get:\n\\(log(y) = x\\)\nThis association is called multiplicative, because if x increases by 1, y increased by a constant factor.\n\n\n\n\n\n\nNote\n\n\n\nThe logarithm is not defined for negative (input) values. And \\(log(0) = -\\infty\\).\n\n\nA side-effect of modelling log_y instead of y is that the distribution shape of the outcome variable changes. This can be useful at times.\nLog-Y Regression can usefully be employed for modelling growth, among othrs, see Example 6.1.\n\nExample 6.1 (Bacteria growth) Some bacteria dish grows with at a fixed proportional rate, that is it doubles its population size in a fixed period of time. This is what is called exponential growth. For concreteness, say, the bacteriae double each two days, starting with 1 unit of bacteria.\nAfter about three weeks, i.e., 10 doubling periods (20 days), we’ll have \\(y\\) units of bacteriae:\n\ne &lt;- 2.7178\ny &lt;- e^10\ny\n\n[1] 21987.45\n\n\nQuite a bit! More than 20 thousand times more than before.\n\n\n\n6.3.2 Exercise\n\nEffect of education on income\nEffect of log-y transformation on the distribution, an example\n\n\n\n\n\n\n\nNote\n\n\n\nThe exercises are written in German Language. Don’t fret. Browsers are able to translate websites instantaneously. Alternatively, go to sites such as Google Translate and enter the URL of the website to be translated. Also check out the webstor of your favorite browser to get an extention such as this one for Google Chrome.\n\n\n\n\n6.3.3 Visualizing Log Transformation\nCheck out this post for an example of a log-y regression visualized.\nThis post puts some more weight to the argument that a log-y transformation is useful (if you want to model multiplicative relations).\n\n\n6.3.4 Further reading\nCheck out this great essay by Kenneth Benoit on different log-variants in regression. Also Gelman, Hill, and Vehtari (2021), chapter 12 (and others), is useful."
  },
  {
    "objectID": "regression2.html#interaction",
    "href": "regression2.html#interaction",
    "title": "6  More linear models",
    "section": "6.4 Interaction",
    "text": "6.4 Interaction\n\n6.4.1 Multiple predictors, no interaction\nRegression analyses can be used with more than one predictor, see Figure Figure 6.1.\n\n\n\n\n\nflowchart LR\nX --&gt; Y1\n\nX1 --&gt; Y2\nX2 --&gt; Y2\n\n\nFigure 6.1: One predictor (X) vs. two predictors (X1, X2)\n\n\n\n\ngiven by Figure ?fig-3dregr2, where a 3D account of a regression is given. 3D means to input variables, and (which is always the case) one output variable.\n\n\n\n\n\n\nNote\n\n\n\nNote that the slope in linear in both axis (X1 and X2).\n\n\nA different perspective is shown here,\nwhere a 3D account of a regression is given. 3D means to input variables, and (which is always the case) one output variable.\n\n\n\n\n\n\nImportant\n\n\n\nIf the slope for one predictor is the same for all values of the other predictor, then we say that no interaction is taking place.\n\n\nHere’s a visualization of a 3D regression plane (not line) without interaction: constant slope in one axis, see the following figure, ?fig-3dregr2. The three cubes show the same data, just turned by different degrees (along the z axis).\n\n\n\n\n\nFigure 6.2: 3D regression plane (not line) without interaction\n\n\n\n\n\n\n\nFigure 6.3: 3D regression plane (not line) without interaction\n\n\n\n\n\n\n\nFigure 6.4: 3D regression plane (not line) without interaction\n\n\n\n\nNote that in the above figure, the slope in each predictor axis equals 1, boringly. Hence the according 2D plots are boring, too.\nFor the sake of an example, consider this linear model:\n\\(mpg \\sim hp + disp\\)\nOr, in more regression like terms:\n\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\), where x1 is hp and x2 is disp in the mtcars dataset.\nIn R terms:\n\nlm3d &lt;- lm(mpg ~ hp + disp, data = mtcars)\n\nThe 3D plot is shown in Figure Figure 6.5.\n\n\n\n\n\nFigure 6.5: mpg ~ hp + disp\n\n\n\n\nHere are the two corresponding 2d (1 predictor) regression models:\n\nlm1 &lt;- lm(mpg ~ hp, data = mtcars)\nplot(estimate_relation(lm1))\nlm2 &lt;- lm(mpg ~ disp, data = mtcars)\nplot(estimate_relation(lm2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheckout this post for a visually slightly more appealing 3d regression plane.\n\n\n6.4.2 Interaction\nFor interaction to happen we relax the assumption that the slope of predictor 1 must be constant for all values of predictor 2.\nIn R, we specify an interaction model like this:\n\nlm3d_interact &lt;- lm(mpg ~ hp + disp + hp:disp, data = mtcars)\n\nThe symbol hp:disp can be read as “the interaction effect of hp and disp”.\nHere’s a visual account, see Figure Figure 6.6.\n\n\n\n\n\nFigure 6.6: mpg ~ hp + disp\n\n\n\n\nCompare Figure 6.6 and Figure 6.5. The difference is the interaction effect.\nIn Figure 6.6 you’ll see that the lines along the Y axis are not parallel anymore. Similarly, the lines along the X axis are not parallel anymore.\n\n\n\n\n\n\nImportant\n\n\n\nIf the regression lines (indicating different values of one predictor) are not parallel, we say that an interaction effect is taking place.\n\n\nHowever, the difference or change between two adjacent values (lines) is constant. This value is the size the regression effect.\n\n\n6.4.3 Interaction made simple\nIf you find that two sophisticated, consider the following simple case.\nFirst, we mutate am to be a factor variable, in order to make things simpler (without loss of generality).\n\nmtcars2 &lt;-\n  mtcars %&gt;% \n  mutate(am_f = factor(am))\n\nNow we use this new variable for a simple regression model:\n\nlm_interact_simple &lt;- lm(mpg ~ disp + am_f + disp:am_f, data = mtcars2)\n\nHere’s the plot, Figure Figure 6.7.\n\nplot(estimate_relation(lm_interact_simple))\n\n\n\n\nFigure 6.7: A simple interaction model\n\n\n\n\nIn this picture, we see that the two regression lines are not parallel, and hence there is evidence of an interaction effect.\nThe interaction effect amounts to the difference in slops in Figure 6.7.\nOne might be inclined to interpret Figure Figure 6.7 as an 3D image, where the one (reddish) line is in the foreground and the blueish line in the background (or vice versa, as you like). Given a 3D image (and hence 2 predictors), we are where we started further above.\nFor completeness, here are the parameters of the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(28)\np\n\n\n\n\n(Intercept)\n25.16\n1.93\n(21.21, 29.10)\n13.07\n&lt; .001\n\n\ndisp\n-0.03\n6.22e-03\n(-0.04, -0.01)\n-4.44\n&lt; .001\n\n\nam f (1)\n7.71\n2.50\n(2.58, 12.84)\n3.08\n0.005\n\n\ndisp × am f (1)\n-0.03\n0.01\n(-0.05, -7.99e-03)\n-2.75\n0.010\n\n\n\n\n\n\n\n6.4.4 Centering variables\nThe effect of of am_f must be interpreted when disp is zero, which does not make much sense.\nTherefore it simplifies the interpretation of regression coefficients to center all input variables, by subtrating the mean value (“demeaning” or “centering”):\n\\[x' = x - \\bar{x}\\] In R, this can be achieved e.g,. in this way:\n\nmtcars3 &lt;- \nmtcars2 %&gt;% \n  mutate(disp_c = disp - mean(disp))\n\n\nlm_interact_simple2 &lt;- lm(mpg ~ disp_c + am_f + disp_c:am_f, data = mtcars3)\nparameters(lm_interact_simple2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n18.7929250\n0.7631321\n0.95\n17.2297199\n20.3561302\n24.6260457\n28\n0.0000000\n\n\ndisp_c\n-0.0275836\n0.0062190\n0.95\n-0.0403225\n-0.0148447\n-4.4354101\n28\n0.0001295\n\n\nam_f1\n0.4517578\n1.3915089\n0.95\n-2.3986189\n3.3021346\n0.3246532\n28\n0.7478567\n\n\ndisp_c:am_f1\n-0.0314548\n0.0114574\n0.95\n-0.0549242\n-0.0079855\n-2.7453781\n28\n0.0104373"
  },
  {
    "objectID": "regression2.html#predictor-relevance",
    "href": "regression2.html#predictor-relevance",
    "title": "6  More linear models",
    "section": "6.5 Predictor relevance",
    "text": "6.5 Predictor relevance\nGiven a model, we might want to know which predictor has the strongest association with the outcome?\nIn order to answer this question, all predictor must have the same scale. Otherwise the importance of a predictor would increase by 1000, if we multiply each of the observations’ values by the same factor. However, this multiplication should not change the relevance of a predictor.\nA simple solution is to standardize all predictors to the same scale (sd=1).\n\nmtcars4 &lt;-\n  mtcars %&gt;% \n  standardize(select = c(\"disp\", \"hp\", \"cyl\"))\n\nBy the way, “standardizing” centers the variable by default to a mean value of zero (by demeaning).\nSee:\n\nhead(mtcars4$disp)\n\n[1] -0.57061982 -0.57061982 -0.99018209  0.22009369  1.04308123 -0.04616698\n\nhead(mtcars$disp)\n\n[1] 160 160 108 258 360 225\n\n\nHere’s the SD:\n\nsd(mtcars4$disp)\n\n[1] 1\n\nsd(mtcars$disp)\n\n[1] 123.9387\n\n\nAnd here’s the mean value:\n\nmean(mtcars4$disp)\n\n[1] -9.084937e-17\n\nmean(mtcars$disp)\n\n[1] 230.7219\n\n\nNow we are in a position to decide which predictor is more important:\n\nm &lt;- lm(mpg ~ disp + hp + cyl, data = mtcars4)\nparameters(m)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n20.090625\n0.5400989\n0.95\n18.984282\n21.1969675\n37.198045\n28\n0.0000000\n\n\ndisp\n-2.334768\n1.2894201\n0.95\n-4.976025\n0.3064896\n-1.810711\n28\n0.0809290\n\n\nhp\n-1.006457\n1.0045056\n0.95\n-3.064094\n1.0511791\n-1.001943\n28\n0.3249519\n\n\ncyl\n-2.192076\n1.4238730\n0.95\n-5.108747\n0.7245958\n-1.539516\n28\n0.1349044"
  },
  {
    "objectID": "regression2.html#exercises",
    "href": "regression2.html#exercises",
    "title": "6  More linear models",
    "section": "6.6 Exercises",
    "text": "6.6 Exercises\n\nPredictor relevance\nAdjusting\nAdjusting 2\nInterpreting Regression coefficients"
  },
  {
    "objectID": "regression2.html#lab",
    "href": "regression2.html#lab",
    "title": "6  More linear models",
    "section": "6.7 Lab",
    "text": "6.7 Lab\nGet your own data, and build a simple model reflecting your research hypothesis based on the topics covered in this chapter. If you are lacking data (or hypothesis) get something close to it."
  },
  {
    "objectID": "regression2.html#glimpse-on-parameter-estimation",
    "href": "regression2.html#glimpse-on-parameter-estimation",
    "title": "6  More linear models",
    "section": "6.8 Glimpse on parameter estimation",
    "text": "6.8 Glimpse on parameter estimation\nAn elegant yet simple explanation of the math of parameter estimation can be found at “go data driven”. A similar approach is presented here.\nConsider this geometric interpretation of the least square method in Figure Figure 6.8.\n\n\n\n\n\nFigure 6.8: Geometric interpretation of the least square method. Source: Oleg Alexandrov on Wikimedia"
  },
  {
    "objectID": "regression2.html#literatur",
    "href": "regression2.html#literatur",
    "title": "6  More linear models",
    "section": "6.9 Literatur",
    "text": "6.9 Literatur\nA recent but already classic book on regression and inference (if this is possible) is the book by Gelman, Hill, and Vehtari (2021). A great textbook on statistical modelling (with a Bayesian flavor) was written by McElreath (2020); it’s suitable for PhD level.\nMathematical foundations can be found in Deisenroth, Faisal, and Ong (2020). Here’s a collection of online resources tapping into statistics and machine learning.\n\n\n\n\nDeisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020. Mathematics for Machine Learning. Cambridge ; New York, NY: Cambridge University Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge: Cambridge University Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. 2nd ed. CRC Texts in Statistical Science. Boca Raton: Taylor; Francis, CRC Press."
  },
  {
    "objectID": "causality.html#r-packages-needed-for-this-chapter",
    "href": "causality.html#r-packages-needed-for-this-chapter",
    "title": "7  Causality",
    "section": "7.1 R packages needed for this chapter",
    "text": "7.1 R packages needed for this chapter\n\nlibrary(tidyverse)\nlibrary(ggdag)  # optional"
  },
  {
    "objectID": "causality.html#intro-to-causality",
    "href": "causality.html#intro-to-causality",
    "title": "7  Causality",
    "section": "7.2 Intro to causality",
    "text": "7.2 Intro to causality\nCheck out this talk."
  },
  {
    "objectID": "causality.html#literature",
    "href": "causality.html#literature",
    "title": "7  Causality",
    "section": "7.3 Literature",
    "text": "7.3 Literature\nRohrer (2018) provides an accessible introduction to causal inference. Slightly more advanced is the introduction by one of the leading figures of the Field, Judea Pearl, Pearl, Glymour, and Jewell (2016). If your after a text book on modelling that covers causal inference, and if you like Bayesian statistics, than you should definitely check out McElreath (2020).\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. 2nd ed. CRC Texts in Statistical Science. Boca Raton: Taylor; Francis, CRC Press.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. Chichester, West Sussex: Wiley.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629."
  },
  {
    "objectID": "casestudies.html#case-studies-on-data-visualization",
    "href": "casestudies.html#case-studies-on-data-visualization",
    "title": "8  Case studies",
    "section": "8.1 Case studies on data visualization",
    "text": "8.1 Case studies on data visualization\n\nFallstudien – NUR Datenvisualisierung\n\nvis-gapminder\nvis-penguins\nvis-mtcars\nAufgabe zur Datenvisualisierung des Diamantenpreises"
  },
  {
    "objectID": "casestudies.html#case-studies-on-explorative-data-analysis",
    "href": "casestudies.html#case-studies-on-explorative-data-analysis",
    "title": "8  Case studies",
    "section": "8.2 Case studies on explorative data analysis",
    "text": "8.2 Case studies on explorative data analysis\n\nFALLSTUDIEN - NUR EXPLORATIVE DATENANALYSE\n\nLouise E. Sinks: TidyTuesday Week 18: Portal Project\nLouise E. Sinks: TidyTuesday Week 17: London Marathon\nLouise E. Sinks: TidyTuesday Week 16: Neolithic Founder Crops\nDatenjudo mit Pinguinen\nData-Wrangling-Aufgaben zur Lebenserwartung\nCase study: data vizualization on flight delays using tidyverse tools\nFallstudie Flugverspätungen - EDA\nFallstudie zur EDA: Top-Gear\nFallstudie zur EDA: OECD-Wellbeing-Studie\nFallstudie zur EDA: Movie Rating\nFallstudie zur EDA: Women in Parliament\nFinde den Tag mit den meisten Flugverspätungen, Datensatz ‘nycflights13’\nCleaning and visualizing genomic data: a case study in tidy analysis\nTidyverse Case Study: Exploring the Billboard Charts\nAnalyse einiger RKI-Coronadaten: Eine reproduzierbare Fallstudie\nOpenCaseStudies - Health Expenditure\nOpen Case Studies: School Shootings in the United States - includes dashboards\nOpen Case Studies: Disparities in Youth Disconnection\nYACSDA Seitensprünge\nThe Open Case Study Search provides a nice collection of helpful case studies.\nifes@FOM Fallstudienseite"
  },
  {
    "objectID": "casestudies.html#case-studies-on-linear-modesl",
    "href": "casestudies.html#case-studies-on-linear-modesl",
    "title": "8  Case studies",
    "section": "8.3 Case studies on linear modesl",
    "text": "8.3 Case studies on linear modesl\n\nFALLSTUDIEN - NUR LINEARE MODELLE\n\nBeispiel für Prognosemodellierung 1, grundlegender Anspruch, Video\nBeispiel für Ihre Prognosemodellierung 2, mittlerer Anspruch\nBeispiel für Ihre Prognosemodellierung 3, hoher Anspruch\nFallstudie: Modellierung von Flugverspätungen\nModelling movie successes: linear regression\nMovies\nFallstudie Einfache lineare Regression in Base-R, Anfängerniveau, Kaggle-Competition TMDB\nFallstudie Sprit sparen\nFallstudie zum Beitrag verschiedener Werbeformate zum Umsatz; eine Fallstudie in Python, aber mit etwas Erfahrung wird man den Code einfach in R umsetzen können (wenn man nicht in Python schreiben will)\nPractical Linear Regression with R: A case study on diamond prices\nCase Study: Italian restaurants in NYC\nVorhersage-Modellierung des Preises von Diamanten\nModellierung Diamantenpreis 2"
  },
  {
    "objectID": "casestudies.html#case-studies-on-machine-learning-using-tidymodels",
    "href": "casestudies.html#case-studies-on-machine-learning-using-tidymodels",
    "title": "8  Case studies",
    "section": "8.4 Case studies on machine learning using tidymodels",
    "text": "8.4 Case studies on machine learning using tidymodels\n\nFALLSTUDIEN - MASCHINELLES LERNEN MIT TIDYMODELS\n\nLouise E. Sinks: One Class SVM\nLouise E. Sinks: Credit Card Fraud: A Tidymodels Tutorial - inklusive Workflow-Sets\nExperimenting with machine learning in R with tidymodels and the Kaggle titanic dataset\nTutorial on tidymodels for Machine Learning\nClassification with Tidymodels, Workflows and Recipes\nA (mostly!) tidyverse tour of the Titanic\nPersonalised Medicine - EDA with tidy R\nTidy TitaRnic\nFallstudie Seegurken\nSehr einfache Fallstudie zur Modellierung einer Regression mit tidymodels\nFallstudie zur linearen Regression mit Tidymodels\nAnalyse zum Verlauf von Covid-Fällen\nFallstudie zur Modellierung einer logististischen Regression mit tidymodels\nFallstudie zu Vulkanausbrüchen (Resampling and kein Tuning)\nFallstudie Himalaya (Resampling and kein Tuning)\nFallstudien zu Studiengebühren\n1. Modell der Fallstudie Hotel Bookings\nAufgaben zur logistischen Regression, PDF\nFallstudie Oregon Schools\nFallstudie Windturbinen\nFallstudie Churn\nEinfache Durchführung eines Modellierung mit XGBoost\nFallstudie Oregon Schools\nFallstudie Churn\nFallstudie Ikea\nFallstudie Wasserquellen in Sierra Leone\nFallstudie Bäume in San Francisco: Random Forest tunen\nFallstudie Vulkanausbrüche\nFallstudie Brettspiele mit XGBoost\nFallstudie Serie The Office: Lasso tunen\nFallstudie NBER Papers\nFallstudie Einfache lineare Regression mit Tidymodels, Kaggle-Competition TMDB\nFallstudie Einfaches Random-Forest-Modell mit Tidymodels, Kaggle-Competition TMDB\nFallstudie Workflow-Set mit Tidymodels, Kaggle-Competition TMDB\nFallstudie Titanic mit Tidymodels bei Kaggle\nEinfache Fallstudie mit Tidymodels bei Kaggle\nExploring the Star Wars “Prequel Renaissance” Using tidymodels and workflowsets\nClassification modelling workflow using tidymodels, Konstantinos Patelis\nTune xgboost models with early stopping to predict shelter animal status\nPredicting injuries for Chicago traffic crashes: Resampling BAG Tree\nUsing tidymodels to Predict Health Insurance Cost: Resmapling"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Deisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020.\nMathematics for Machine Learning. Cambridge ; New York,\nNY: Cambridge University Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and\nOther Stories. Analytical Methods for Social Research. Cambridge:\nCambridge University Press.\n\n\nGoodman, Steven. 2008. “A Dirty Dozen: Twelve p-Value\nMisconceptions.” Seminars in Hematology, Interpretation\nof quantitative research, 45 (3): 135–40. https://doi.org/10.1053/j.seminhematol.2008.04.003.\n\n\nHernán, Miguel A., John Hsu, and Brian Healy. 2019. “A Second\nChance to Get Causal Inference Right: A Classification of Data Science\nTasks.” Chance 32 (1): 42–49. https://doi.org/10.1080/09332480.2019.1579578.\n\n\nIsmay, Chester, and Albert Young-Sun Kim. 2020. Statistical\nInference via Data Science: A ModernDive into r and the\nTidyverse. Chapman & Hall/CRC the r Series. Boca\nRaton: CRC Press / Taylor & Francis Group. https://moderndive.com/.\n\n\nMacKay, R. J., and R. W. Oldford. 2000. “Scientific Method,\nStatistical Method and the Speed of Light.” Statistical\nScience 15 (3): 254–78. https://doi.org/10.1214/ss/1009212817.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in r and Stan. 2nd ed. CRC Texts in\nStatistical Science. Boca Raton: Taylor; Francis, CRC\nPress.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal\nInference in Statistics: A Primer. Chichester, West Sussex: Wiley.\n\n\nPoldrack, Russell. 2022. Statistical Thinking for the 21st\nCentury. https://statsthinking21.github.io/statsthinking21-core-site/index.html.\n\n\nRoback, Paul, and Julie Legler. 2021. Beyond Multiple Linear\nRegression: Applied Generalized Linear Models and Multilevel Models\nin. 1st ed. Chapman and Hall Texts in Statistical Science. Boca\nRaton: CRC Press.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and\nCausation: Graphical Causal Models for Observational Data.”\nAdvances in Methods and Practices in Psychological Science 1\n(1): 27–42. https://doi.org/10.1177/2515245917745629.\n\n\nSauer, Sebastian. 2019. Moderne Datenanalyse Mit r: Daten Einlesen,\nAufbereiten, Visualisieren Und Modellieren. 1. Auflage 2019.\nFOM-Edition. Wiesbaden: Springer. https://www.springer.com/de/book/9783658215866.\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The\nASA’s Statement on p-Values: Context, Process, and\nPurpose.” The American Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108.\n\n\nWasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. 2019.\n“Moving to a World Beyond ‘\np&lt;/i&gt; p &lt;\n0.05’.” The American Statistician 73 (March):\n1–19. https://doi.org/10.1080/00031305.2019.1583913.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science:\nVisualize, Model, Transform, Tidy, and Import Data. O’Reilly Media.\nhttps://r4ds.had.co.nz/index.html.\n\n\nWild, Chris J, and Maxine Pfannkuch. 1999. “Statistical Thinking\nin Empirical Enquiry.” International Statistical Review\n67 (3): 223–48."
  }
]