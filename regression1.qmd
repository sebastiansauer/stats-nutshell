# Regression basics



## Regression as the umbrella tool

![One regression](https://memegenerator.net/img/instances/86435221.jpg){width="50%"}


Alternatively, 
venture into the forest of statistical tests as [oultined eg here, at Uni Muenster](https://web.archive.org/web/20091029162244/http://www.wiwi.uni-muenster.de/ioeb/en/organisation/pfaff/stat_overview_table.html).


You may want to ponder on this image of a decision tree of which test to choose, see Figure @fig-choose-test.

![Choose your test carefully](img/choose-test.png){#fig-choose-test}




## Common statistical tests are linear models


As Jonas Kristoffer Lindel√∏v tells us,
we can formulate most statistical tests as a linear model, ie., a regression.


![Common statistical tests as linear models](https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.png)

## R-packages needed

```{r}
#| message: false
library(rstanarm)
library(tidyverse)
library(easystats)
```




## In all its glory


```{r}
#| message: false
#| echo: false
ggplot(mtcars) +
  aes(x = hp, y = mpg) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_minimal()
```


## First model: one metric predictor

First, let's load some data:

```{r}
data(mtcars)
glimpse(mtcars)
```


### Frequentist

Define and fit the model:

```{r}
lm1_freq <- lm(mpg ~ hp, data = mtcars)
```


Get the parameter values:

```{r}
parameters(lm1_freq)
```

Plot the model parameters:

```{r}
plot(parameters(lm1_freq))
```


### Bayesian



```{r}
lm1_bayes <- stan_glm(mpg ~ hp, data = mtcars, refresh = 0)
```

Actually, we want to suppress some overly verbose output, using `refresh = 0`:
```{r}
lm1_bayes <- stan_glm(mpg ~ hp, data = mtcars, refresh = 0)
```



Get the parameter values:

```{r}
parameters(lm1_bayes)
```

Plot the model parameters:

```{r}
plot(parameters(lm1_bayes))
```


### Model performance



```{r}
r2(lm1_freq)
```


```{r}
r2(lm1_bayes)
```



### Model check

```{r}
#| fit-width: 10
#| out-width: "100%"
#| fig-asp: 1
check_model(lm1_freq)
```


```{r}
#| fit-width: 10
#| out-width: "100%"
#| fig-asp: 1
check_model(lm1_bayes)
```


### Get some predictions

```{r}
lm1_pred <- estimate_relation(lm1_freq)
lm1_pred
```


More details on the above function can be found on the [respective page at the easystats site](https://easystats.github.io/modelbased/reference/estimate_expectation.html#functions-for-estimating-predicted-values-and-uncertainty).



### Plot the model


```{r}
plot(lm1_pred)
```


## More of this

More technical details for gauging model performance and model quality,
can be found on the site of [the R package "performance](https://easystats.github.io/performance/) at the easystats site.



## Multiple metric predictors

Assume we have a theory that dictates that fuel economy is a (causal) function of horse power and engine displacement.

```{r}
lm2_freq <- lm(mpg ~ hp + disp, data = mtcars)
parameters(lm2_freq)
```


Similarly for Bayes inference:

```{r}
#| results: hide
lm2_bayes <- stan_glm(mpg ~ hp + disp, data = mtcars)
```

Results
```{r}
parameters(lm2_bayes)
plot(parameters(lm2_bayes))
r2(lm2_bayes)
```



Depending on the value of `disp` the prediction of `mpg` from `hp` will vary:

```{r}
lm2_pred <- estimate_relation(lm2_freq)
plot(lm2_pred)
```



## One nominal predictor


```{r}
mtcars2 <-
  mtcars %>% 
  mutate(am_f = factor(am))

lm3a <- lm(mpg ~ am_f, data = mtcars2)
parameters(lm3a)
```




```{r}
lm3a_means <- estimate_means(lm3a, at = "am_f")
lm3a_means 
```

```{r}
plot(lm3a_means)
```
Note that we should have converted `am` to a factor variable before fitting the model.
Otherwise, the plot won't work.


Here's a more hand-crafted version of the last plot:


```{r}
ggplot(mtcars2) +
  aes(x = am_f, y = mpg) +
  geom_violin() +
  geom_jitter(width = .1, alpha = .5) +
  geom_pointrange(data = lm3a_means,
                  color = "orange",
                  aes(ymin = CI_low, ymax = CI_high, y = Mean)) +
  geom_line(data = lm3a_means, aes(y = Mean, group = 1))
```


## One metric and one nominal predictor


```{r}
mtcars2 <-
  mtcars %>% 
  mutate(cyl = factor(cyl))

lm4 <- lm(mpg ~ hp + cyl, data = mtcars2)
parameters(lm4)
```


```{r}
lm4_pred <- estimate_relation(lm4)
plot(lm4_pred)
```


## What about correlation?


Correlation is really a close cousin to regression. In fact, regression with standardized variables amounts to correlation.

Let's get the correlation matrix of the variables in involved in `lm4`.

```{r}
lm4_corr <- 
  mtcars %>% 
  select(mpg, hp, disp) %>% 
  correlation()

lm4_corr
```



```{r}
plot(summary(lm4_corr))
```




## Exercises

1. [mtcars simple 1](https://datenwerk.netlify.app/post/mtcars-simple1/mtcars-simple1/)
1. [mtcars simple 2](https://datenwerk.netlify.app/post/mtcars-simple2/mtcars-simple2/)
1. [mtcars simple 3](https://datenwerk.netlify.app/post/mtcars-simple3/mtcars-simple3/)


## Lab

Get your own data, and build a simple model reflecting your research hypothesis. If you are lacking data (or hypothesis) get something close to it.







