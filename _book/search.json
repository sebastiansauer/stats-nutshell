[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "stats-nutshell",
    "section": "",
    "text": "This is an introductory course on statistical modelling. Welcome!\nThe focus of this course is on how to specify a theoretical idea (possibly vague) in a testable statistical model.\n\n\n\nAnalyzing research data can broadly be classified in three parts: explorative data analysis, modeling (including inference), and visualization. Either part is pivotal in its own right, but it can be argued that modeling is at the core of the scientific endeavor. However, in practice, modeling, visualization, and data exploration is heavily intertwined, so that three parts may be recognized (as individual entities) but not usefully separated from each other. This idea provides the rationale of this course: Data exploration, data visualization and data modeling is discussed as an integrated framework.\nThe focus is on practical data analysis; theoretical concepts are, where mentioned, second class citizens due to time constraints and the didactic aims of the course.\nFor example, statistical inference – such as p-values and confidence intervals – are not more than touched briefly, as the instructor believes that modeling, not inference, is of prime importance for the auditorium.\nWe will use the R environment for all computations (freely available). Please bring your own Laptop with R and RStudio installed (installation guides are provided). Data and R code will be provided.\n\n\n\nThe course is set-up as a “crash course” which indicates that we’ll rather try to cover a breadth of steps rather than digging deep at certain particular points. The rationale of this approach is that before digging deep, it is necessary to gain an overview of the territory. In addition, if one particular topic is not of interest to a given student (perhaps to difficult/simple), not much time is lost.\nBe warned! Compare this crash course to a dancing crash course right before your wedding: A lot can be achieved by such a course in some instances, or rather, the worst consequences (of not knowing how to dance) may be fenced off, but one should not expect to be a dancing queen (king) thereafter.\n\n\n\nModels and modeling are of pivotal importance in many sciences, not only for providing an explanation of nature en miniature (theoretical models), but also for gauging how closely the empirical data at hand match the theoretical model. Translating a theoretical model into statistical language is called statistical modeling and provides the guiding principle in this introductory course. Regression models will be presented as a lingua franca of statistical modeling, and we will learn that many empirical questions can (comfortably) be analyzed using a regression framework. Depending on the background and aims of the participants (and time permitting), we will shed light on some standard topics such as model comparison, classification models, and typical pitfalls. Given a more advanced auditorium, we may want to explore how causal and non-causal associations can be translated and tested using simple linear statistical models. Foundational ideas of statistical modeling will be accompanied by short examples and case studies to facilitate transfer and practical application after the course.\n\n\n\nBasic computer usage knowledge is needed (downloading materials from the internet, operating a PC, etc). Basic R knowledge is needed. Basic knowledge of statistical concepts (such as descriptive statistics) is needed. Willingness to learn is essential.\n\n\n\nUpon successful completion of this course, students should be able to:\n\nselect the right statistical visualization for a variety of data contexts\n“crunch” or “wrangle” data\nexplain what statistical modeling means\nformulate basic statistical models\ndifferentiate between predictive and explanatory modeling\napply the methods to own datasets\n\n\n\n\nThis course builds on the freely available e-book ModernDive. Each topic is paralleled by an ackompagnying chapter from ModernDive. A hard copy can be purchased here. The book is for sale in print here.\n\n\n\nThis course can be presented as a one-day seminar or split-up in four blocks.\nThe course can be held in English or German.\nPlease bring your own computer and read the notes regarding course logistics in advance. Note that some upfront preparation is needed from the learners.\nR and RStudio1 will be needed throughout the course. Please make sure that the IT is running. In case of technical difficulties with R feel free to use RStudio Cloud; free plans are available.\nAll learning materials (such as literature, code, data) will be provided in electronic format.\n\n\n\n\nInstall R and RStudio, see ModernDive Chap. 1.1. In case you have your R running on your system, please make sure that you’re uptodate. If outdated, download and install the most recent versions of the software. Similarly, hit the “Update” button in RStudio’s “Packages” tab to update your packages if you have not done so for a couple of months.\nInstall the necessary R packages as used in the book chapters covered in this course (see the sections on “Needed packages” in each chapter). If in doubt, see here the instructions on how to install R packages. Here’s the actual list on the R packages we’ll need.\nStudents new to R are advised to learn the basics, see ModernDive, Chap 1.2 - 1.5\nSign-up to RStudio Cloud\nBring your own laptop\n\nWith R and RStudio installed\nBrowse to the textbook website for this course\n\nMake sure your internet connection is stable and your loudspeaker/headset is working; a webcam is helpful.\nStudents are advised to review the course materials after each session.\nI recommend that you carefully check the course description to make sure the course fits your needs (not too advanced/basic).\n\n\n\n\nThis course can rather be considered a workshop in the sense that the instructor uses a dialogue-based approach to teaching and that there are numerous exercises during the course. Instead of providing long talks to the students, the instructor feels obligated to engage students in back-and-forth conversations. Similarly, the presentation of a large number of Powerpoint slide is avoided. Instead, a thorough course literature is available (free online), so that students will have no barrier in diving deeply into the materials and ideas presented. However, during class it is more important to transmit the pivotal ideas; details need to be read and worked by the students individually after (and before) the course. As an alternative to presenting a lot of text on slides, in this course there will be a (electronic) whiteboard where concepts are developed dynamically and in pace of the teaching conversation thereby adjusting the “dose” of new thoughts to the actual pace of the instruction.\n\n\n\n\n\n\nData Visualization using the grammar of graphics and ggplot2\nData Wrangling based on the tidyverse in R\nBasic concepts of statistical modelling\nPrimer on causal inference\nIntroduction to regression analysis\n\n\n\n\n\n\n\nData visualization, see ModernDive Chap. 2, and get the R code here\n\nExploring common types of statistical diagrams, the “5NG”\nDiscussing when (not) to use diagrams see Anscombe’s Quartett, and when to use which one\nBuilding elegant graphics in R\n\n\n\n\n\n\nData wrangling, see ModernDive Chap. 3, and get the R code here\n\nA taxonomy of typical data operations\nHow to perform common data operations with R\nSummarizing data (aka computing descriptive statistics)\n\n\n\n\n\n\nExercises\n\nExercises on life expectancy.\nCase study on the visualization of flight delays\nAdvanced case study on one hit wonders\nVisualization covid cases\nCase study on nominal data: Survival on the Titanic\nInspiration for own project: Visualize Covid-19 cases from this source.\n\n\n\n\n\n\n\n\n\nBasics of modelling, see ModernDive Chap. 5.0, and get the R code here\n\nWhat is modelling?\nBasic terminology\nPrediction vs. explanation\n\nSome thoughts on causal inference, see ModernDive Chap. 5.3.1\nRegression with one numerical predictor, see ModernDive Chap. 5.1\nRegression with one categorical predictor, see ModernDive Chap. 5.2\nAssessing model fit (using (adjusted) \\(R^2\\)), see ModernDive Chap. 5.3.2\nFor some tips and tricks on typical issues, see ModernDive tips and tricks\n\n\n\n\n\nExercises/Case studies:\n\nPrices of Boston houses, first part\nModeling movie succes, first part\n\n\n\n\n\n\n\n\n\nSlightly more advanced topics on linear regression such as multiple regression and interaction, see ModernDive Chap. 6, and get the R code here\nOne numerical and one categorical predictor, see ModernDive Chap. 6.1\nTwo numerical predictors, see ModernDive Chap. 6.2\nSimpson’s paradox and more on causal inference, see ModernDive Chap. 6.3.3\n\n\n\n\n\nExercises/Case studies:\n\nPrices of Boston houses, second part\nModeling movie succes, second part\nModeling flight delays\n\n\n\n\n\n\n\nThis session is dedicated to work on real projects brought in by the students.\nIn addition, open questions regarding the presented concepts are being discussed.\n\n\n\n\n\nSebastian Sauer works as a professor at Ansbach university, teaching statistics and related stuff. Analyzing data to answer questions related to social phenomena is one of his major interests. He is trying to help raising the methological (and particularly statistical) skills in the sciences (ie., scientists). The programming language “R” is one of his favorite tools. He sees himself as a learner, and is particularly interested learning more on quantitative approaches to understand nature. Open Science is a hot topic to him. He hopes to contribute to pressing social problems such as populism by bringing in his statistical and psychological know-how. He writes a blog which serves as a sketchpad for stuff in his mind (not immune to thought updates) at https://data-se.netlify.app/. Sebastian is the author of “Moderne Datenanalyse mit R” (Sauer 2019). His publication list is available on Google Scholar.\n\n\n\nFeel free to contact me via email at sebastiansauer1@gmail.com.\n\n\n\nThere is no assessment, there are no grades!\n\n\n\nIt’s my goal to make this an excellent course and a stimulating and enjoyable experience for all of us. So that I can find out if this is happening, I encourage feedback—be it positive or negative—on all aspects of the course at any time. For example, if something I’m doing is making it difficult for you to learn, then let me know before it’s too late; if you particularly enjoyed something we did in class, say so so that we can do it again.\n\n\n\nMost of the materials as presented below is made available through the course book ModernDive. Please check the relevant chapters of the book before the course to make sure you have all materials available."
  },
  {
    "objectID": "index.html#where-are-the-slides",
    "href": "index.html#where-are-the-slides",
    "title": "stats-nutshell",
    "section": "Where are the slides?",
    "text": "Where are the slides?\nThere are none. I feel that slides are not optimal for learning. In class, slides can be detrimental if they are too wordy because that distracts from that the dialogue with the instructor, and I hold this very dialogue as essential. Outside of class, slides are neither helpful. Instead, a good book is much more beneficial, because in a book, there’s enough room to patiently explain in sufficient details, an endeavor which is impossible for a slide deck.\nTo underline my messages to you, dear learners, I will use some sketches on a virtual whiteboard, some interactive apps, live coding, and some (pre-prepared) diagrams. That’s a bit similar to what happens at Khan Academy. You might have noticed that many courses at Coursera follow a similar approach.\nI readily confess that this approach is novel to many learners in these days, learners who are accustomed to hundreds of Powerpoint slides. Please be open and I think you will appreciate this didactic style.\n\n\n\n\nSauer, Sebastian. 2019. Moderne Datenanalyse Mit r: Daten Einlesen, Aufbereiten, Visualisieren Und Modellieren. 1. Auflage 2019. FOM-Edition. Wiesbaden: Springer. https://www.springer.com/de/book/9783658215866."
  },
  {
    "objectID": "goals.html",
    "href": "goals.html",
    "title": "1  Goals in statistics",
    "section": "",
    "text": "flowchart LR\n  A{Goals} --> B(describe)\n  A --> C(predict)\n  A --> D(explain)\n  B --> E(distribution)\n  B --> F(assocation)\n  B --> G(extrapolation)\n  C --> H(point estimate)\n  C --> I(interval)\n  D --> J(causal inference)\n  D --> K(population)\n  D --> L(latent construct)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that “goals” do not exist in the world. We make them up in our heads. Hence, they have no ontological existence, they are epistemological beasts. This entails that we are free to devise goals as we wish, provided we can convince ourselves and other souls of the utility of our creativity."
  },
  {
    "objectID": "goals.html#further-reading",
    "href": "goals.html#further-reading",
    "title": "1  Goals in statistics",
    "section": "1.2 Further reading",
    "text": "1.2 Further reading\nHernán, Hsu, and Healy (2019) distinguish:\nHernán et al. (2019) distinguish:\n\nDescription: “How can women aged 60–80 years with stroke history be partitioned in classes defined by their characteristics?”\nPrediction: “What is the probability of having a stroke next year for women with certain characteristics?”\nCausal inference: “Will starting a statin reduce, on average, the risk of stroke in women with certain characteristics?”\n\nGelman, Hill, and Vehtari (2021), chap. 1.1 proposes the three “challenges” of statistical inference.\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge: Cambridge University Press.\n\n\nHernán, Miguel A., John Hsu, and Brian Healy. 2019. “A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks.” Chance 32 (1): 42–49. https://doi.org/10.1080/09332480.2019.1579578."
  },
  {
    "objectID": "inference.html#what-is-it",
    "href": "inference.html#what-is-it",
    "title": "2  Inference",
    "section": "2.1 What is it?",
    "text": "2.1 What is it?\nStatistical inference, according to Gelman, Hill, and Vehtari (2021), chap. 1.1, faces the challenge of generalizing from the particular to the general.\nIn more details, this amounts to generalizing from …\n\na sample to a population\na treatment to a control group (i.e., causal inference)\nobserved measurement to the underlying (“latent”) construct of interest\n\n\n\n\n\n\n\nImportant\n\n\n\nStatistical inference is concerned with making general claims from particular data using mathematical tools."
  },
  {
    "objectID": "inference.html#population-and-sample",
    "href": "inference.html#population-and-sample",
    "title": "2  Inference",
    "section": "2.2 Population and sample",
    "text": "2.2 Population and sample\nWe want to have an estimate of some population value, for example the proportion of A.\nHowever, all we have is a subset, a sample of the populuation. Hence, we need to infer from the sample to the popluation. We do so by generalizing from the sample to the population, see Figure Figure 2.1.\n\n\n\n\n\n\n\n(a) Population\n\n\n\n\n\n\n\n(b) Sample\n\n\n\n\nFigure 2.1: Population vs. sample (Image credit: Karsten Luebke)"
  },
  {
    "objectID": "inference.html#whats-not-inference",
    "href": "inference.html#whats-not-inference",
    "title": "2  Inference",
    "section": "2.3 What’s not inference?",
    "text": "2.3 What’s not inference?\nConsider fig. Figure 2.2 which epitomizes the difference between descriptive and inferential statistics.\n\n\n\n\n\nFigure 2.2: The difference between description and inference"
  },
  {
    "objectID": "inference.html#when-size-helps",
    "href": "inference.html#when-size-helps",
    "title": "2  Inference",
    "section": "2.4 When size helps",
    "text": "2.4 When size helps\nLarger samples allow for more precise estimations (ceteris paribus).\n\n\n\nSample size in motion, Image credit: Karsten Luebke"
  },
  {
    "objectID": "inference.html#what-flavors-are-available",
    "href": "inference.html#what-flavors-are-available",
    "title": "2  Inference",
    "section": "2.5 What flavors are available?",
    "text": "2.5 What flavors are available?\nTypically, when one hears “inference” one thinks of p-values and null hypothesis testing. Those procedures are examples of the school of Frequentist statistics.\nHowever, there’s a second flavor of statistics to be mentioned here: Bayesian statistics.\n\n2.5.1 Frequentist inference\nFrequentism is not concerned about the probability of your research hypothesis.\nFrequentism is all about controling the long-term error. For illustration, suppose you are the CEO of a factory producing screws, and many of them. As the boss, you are not so much interested if a particular scree is in order (or faulty). Rather you are interested that the overall, long-term error rate of your production is low. One may add that your goal might not the minimize the long-term error, but to control it to a certain level - it may be to expensive to produce super high quality screws. Some decent, but cheap screws, might be more profitable.\n\n\n2.5.2 Bayes inference\nBayes inference is concerned about the probability of your research hypothesis.\nIt simply redestributes your beliefs based on new data (evidence) you observe:\n\n\n\n\nflowchart LR\n  A(prior belief) --> B(new data) --> C(posterior belief)\n\n\n\n\n\n\n\n\n\nIn more detail, the posterior belief is formalized as the posterior probability. The Likelihood is the probability of the data given some hypothesis. The normalizing constant serves to give us a number between zero and one.\n\\[\\overbrace{\\Pr(\\color{blue}{H}|\\color{green}{D})}^\\text{posterior probability} = \\overbrace{Pr(\\color{blue}{H})}^\\text{prior} \\frac{\\overbrace{Pr(\\color{green}{D}|\\color{blue}{H})}^\\text{likelihood}}{\\underbrace{Pr(\\color{green}{D})}_{\\text{normalizing constant}}}\\]\nIn practice, the posterior probability of your hypothesis is, the average of your prior and the Likelihood of your data.\n\n\n\nPrior-Likelihood-Posterior"
  },
  {
    "objectID": "inference.html#but-which-one-should-i-consume",
    "href": "inference.html#but-which-one-should-i-consume",
    "title": "2  Inference",
    "section": "2.6 But which one should I consume?",
    "text": "2.6 But which one should I consume?\nPRO Frequentist:\n\nYour supervisor and reviewers will be more familiar with it\nThe technical overhead is simpler compared to Bayes\n\nPRO Bayes:\n\nYou’ll probably want to have a posterior probability of your hypothesis\nYou may appear as a cool kid and an early adoptor of emering statistical methods\n\n\n\n\n\n\n\nTip\n\n\n\nYou’ll learn that the technical setup used for doing Bayes statistics is quite similar to doing frequentist statistics. Stay tuned."
  },
  {
    "objectID": "inference.html#comment-from-xkcd",
    "href": "inference.html#comment-from-xkcd",
    "title": "2  Inference",
    "section": "2.7 Comment from xkcd",
    "text": "2.7 Comment from xkcd\n\n\n\n\n\n\n\n\n\nQuelle"
  },
  {
    "objectID": "inference.html#p-value",
    "href": "inference.html#p-value",
    "title": "2  Inference",
    "section": "2.8 p-value",
    "text": "2.8 p-value\nThe p-value has been used as the pivotal criterion to decide about whether or not a research hypothesis were to be “accepted” (a term forbidden in frequentist and Popperian langauge) or to be rejected. However, more recently, it is advised to use the p-value only as one indicator among multiple; see Wasserstein and Lazar (2016) and Wasserstein, Schirm, and Lazar (2019).\n\n\n\n\n\n\nImportant\n\n\n\nThe p-value is defined as the probability of obtaining the observed data (or more extreme) under the assumption of no effect.\n\n\nFigure Figure 2.3 visualizes the p-value.\n\n\n\n\n\nFigure 2.3: Visualization of the p-value"
  },
  {
    "objectID": "inference.html#some-confusion-remains-about-the-p-value",
    "href": "inference.html#some-confusion-remains-about-the-p-value",
    "title": "2  Inference",
    "section": "2.9 Some confusion remains about the p-value",
    "text": "2.9 Some confusion remains about the p-value\n\n\nfrom Imgflip Meme Generator\n\nGoodman (2008) provides an entertaining overview on typical misconceptions of the p-value full text.\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge: Cambridge University Press.\n\n\nGoodman, Steven. 2008. “A Dirty Dozen: Twelve p-Value Misconceptions.” Seminars in Hematology, Interpretation of quantitative research, 45 (3): 135–40. https://doi.org/10.1053/j.seminhematol.2008.04.003.\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA’s Statement on p-Values: Context, Process, and Purpose.” The American Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108.\n\n\nWasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. 2019. “Moving to a World Beyond ‘ p</i> p < 0.05’.” The American Statistician 73 (March): 1–19. https://doi.org/10.1080/00031305.2019.1583913."
  },
  {
    "objectID": "regression1.html",
    "href": "regression1.html",
    "title": "3  Regression basics",
    "section": "",
    "text": "One regression\n\n\nAlternatively, venture into the forest of statistical tests as oultined eg here, at Uni Muenster.\nYou may want to ponder on this image of a decision tree of which test to choose, see Figure Figure 3.1.\n\n\n\nFigure 3.1: Choose your test carefully"
  },
  {
    "objectID": "regression1.html#common-statistical-tests-are-linear-models",
    "href": "regression1.html#common-statistical-tests-are-linear-models",
    "title": "3  Regression basics",
    "section": "3.2 Common statistical tests are linear models",
    "text": "3.2 Common statistical tests are linear models\nAs Jonas Kristoffer Lindeløv tells us, we can formulate most statistical tests as a linear model, ie., a regression.\n\n\n\nCommon statistical tests as linear models"
  },
  {
    "objectID": "regression1.html#r-packages-needed",
    "href": "regression1.html#r-packages-needed",
    "title": "3  Regression basics",
    "section": "3.3 R-packages needed",
    "text": "3.3 R-packages needed\n\nlibrary(rstanarm)\nlibrary(tidyverse)\nlibrary(easystats)"
  },
  {
    "objectID": "regression1.html#in-all-its-glory",
    "href": "regression1.html#in-all-its-glory",
    "title": "3  Regression basics",
    "section": "3.4 In all its glory",
    "text": "3.4 In all its glory"
  },
  {
    "objectID": "regression1.html#first-model-one-metric-predictor",
    "href": "regression1.html#first-model-one-metric-predictor",
    "title": "3  Regression basics",
    "section": "3.5 First model: one metric predictor",
    "text": "3.5 First model: one metric predictor\nFirst, let’s load some data:\n\ndata(mtcars)\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\n3.5.1 Frequentist\nDefine and fit the model:\n\nlm1_freq <- lm(mpg ~ hp, data = mtcars)\n\nGet the parameter values:\n\nparameters(lm1_freq)\n\nParameter   | Coefficient |   SE |         95% CI | t(30) |      p\n------------------------------------------------------------------\n(Intercept) |       30.10 | 1.63 | [26.76, 33.44] | 18.42 | < .001\nhp          |       -0.07 | 0.01 | [-0.09, -0.05] | -6.74 | < .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nPlot the model parameters:\n\nplot(parameters(lm1_freq))\n\n\n\n\n\n\n3.5.2 Bayesian\n\nlm1_bayes <- stan_glm(mpg ~ hp, data = mtcars, refresh = 0)\n\nActually, we want to suppress some overly verbose output, using refresh = 0:\n\nlm1_bayes <- stan_glm(mpg ~ hp, data = mtcars, refresh = 0)\n\nGet the parameter values:\n\nparameters(lm1_bayes)\n\nParameter   | Median |         95% CI |   pd | % in ROPE |  Rhat |     ESS |                   Prior\n----------------------------------------------------------------------------------------------------\n(Intercept) |  30.01 | [26.71, 33.35] | 100% |        0% | 1.000 | 3646.00 | Normal (20.09 +- 15.07)\nhp          |  -0.07 | [-0.09, -0.05] | 100% |      100% | 1.000 | 3596.00 |   Normal (0.00 +- 0.22)\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n\n\nPlot the model parameters:\n\nplot(parameters(lm1_bayes))\n\n\n\n\n\n\n3.5.3 Model performance\n\nr2(lm1_freq)\n\n# R2 for Linear Regression\n       R2: 0.602\n  adj. R2: 0.589\n\n\n\nr2(lm1_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.583 (95% CI [0.370, 0.742])\n\n\n\n\n3.5.4 Model check\n\ncheck_model(lm1_freq)\n\n\n\n\n\ncheck_model(lm1_bayes)\n\n\n\n\n\n\n3.5.5 Get some predictions\n\nlm1_pred <- estimate_relation(lm1_freq)\nlm1_pred\n\nModel-based Expectation\n\nhp     | Predicted |   SE |         95% CI\n------------------------------------------\n52.00  |     26.55 | 1.18 | [24.15, 28.95]\n83.44  |     24.41 | 0.94 | [22.49, 26.32]\n114.89 |     22.26 | 0.75 | [20.72, 23.80]\n146.33 |     20.11 | 0.68 | [18.72, 21.51]\n177.78 |     17.97 | 0.75 | [16.43, 19.50]\n209.22 |     15.82 | 0.93 | [13.92, 17.73]\n240.67 |     13.68 | 1.17 | [11.29, 16.07]\n272.11 |     11.53 | 1.44 | [ 8.59, 14.48]\n303.56 |      9.39 | 1.73 | [ 5.86, 12.92]\n335.00 |      7.24 | 2.02 | [ 3.11, 11.38]\n\nVariable predicted: mpg\nPredictors modulated: hp\n\n\nMore details on the above function can be found on the respective page at the easystats site.\n\n\n3.5.6 Plot the model\n\nplot(lm1_pred)"
  },
  {
    "objectID": "regression1.html#model-performance",
    "href": "regression1.html#model-performance",
    "title": "3  Regression basics",
    "section": "3.6 Model performance",
    "text": "3.6 Model performance\n\nr2(lm1_freq)\n\n# R2 for Linear Regression\n       R2: 0.602\n  adj. R2: 0.589\n\n\n\nr2(lm1_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.586 (95% CI [0.377, 0.739])"
  },
  {
    "objectID": "regression1.html#model-check",
    "href": "regression1.html#model-check",
    "title": "3  Regression basics",
    "section": "3.7 Model check",
    "text": "3.7 Model check\n\ncheck_model(lm1_freq)\n\n\n\n\n\ncheck_model(lm1_bayes)"
  },
  {
    "objectID": "regression1.html#more-of-this",
    "href": "regression1.html#more-of-this",
    "title": "3  Regression basics",
    "section": "3.6 More of this",
    "text": "3.6 More of this\nMore technical details for gauging model performance and model quality, can be found on the site of the R package “performance at the easystats site."
  },
  {
    "objectID": "regression1.html#multiple-metric-predictors",
    "href": "regression1.html#multiple-metric-predictors",
    "title": "3  Regression basics",
    "section": "3.7 Multiple metric predictors",
    "text": "3.7 Multiple metric predictors\nAssume we have a theory that dictates that fuel economy is a (causal) function of horse power and engine displacement.\n\nlm2_freq <- lm(mpg ~ hp + disp, data = mtcars)\nparameters(lm2_freq)\n\nParameter   | Coefficient |       SE |         95% CI | t(29) |      p\n----------------------------------------------------------------------\n(Intercept) |       30.74 |     1.33 | [28.01, 33.46] | 23.08 | < .001\nhp          |       -0.02 |     0.01 | [-0.05,  0.00] | -1.86 | 0.074 \ndisp        |       -0.03 | 7.40e-03 | [-0.05, -0.02] | -4.10 | < .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nSimilarly for Bayes inference:\n\nlm2_bayes <- stan_glm(mpg ~ hp + disp, data = mtcars)\n\nResults\n\nparameters(lm2_bayes)\n\nParameter   | Median |         95% CI |     pd | % in ROPE |  Rhat |     ESS |                   Prior\n------------------------------------------------------------------------------------------------------\n(Intercept) |  30.75 | [28.04, 33.44] |   100% |        0% | 1.000 | 3914.00 | Normal (20.09 +- 15.07)\nhp          |  -0.02 | [-0.05,  0.00] | 96.03% |      100% | 1.001 | 2190.00 |   Normal (0.00 +- 0.22)\ndisp        |  -0.03 | [-0.05, -0.01] |   100% |      100% | 1.000 | 2272.00 |   Normal (0.00 +- 0.12)\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n\nplot(parameters(lm2_bayes))\n\n\n\nr2(lm2_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.731 (95% CI [0.589, 0.839])\n\n\nDepending on the value of disp the prediction of mpg from hp will vary:\n\nlm2_pred <- estimate_relation(lm2_freq)\nplot(lm2_pred)"
  },
  {
    "objectID": "regression1.html#one-nominal-predictor",
    "href": "regression1.html#one-nominal-predictor",
    "title": "3  Regression basics",
    "section": "3.8 One nominal predictor",
    "text": "3.8 One nominal predictor\n\nmtcars2 <-\n  mtcars %>% \n  mutate(am_f = factor(am))\n\nlm3a <- lm(mpg ~ am_f, data = mtcars2)\nparameters(lm3a)\n\nParameter   | Coefficient |   SE |         95% CI | t(30) |      p\n------------------------------------------------------------------\n(Intercept) |       17.15 | 1.12 | [14.85, 19.44] | 15.25 | < .001\nam f [1]    |        7.24 | 1.76 | [ 3.64, 10.85] |  4.11 | < .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\n\nlm3a_means <- estimate_means(lm3a, at = \"am_f\")\nlm3a_means \n\nEstimated Marginal Means\n\nam_f |  Mean |   SE |         95% CI\n------------------------------------\n0    | 17.15 | 1.12 | [14.85, 19.44]\n1    | 24.39 | 1.36 | [21.62, 27.17]\n\nMarginal means estimated at am_f\n\n\n\nplot(lm3a_means)\n\n\n\n\nNote that we should have converted am to a factor variable before fitting the model. Otherwise, the plot won’t work.\nHere’s a more hand-crafted version of the last plot:\n\nggplot(mtcars2) +\n  aes(x = am_f, y = mpg) +\n  geom_violin() +\n  geom_jitter(width = .1, alpha = .5) +\n  geom_pointrange(data = lm3a_means,\n                  color = \"orange\",\n                  aes(ymin = CI_low, ymax = CI_high, y = Mean)) +\n  geom_line(data = lm3a_means, aes(y = Mean, group = 1))"
  },
  {
    "objectID": "regression1.html#one-metric-and-one-nominal-predictor",
    "href": "regression1.html#one-metric-and-one-nominal-predictor",
    "title": "3  Regression basics",
    "section": "3.9 One metric and one nominal predictor",
    "text": "3.9 One metric and one nominal predictor\n\nmtcars2 <-\n  mtcars %>% \n  mutate(cyl = factor(cyl))\n\nlm4 <- lm(mpg ~ hp + cyl, data = mtcars2)\nparameters(lm4)\n\nParameter   | Coefficient |   SE |          95% CI | t(28) |      p\n-------------------------------------------------------------------\n(Intercept) |       28.65 | 1.59 | [ 25.40, 31.90] | 18.04 | < .001\nhp          |       -0.02 | 0.02 | [ -0.06,  0.01] | -1.56 | 0.130 \ncyl [6]     |       -5.97 | 1.64 | [ -9.33, -2.61] | -3.64 | 0.001 \ncyl [8]     |       -8.52 | 2.33 | [-13.29, -3.76] | -3.66 | 0.001 \n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\n\nlm4_pred <- estimate_relation(lm4)\nplot(lm4_pred)"
  },
  {
    "objectID": "regression1.html#exercises",
    "href": "regression1.html#exercises",
    "title": "3  Regression basics",
    "section": "3.11 Exercises",
    "text": "3.11 Exercises\n\nmtcars simple 1\nmtcars simple 2\nmtcars simple 3"
  },
  {
    "objectID": "regression1.html#lab",
    "href": "regression1.html#lab",
    "title": "3  Regression basics",
    "section": "3.12 Lab",
    "text": "3.12 Lab\nGet your own data, and build a simple model reflecting your research hypothesis. If you are lacking data (or hypothesis) get something close to it."
  },
  {
    "objectID": "regression2.html#multiplicative-associations",
    "href": "regression2.html#multiplicative-associations",
    "title": "4  Advanced Regression",
    "section": "4.2 Multiplicative associations",
    "text": "4.2 Multiplicative associations"
  },
  {
    "objectID": "regression2.html#glimpse-on-parameter-estimation",
    "href": "regression2.html#glimpse-on-parameter-estimation",
    "title": "4  Advanced Regression",
    "section": "4.3 Glimpse on parameter estimation",
    "text": "4.3 Glimpse on parameter estimation\nAn elegant yet simple explanation of the math of parameter estimation can be found at “go data driven”."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and\nOther Stories. Analytical Methods for Social Research. Cambridge:\nCambridge University Press.\n\n\nGoodman, Steven. 2008. “A Dirty Dozen: Twelve p-Value\nMisconceptions.” Seminars in Hematology, Interpretation\nof quantitative research, 45 (3): 135–40. https://doi.org/10.1053/j.seminhematol.2008.04.003.\n\n\nHernán, Miguel A., John Hsu, and Brian Healy. 2019. “A Second\nChance to Get Causal Inference Right: A Classification of Data Science\nTasks.” Chance 32 (1): 42–49. https://doi.org/10.1080/09332480.2019.1579578.\n\n\nSauer, Sebastian. 2019. Moderne Datenanalyse Mit r: Daten Einlesen,\nAufbereiten, Visualisieren Und Modellieren. 1. Auflage 2019.\nFOM-Edition. Wiesbaden: Springer. https://www.springer.com/de/book/9783658215866.\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The\nASA’s Statement on p-Values: Context, Process, and\nPurpose.” The American Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108.\n\n\nWasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. 2019.\n“Moving to a World Beyond ‘\np</i> p <\n0.05’.” The American Statistician 73 (March):\n1–19. https://doi.org/10.1080/00031305.2019.1583913."
  },
  {
    "objectID": "regression1.html#what-about-correlation",
    "href": "regression1.html#what-about-correlation",
    "title": "3  Regression basics",
    "section": "3.10 What about correlation?",
    "text": "3.10 What about correlation?\nCorrelation is really a close cousin to regression. In fact, regression with standardized variables amounts to correlation.\nLet’s get the correlation matrix of the variables in involved in lm4.\n\nlm4_corr <- \n  mtcars %>% \n  select(mpg, hp, disp) %>% \n  correlation()\n\nlm4_corr\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |     r |         95% CI | t(30) |         p\n--------------------------------------------------------------------\nmpg        |         hp | -0.78 | [-0.89, -0.59] | -6.74 | < .001***\nmpg        |       disp | -0.85 | [-0.92, -0.71] | -8.75 | < .001***\nhp         |       disp |  0.79 | [ 0.61,  0.89] |  7.08 | < .001***\n\np-value adjustment method: Holm (1979)\nObservations: 32\n\n\n\nplot(summary(lm4_corr))"
  }
]