[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "stats-nutshell",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "stats-nutshell",
    "section": "Welcome!",
    "text": "Welcome!\nThis is an introductory course on statistical modelling. Welcome!\nThe focus of this course is on how to specify a theoretical idea (possibly vague) in a testable statistical model.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#please-read-me",
    "href": "index.html#please-read-me",
    "title": "stats-nutshell",
    "section": "Please read me",
    "text": "Please read me\nIn order to benefit as much as possible from this course, it is necessary for you to read this preface information. Yoda agrees (s. Figure Yoda finds you should read the manual).\n\n\n\n\n\n\nFigure 1: Yoda finds you should read the manual",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#pdf-version",
    "href": "index.html#pdf-version",
    "title": "stats-nutshell",
    "section": "PDF-Version",
    "text": "PDF-Version\nUse the print button of your browser to print the html page into a PDF page.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "stats-nutshell",
    "section": "Course description",
    "text": "Course description\nAnalyzing research data can broadly be classified in three parts: explorative data analysis, modeling (including inference), and visualization. Either part is pivotal in its own right, but it can be argued that modeling is at the core of the scientific endeavor. However, in practice, modeling, visualization, and data exploration is heavily intertwined, so that three parts may be recognized (as individual entities) but not usefully separated from each other. This idea provides the rationale of this course: Data exploration, data visualization and data modeling is discussed as an integrated framework.\nThe focus is on practical data analysis; theoretical concepts are, where mentioned, second class citizens due to time constraints and the didactic aims of the course.\nFor example, statistical inference – such as p-values and confidence intervals – are not more than touched briefly, as the instructor believes that modeling, not inference, is of prime importance for the auditorium.\nWe will use the R environment for all computations (freely available). Please bring your own Laptop with R and RStudio installed (installation guides are provided). Data and R code will be provided.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#were-on-a-crash-course",
    "href": "index.html#were-on-a-crash-course",
    "title": "stats-nutshell",
    "section": "We’re on a crash course",
    "text": "We’re on a crash course\nThe course is set-up as a “crash course” which indicates that we’ll rather try to cover a breadth of steps rather than digging deep at certain particular points. The rationale of this approach is that before digging deep, it is necessary to gain an overview of the territory. In addition, if one particular topic is not of interest to a given student (perhaps to difficult/simple), not much time is lost.\nBe warned! Compare this crash course to a dancing crash course right before your wedding: A lot can be achieved by such a course in some instances, or rather, the worst consequences (of not knowing how to dance) may be fenced off, but one should not expect to be a dancing queen (king) thereafter.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#more-on-modelling",
    "href": "index.html#more-on-modelling",
    "title": "stats-nutshell",
    "section": "More on modelling",
    "text": "More on modelling\nModels and modeling are of pivotal importance in many sciences, not only for providing an explanation of nature en miniature (theoretical models), but also for gauging how closely the empirical data at hand match the theoretical model. Translating a theoretical model into statistical language is called statistical modeling and provides the guiding principle in this introductory course. Regression models will be presented as a lingua franca of statistical modeling, and we will learn that many empirical questions can (comfortably) be analyzed using a regression framework. Depending on the background and aims of the participants (and time permitting), we will shed light on some standard topics such as model comparison, classification models, and typical pitfalls. Given a more advanced auditorium, we may want to explore how causal and non-causal associations can be translated and tested using simple linear statistical models. Foundational ideas of statistical modeling will be accompanied by short examples and case studies to facilitate transfer and practical application after the course.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-prerequisites",
    "href": "index.html#course-prerequisites",
    "title": "stats-nutshell",
    "section": "Course prerequisites",
    "text": "Course prerequisites\nBasic computer usage knowledge is needed (downloading materials from the internet, operating a PC, etc). Basic R knowledge is needed. Basic knowledge of statistical concepts (such as descriptive statistics) is needed. Willingness to learn is essential.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "stats-nutshell",
    "section": "Learning objectives",
    "text": "Learning objectives\nUpon successful completion of this course, students should be able to:\n\nselect the right statistical visualization for a variety of data contexts\n“crunch” or “wrangle” data\nexplain what statistical modeling means\nformulate basic statistical models\ndifferentiate between predictive and explanatory modeling\napply the methods to own datasets",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-literature",
    "href": "index.html#course-literature",
    "title": "stats-nutshell",
    "section": "Course Literature",
    "text": "Course Literature\nThis course builds on the freely available e-book ModernDive. Each topic is paralleled by an accompanying chapter from ModernDive. A hard copy can be purchased here. The book is for sale in print here.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "stats-nutshell",
    "section": "Course logistics",
    "text": "Course logistics\nThis course can be presented as a one-day seminar or split-up in two or more blocks.\nThe course can be held in English or German.\nPlease bring your own computer and read the notes regarding course logistics in advance. Note that some upfront preparation is needed from the learners.\nR and RStudio1 will be needed throughout the course. Please make sure that the IT is running. In case of technical difficulties with R feel free to use RStudio Cloud; free plans are available.\nAll learning materials (such as literature, code, data) will be provided in electronic format.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#upfront-student-preparation",
    "href": "index.html#upfront-student-preparation",
    "title": "stats-nutshell",
    "section": "UPFRONT student preparation",
    "text": "UPFRONT student preparation\n\nInstall R and RStudio, see ModernDive Chap. 1.1. In case you have your R running on your system, please make sure that you’re uptodate. If outdated, download and install the most recent versions of the software. Similarly, hit the “Update” button in RStudio’s “Packages” tab to update your packages if you have not done so for a couple of months.\nSign-in at RStudio Cloud. It’s super helpful because I as the techer can provide you with an environment where all R stuff is ready to use (packages installed etc).\nInstall the necessary R packages as used in the book chapters covered in this course (see the sections on “Needed packages” in each chapter). If in doubt, see here the instructions on how to install R packages. Here’s the actual list on the R packages we’ll need.\nStudents new to R are advised to learn the basics, see ModernDive, Chap 1.2 - 1.5\nBring your own laptop\nMake sure your internet connection is stable and your loudspeaker/headset is working; a webcam is helpful.\nStudents are advised to review the course materials after each session.\nI recommend that you carefully check the course description to make sure the course fits your needs (not too advanced/basic).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#didactic-outline",
    "href": "index.html#didactic-outline",
    "title": "stats-nutshell",
    "section": "Didactic outline",
    "text": "Didactic outline\nThis course can rather be considered a workshop in the sense that the instructor uses a dialogue-based approach to teaching and that there are numerous exercises during the course. Instead of providing long talks to the students, the instructor feels obligated to engage students in back-and-forth conversations. Similarly, the presentation of a large number of Powerpoint slide is avoided. Instead, a thorough course literature is available (free online), so that students will have no barrier in diving deeply into the materials and ideas presented. However, during class it is more important to transmit the pivotal ideas; details need to be read and worked by the students individually after (and before) the course. As an alternative to presenting a lot of text on slides, in this course there will be a (electronic) whiteboard where concepts are developed dynamically and in pace of the teaching conversation thereby adjusting the “dose” of new thoughts to the actual pace of the instruction.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "stats-nutshell",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\nNote\n\n\n\nPlease not that the focus and the amount covered in a course strongly depends on the background, aims and prior knowledge of audience.\n\n\n\nOverview on topics covered\n\nData visualization building the grammar of graphics and ggplot2\nData wrangling based on the tidyverse in R\nBasic concepts of statistical modelling\nPrimer on causal inference\nIntroduction to regression analysis\nQuick refresher of statistical inference",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "stats-nutshell",
    "section": "Instructor",
    "text": "Instructor\n\nBio\nSebastian Sauer works as a professor at Ansbach university, teaching statistics and related stuff. Analyzing data to answer questions related to social phenomena is one of his major interests. He is trying to help raising the methological (and particularly statistical) skills in the sciences (ie., scientists). The programming language “R” is one of his favorite tools. He sees himself as a learner, and is particularly interested learning more on quantitative approaches to understand nature. Open Science is a hot topic to him. He hopes to contribute to pressing social problems such as populism by bringing in his statistical and psychological know-how. He writes a blog which serves as a sketchpad for stuff in his mind (not immune to thought updates) at https://data-se.netlify.app/. Sebastian is the author of “Moderne Datenanalyse mit R” (Sauer, 2019). His publication list is available on Google Scholar.\n\n\nContact\nCheck-out my personal homepage here.\nFeel free to contact me via email at any time at profsebastiansauerATgmail.com.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#assessment-and-grades",
    "href": "index.html#assessment-and-grades",
    "title": "stats-nutshell",
    "section": "Assessment and grades",
    "text": "Assessment and grades\nThere is no assessment, there are no grades!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#talk-to-me",
    "href": "index.html#talk-to-me",
    "title": "stats-nutshell",
    "section": "Talk to me",
    "text": "Talk to me\nIt’s my goal to make this an excellent course and a stimulating and enjoyable experience for all of us. So that I can find out if this is happening, I encourage feedback—be it positive or negative—on all aspects of the course at any time. For example, if something I’m doing is making it difficult for you to learn, then let me know before it’s too late; if you particularly enjoyed something we did in class, say so so that we can do it again.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "stats-nutshell",
    "section": "Course materials",
    "text": "Course materials\nMost of the materials as presented below is made available through the course book ModernDive. Please check the relevant chapters of the book before the course to make sure you have all materials available.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "stats-nutshell",
    "section": "Licence",
    "text": "Licence\nThis is permissive work, see the licence here.\nThe author is Sebastian Sauer.\nCheck out the Github repo of this project.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "stats-nutshell",
    "section": "Resources",
    "text": "Resources\n\nRecommendations\n\nRStudio Cheatsheets, particularly on data wrangling, and data vizualization\nBook R for Data Science as a handy reference or a serious text book.\nTidy Tuesday video series\nPost your open question on Stack Overflow.\nFollow #rstats hashtag on Twitter.\n\nFor students willing to learn more and go deeper (than the concepts explored in the present course), this book on regression modelling, and this book on statistical learning are recommended. For German folks, check out my book on modern data analysis.\nSuggested literature for deepening the analytic skills include Statistical Rethinking. For an introduction to graphical causal models, check out Julia Rohrer’s paper. For a more in-depth journey, consider reading this book. While I wholeheartedly recommend such books, we will not be able to discuss many of the ideas presented therein in class (in this course) due to time constraints.\n\n\nR Packages\nAll R packages are accessible through the course book; please consult the relevant chapters. Please install all R packages used before the course. Here’s a tutorial on how to install R packages.\nThe most important R packages for this course are:\n\ntidyverse\neasystats\n\nThe following packages are useful for data access (but not strictly mandatory):\n\npalmerpenguins\nggpubr\nrio\nvtree\nvisdat\ndataexplorer\ntableOne\nflextable\ngapminder\nnycflights13\nfivethirtyeight\nskimr\nISLR\n\nFor the Bayes models you’ll need some extra software (free, save and stable), but somewhat more hassle to install. Using Bayes in this course is optional. You don’t miss a lot if you don’t use it.\n\nrstanarm\n\nFor the R package {rstanarm} to run, you’ll need to install RStan. On Windows, this amounts to installing RTools. On Mac, you’ll need to install the XCode CLI2.\nIn sum, follow the instructions on the RStan website. It’s unfortunately a bit complicated.\n\n\nData\nAll data are accessible through the course book; please consult the relevant chapters.\n\n\nLabs (case studies)\nPractical data analysis skills can be practiced using these labs; in addition Chapter 11 provides two cases studies. Note that such content may be used as homework.\nThere are a lot of case studies scattered on the internet.\n\n\nSketching causal models\nDagitty is great tool for sketching causal graphs (DAGs), it can be usd in your browser or as R package. Here’s an example of a collider bias. Check out this post for an intuitive explanation.\n\n\nGerman introductary course\nReaders who speak German may check out this Blitzkurs into data analysis using R.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#where-are-the-slides",
    "href": "index.html#where-are-the-slides",
    "title": "stats-nutshell",
    "section": "Where are the slides?",
    "text": "Where are the slides?\nThere are none. I feel that slides are not optimal for learning. In class, slides can be detrimental if they are too wordy because that distracts from that the dialogue with the instructor, and I hold this very dialogue as essential. Outside of class, slides are neither helpful. Instead, a good book is much more beneficial, because in a book, there’s enough room to patiently explain in sufficient details, an endeavor which is impossible for a slide deck.\nTo underline my messages to you, dear learners, I will use some sketches on a virtual whiteboard, some interactive apps, live coding, and some (pre-prepared) diagrams. That’s a bit similar to what happens at Khan Academy. You might have noticed that many courses at Coursera follow a similar approach.\nI readily confess that this approach is novel to many learners in these days, learners who are accustomed to hundreds of Powerpoint slides. Please be open and I think you will appreciate this didactic style.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#technical-details",
    "href": "index.html#technical-details",
    "title": "stats-nutshell",
    "section": "Technical Details",
    "text": "Technical Details\nLast update: 2024-03-11 17:07:30\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       macOS Big Sur ... 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Berlin\n date     2024-03-07\n pandoc   3.1.12.2 @ /usr/local/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.6.2   2023-12-11 [1] CRAN (R 4.2.0)\n digest        0.6.33  2023-07-07 [1] CRAN (R 4.2.0)\n evaluate      0.23    2023-11-01 [1] CRAN (R 4.2.0)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.2.0)\n htmltools     0.5.7   2023-11-03 [1] CRAN (R 4.2.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.2.0)\n jsonlite      1.8.8   2023-12-04 [1] CRAN (R 4.2.0)\n knitr         1.45    2023-10-30 [1] CRAN (R 4.2.1)\n rlang         1.1.3   2024-01-10 [1] CRAN (R 4.2.1)\n rmarkdown     2.25    2023-09-18 [1] CRAN (R 4.2.0)\n rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.2.0)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.2.0)\n xfun          0.41    2023-11-01 [1] CRAN (R 4.2.0)\n\n [1] /Users/sebastiansaueruser/Rlibs\n [2] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\nSauer, S. (2019). Moderne Datenanalyse mit R: Daten einlesen, aufbereiten, visualisieren und modellieren (1. Auflage 2019). Springer. https://www.springer.com/de/book/9783658215866",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "stats-nutshell",
    "section": "",
    "text": "Desktop version, not the server↩︎\npossibly you need also a Fortran compiler, but maybe that’s optional↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "020-basics.html",
    "href": "020-basics.html",
    "title": "1  Basics",
    "section": "",
    "text": "1.1 Embarking",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "020-basics.html#embarking",
    "href": "020-basics.html#embarking",
    "title": "1  Basics",
    "section": "",
    "text": "Exercise 1.1 (Hello stats) Present yourself to the group along the lines of the following three tags:\n\nYour primary scientific interest\nYour expectations to this course\nYour background in statistics and R\n\nIf you want, feel free to add a fun fact. \\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "020-basics.html#goals-in-statistics",
    "href": "020-basics.html#goals-in-statistics",
    "title": "1  Basics",
    "section": "1.2 Goals in statistics",
    "text": "1.2 Goals in statistics\n\n1.2.1 Taxonmy of goals\nMany stories to be told. Here’s one, on the goals pursued in statistics (and related fields), see Figure Figure 1.1.\n\n\n\n\n\n\nflowchart LR\n  A{Goals} --&gt; B(describe)\n  A --&gt; C(predict)\n  A --&gt; D(explain)\n  B --&gt; E(distribution)\n  B --&gt; F(assocation)\n  B --&gt; G(extrapolation)\n  C --&gt; H(point estimate)\n  C --&gt; I(interval)\n  D --&gt; J(causal inference)\n  D --&gt; K(population)\n  D --&gt; L(latent construct)\n\n\n\n\n\nFigure 1.1: A taxonomy of statistical goals\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that “goals” do not exist in the world. We make them up in our heads. Hence, they have no ontological existence, they are epistemological beasts. This entails that we are free to devise goals as we wish, provided we can convince ourselves and other souls of the utility of our creativity.\n\n\nHernán et al. (2019) distinguish:\n\nDescription: “How can women aged 60–80 years with stroke history be partitioned in classes defined by their characteristics?”\nPrediction: “What is the probability of having a stroke next year for women with certain characteristics?”\nCausal inference: “Will starting a statin reduce, on average, the risk of stroke in women with certain characteristics?”\n\n\n\n1.2.2 Lab: Your goals\nMatch your (most pressing) research goal to the nomenclature for scientific goals as shown in Figure 1.1. Explain your reasoning.\nNext, put three research themes or studies you particularly like to this nomenclature and explain your reasoning.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "020-basics.html#a-framework-for-problem-solving",
    "href": "020-basics.html#a-framework-for-problem-solving",
    "title": "1  Basics",
    "section": "1.3 A framework for problem solving",
    "text": "1.3 A framework for problem solving\n\n1.3.1 PPDAC\nThe PPDAC Model is a methodological framework (aka a model) for applying the scientific method to any analytical or research question, or at least it is applicable to quite a few (MacKay & Oldford, 2000). It is not meant to be a rigid sequence, but rather a cycle that may turn a number of rounds like a spiral. Statistician Chris Wild puts the PPDAC cycle in the following figure, see Figure Figure 1.2. In this short essay, he summaries his ideas on how to use the PPDAC as a tool for data analysis in problem solving.\n\n\n\n\n\n\nFigure 1.2: PPDAC cycle. Image source: Chris Wild\n\n\n\nWickham and Grolemund (see Figure Figure 2.1 in Section 2.3) provide a suggestion of the parts of the statistical analyses, that is the “Analysis” step in the PPDAC.\n\n\n1.3.2 Fundamental issues in data analysis\nWild & Pfannkuch (1999) further note that variation is one of the essential characteristics of data. They discern to types of variation however, see Figure Figure 1.3.\n\n\n\n\n\n\nFigure 1.3: Two types of variartion. Image source: Chris Wild\n\n\n\nWild & Pfannkuch (1999) give a more systematic overview on how a quantitative research question - applied or basic - can be tackled and conceived. For example, in their paper the authors enumarate some dispositions that researcher should embrace in order to fruitfully engage in empirical research:\n\nScepticism\nImagination\nCuriosity\nOpennness\nA propensity to seek deeper menaing\nBeing logical\nEngagement\nPerseverance",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "020-basics.html#getting-started",
    "href": "020-basics.html#getting-started",
    "title": "1  Basics",
    "section": "1.4 Getting started",
    "text": "1.4 Getting started\n\n1.4.1 R Basics\nCheck out the course Statistics1, chapter on importing data for an accessible introduction to getting started with R and RStudio.\nPlease also note that R and RStudio should be installed before starting (this course).\nIn addition, your R packages should be updtodate, according to Arnold Schwarzenegger (s. Figure 1.4).\n\n\n\n\n\n\nFigure 1.4: Keep your R packages uptodate, or risk being an outdated model, Arnie says\n\n\n\n\n\n1.4.2 Help me, I’m lost\nCheck-out this introductory statistics course.\nPro-Tipp: Use the translation tool of your browswer to translate into your favorite language.\n\n\n1.4.3 Initial quiz\nTo get an idea whether you have digested some R basics, consider this quiz.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "020-basics.html#data-import",
    "href": "020-basics.html#data-import",
    "title": "1  Basics",
    "section": "1.5 Data import",
    "text": "1.5 Data import\nCheck out chapter 3 in Statistics1 on how to import data into RStudio and for some basic concepts about “tidy data”.\nSpoiler: There’s a button in RStudio in the “Environment” Pane saying “Import Dataset”. Just click it, and things should work out.\n\n\n\n\n\n\nNote\n\n\n\nI strongly advice working with Projects in RStudio, as it makes working with file paths a lot easier.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "020-basics.html#sec-blitz-data",
    "href": "020-basics.html#sec-blitz-data",
    "title": "1  Basics",
    "section": "1.6 Blitz start with data",
    "text": "1.6 Blitz start with data\nWe’ll work predominantly with the following data sets.\n\n1.6.1 Motor Trends Cars\nTo blitz start with data, type the following in R:\n\ndata(mtcars)\n\nAnd the data set mtcars will be available.\nTo get help for the data set, type help(mtcars)\n Download the mtcars data \n\n\n1.6.2 Penguins\n\n\n\nAllez, penguins! Image Credit: Allison Horst, CCO\n\n\nA bit more advanced, but it’s a nice data set, try the Palmer Penguins data set:\n\nd &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\n\nhead(d)  # see the first few rows, the \"head\" of the table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrownames\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\n\n2007\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n6\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\nHere’s some documentation (code book) for this data set.\n Download the penguins data",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "020-basics.html#lab",
    "href": "020-basics.html#lab",
    "title": "1  Basics",
    "section": "1.7 Lab",
    "text": "1.7 Lab\nImport your research data into R.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "020-basics.html#more-data-sets",
    "href": "020-basics.html#more-data-sets",
    "title": "1  Basics",
    "section": "1.9 More data sets",
    "text": "1.9 More data sets\nCheck out this curated list of data sets useful for learning and practicing your data skills.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "020-basics.html#if-nothing-else-helps",
    "href": "020-basics.html#if-nothing-else-helps",
    "title": "1  Basics",
    "section": "1.10 If nothing else helps",
    "text": "1.10 If nothing else helps\nStay calm and behold the infinity.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "020-basics.html#going-further",
    "href": "020-basics.html#going-further",
    "title": "1  Basics",
    "section": "1.11 Going further",
    "text": "1.11 Going further\nSimilar to the “goals” of statistics as presented here, Gelman et al. (2021), chap. 1.1 proposes three “challenges” of statistical inference.\nWild & Pfannkuch (1999) discuss the thought processes involved in statistical problem solving seen from a broad perspective. Ismay & Kim (2020) is a helpful start into the first steps in R.\nIf you want to dig deeper, check-out this course on statistical inference using Bayes statistics. If you are interested in predictive modeling, check-out this couse.\n\n\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nHernán, M. A., Hsu, J., & Healy, B. (2019). A second chance to get causal inference right: A classification of data science tasks. Chance, 32(1), 42–49. https://doi.org/10.1080/09332480.2019.1579578\n\n\nIsmay, C., & Kim, A. Y.-S. (2020). Statistical inference via data science: A ModernDive into R and the Tidyverse. CRC Press / Taylor & Francis Group. https://moderndive.com/\n\n\nMacKay, R. J., & Oldford, R. W. (2000). Scientific method, statistical method and the speed of light. Statistical Science, 15(3), 254–278. https://doi.org/10.1214/ss/1009212817\n\n\nWild, C. J., & Pfannkuch, M. (1999). Statistical thinking in empirical enquiry. International Statistical Review, 67(3), 223–248.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "030-EDA.html",
    "href": "030-EDA.html",
    "title": "2  Exploratory Data Analysis",
    "section": "",
    "text": "2.1 R packages needed for this chapter\nMake sure to run the following code chunks in order to start the neccessary R packages.\nlibrary(easystats)  # make stats easy again\nlibrary(tidyverse)  # data wrangling\nlibrary(tableone)  # tables, optional\nlibrary(rio)  # import/export data, eg., to excel\nlibrary(ggpubr)  # simple data visualization\nlibrary(ggstatsplot)  # data visualization ornamented with statistics\nCheck out the course Statistics1, chapter on introducing R for an accessible introduction to getting started with R and RStudio and on how to install R packages.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "030-EDA.html#r-packages-needed-for-this-chapter",
    "href": "030-EDA.html#r-packages-needed-for-this-chapter",
    "title": "2  Exploratory Data Analysis",
    "section": "",
    "text": "Caution\n\n\n\nYou can only start a package if you have it installed upfront.\n\n\n\n\nExercise 2.1 (Install the missing packages) Take a minute to install any of the packages above which are not yet installed on your machine. Hint: Click on the pane Packages and then hit the button Install. \\(\\square\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "030-EDA.html#whats-eda",
    "href": "030-EDA.html#whats-eda",
    "title": "2  Exploratory Data Analysis",
    "section": "2.2 What’s EDA?",
    "text": "2.2 What’s EDA?\nExploratory Data Analysis (EDA) is a procedure to scrutinize a dataset at hand in order learn about it. EDA comprises descriptive statistics, data visualization and data transformation techniques (such as dimension reduction).\nIt’s not so mathematical deep as modelling, but in practice it’s really important.\nThere’s this famous saying:\n\nIn Data Science, 80% of time spent prepare data, 20% of time spent complain about the need to prepare data.\n\nEDA can roughly be said to comprise the following parts:\n\nImporting (and exporting) data\nData cleansing (such as deal with missing data etc)\nData transformation or “wrangling” (such as long to wide format)\nComputing desriptive statistics (such as the notorious mean)\nAnalyzing distributions (is it normal?)\nFinding patterns in data (aka data mining)\nMore complex data transformation techniques (such as factor analysis)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "030-EDA.html#sec-data-journey",
    "href": "030-EDA.html#sec-data-journey",
    "title": "2  Exploratory Data Analysis",
    "section": "2.3 Data journey",
    "text": "2.3 Data journey\nWickham & Grolemund (2016) present a visual sketch of what could be called the “data journey”, i.e., the steps we are taking in order to learn from data, seen from an hands-on angle, see Figure 2.1.\n\n\n\n\n\n\nFigure 2.1: The data journey",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "030-EDA.html#blitz-data",
    "href": "030-EDA.html#blitz-data",
    "title": "2  Exploratory Data Analysis",
    "section": "2.4 Blitz data",
    "text": "2.4 Blitz data\nSee Section 1.6 for some data sets suitable to get going.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "030-EDA.html#data-cleansing",
    "href": "030-EDA.html#data-cleansing",
    "title": "2  Exploratory Data Analysis",
    "section": "2.5 Data cleansing",
    "text": "2.5 Data cleansing\nThe R package {janitor} provides some nice stuff for data cleansing. Check out this case study.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "030-EDA.html#convenience-functions",
    "href": "030-EDA.html#convenience-functions",
    "title": "2  Exploratory Data Analysis",
    "section": "2.6 Convenience functions",
    "text": "2.6 Convenience functions\nThere a quite a few functions (residing in some packages) that help you doing EDA from a helicoptor point of view. In other words, you do not have to pay attention to nitty-gritty details, the function will do that for you. This is approach is, well, convenient, but of course comes at a price. You will not have a great amount of choice and influence on the way the data is analyzed and presented.\nWe’ll use the penguins data set for a demonstration.\n\npenguins &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\n\n\n2.6.1 Data Explorer\nThere are many systems and approaches to explore data. One particular interesting system is the R-package DataExplorer.\n\n\n\nR-package DataExplorer\n\n\nCheck it out on its Githup page.\n\nlibrary(DataExplorer)\n\nFor example, we can easily get an overview of the distribution of all (quantitative) variables in the data set. What do you notice on inspecting the distributions, see Figure 2.2?\n\nplot_histogram(penguins)\n\n\n\n\n\n\n\nFigure 2.2: An overview of the distribution can easily attained using the function plot_histogram\n\n\n\n\n\nA quick “introduction” to the data set is provided by the function plot_intro, see Figure 2.3.\n\nplot_intro(penguins)\n\n\n\n\n\n\n\nFigure 2.3: An introduction view on a data set\n\n\n\n\n\nWe can also get a glimpse on all the qualitative data columns in our datra set, see Figure 2.4.\n\nplot_bar(penguins)\n\n\n\n\n\n\n\nFigure 2.4: Bar plots visualizing the distribution of qualitative variables using the function plot_bar\n\n\n\n\n\n\n\n2.6.2 visdat\nThe r package visdat provides a “fingerprint” of a data set.\nLet’s show-case it using the penguins data set.\n\nlibrary(visdat)  # must be installed\nvis_dat(penguins)\n\n\n\n\nA fingerprint of the penguins data set; for each variable the data type and the number of missings is shown.\n\n\n\n\n\n\n2.6.3 vtree\nA bit similar to {DataExplorer}, the R package {vtree} helps to explore visually datasets, see Figure 2.5.\n\nlibrary(vtree)\nvtree(penguins, c(\"sex\", \"island\"))\n\n\n\n\n\n\n\nFigure 2.5: Breaking down the counts in a data set using vtree\n\n\n\n\n\n\n\n2.6.4 TableOne\nThe R package {tableOne} provides something like the typical “Table 1” in many papers.\nFrom the homepage:\n\nThe tableone package is an R package that eases the construction of “Table 1”, i.e., patient baseline characteristics table commonly found in biomedical research papers. The packages can summarize both continuous and categorical variables mixed within one table. Categorical variables can be summarized as counts and/or percentages. Continuous variables can be summarized in the “normal” way (means and standard deviations) or “nonnormal” way (medians and interquartile ranges).\n\n\npenguins &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\n\nIt’s quite simple to use:\n\nlibrary(tableone)\nCreateTableOne(data = penguins)\n\n                               \n                                Overall         \n  n                                 344         \n  rownames (mean (SD))           172.50 (99.45) \n  species (%)                                   \n     Adelie                         152 (44.2)  \n     Chinstrap                       68 (19.8)  \n     Gentoo                         124 (36.0)  \n  island (%)                                    \n     Biscoe                         168 (48.8)  \n     Dream                          124 (36.0)  \n     Torgersen                       52 (15.1)  \n  bill_length_mm (mean (SD))      43.92 (5.46)  \n  bill_depth_mm (mean (SD))       17.15 (1.97)  \n  flipper_length_mm (mean (SD))  200.92 (14.06) \n  body_mass_g (mean (SD))       4201.75 (801.95)\n  sex (%)                                       \n                                     11 ( 3.2)  \n     female                         165 (48.0)  \n     male                           168 (48.8)  \n  year (mean (SD))              2008.03 (0.82)  \n\n\nTo get more detailled results, use the summary method:\n\ntab1 &lt;- CreateTableOne(data = penguins)\nsummary(tab1)\n\n\n     ### Summary of continuous variables ###\n\nstrata: Overall\n                    n miss p.miss mean    sd median  p25  p75  min  max  skew\nrownames          344    0    0.0  172  99.4    172   87  258    1  344  0.00\nbill_length_mm    344    2    0.6   44   5.5     44   39   48   32   60  0.05\nbill_depth_mm     344    2    0.6   17   2.0     17   16   19   13   22 -0.14\nflipper_length_mm 344    2    0.6  201  14.1    197  190  213  172  231  0.35\nbody_mass_g       344    2    0.6 4202 802.0   4050 3550 4750 2700 6300  0.47\nyear              344    0    0.0 2008   0.8   2008 2007 2009 2007 2009 -0.05\n                  kurt\nrownames          -1.2\nbill_length_mm    -0.9\nbill_depth_mm     -0.9\nflipper_length_mm -1.0\nbody_mass_g       -0.7\nyear              -1.5\n\n=======================================================================================\n\n     ### Summary of categorical variables ### \n\nstrata: Overall\n     var   n miss p.miss     level freq percent cum.percent\n species 344    0    0.0    Adelie  152    44.2        44.2\n                         Chinstrap   68    19.8        64.0\n                            Gentoo  124    36.0       100.0\n                                                           \n  island 344    0    0.0    Biscoe  168    48.8        48.8\n                             Dream  124    36.0        84.9\n                         Torgersen   52    15.1       100.0\n                                                           \n     sex 344    0    0.0             11     3.2         3.2\n                            female  165    48.0        51.2\n                              male  168    48.8       100.0\n                                                           \n\n\nHow to export to MS Office? Well, one simple approach is obviously to copy-paste. Checkout Section 2.9 for more advanced options.\n\n\n2.6.5 The easystats way\nThere are some packages, such as {easystats}, which provide comfortable access to basic statistics:\n\nlibrary(easystats)  # once per session\ndescribe_distribution(penguins)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nMin\nMax\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nrownames\n172.50000\n99.4484791\n172.500\n1.0\n344.0\n0.0000000\n-1.2000000\n344\n0\n\n\nbill_length_mm\n43.92193\n5.4595837\n9.300\n32.1\n59.6\n0.0531181\n-0.8760270\n342\n2\n\n\nbill_depth_mm\n17.15117\n1.9747932\n3.125\n13.1\n21.5\n-0.1434646\n-0.9068661\n342\n2\n\n\nflipper_length_mm\n200.91520\n14.0617137\n23.250\n172.0\n231.0\n0.3456818\n-0.9842729\n342\n2\n\n\nbody_mass_g\n4201.75439\n801.9545357\n1206.250\n2700.0\n6300.0\n0.4703293\n-0.7192219\n342\n2\n\n\nyear\n2008.02907\n0.8183559\n2.000\n2007.0\n2009.0\n-0.0537278\n-1.5049366\n344\n0\n\n\n\n\n\n\ndescribe_distribution provides us with an overview on typical descriptive summaries.\nHow to export to MS Office? Well, one simple approach is obviously to copy-paste. Checkout Section 2.9 for more advanced options.\nFor nominal variables, consider data_tabulate:\n\ndata_tabulate(penguins, select = c(\"sex\", \"island\")) |&gt; \n  print_md() \n\n\nFrequency Table\n\n\nVariable\nValue\nN\nRaw %\nValid %\nCumulative %\n\n\n\n\nsex\n(NA)\n11\n3.20\n3.20\n3.20\n\n\n\nfemale\n165\n47.97\n47.97\n51.16\n\n\n\nmale\n168\n48.84\n48.84\n100.00\n\n\n\n(NA)\n0\n0.00\n(NA)\n(NA)\n\n\n\n\n\n\n\n\n\n\nisland\nBiscoe\n168\n48.84\n48.84\n48.84\n\n\n\nDream\n124\n36.05\n36.05\n84.88\n\n\n\nTorgersen\n52\n15.12\n15.12\n100.00\n\n\n\n(NA)\n0\n0.00\n(NA)\n(NA)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that print_md helps to get a more visually pleasing (HTML) output, and not only raw command line style output.\nWe can also get grouped tabulations, which amounts to something similar to a contingency table:\n\npenguins %&gt;% \n  group_by(sex) %&gt;% \n  data_tabulate(select = \"island\", collapse = TRUE)\n\n# Frequency Table\n\nVariable |        Group |     Value |  N | Raw % | Valid % | Cumulative %\n---------+--------------+-----------+----+-------+---------+-------------\nisland   |       sex () |    Biscoe |  5 | 45.45 |   45.45 |        45.45\n         |              |     Dream |  1 |  9.09 |    9.09 |        54.55\n         |              | Torgersen |  5 | 45.45 |   45.45 |       100.00\n         |              |      &lt;NA&gt; |  0 |  0.00 |    &lt;NA&gt; |         &lt;NA&gt;\n---------+--------------+-----------+----+-------+---------+-------------\nisland   | sex (female) |    Biscoe | 80 | 48.48 |   48.48 |        48.48\n         |              |     Dream | 61 | 36.97 |   36.97 |        85.45\n         |              | Torgersen | 24 | 14.55 |   14.55 |       100.00\n         |              |      &lt;NA&gt; |  0 |  0.00 |    &lt;NA&gt; |         &lt;NA&gt;\n---------+--------------+-----------+----+-------+---------+-------------\nisland   |   sex (male) |    Biscoe | 83 | 49.40 |   49.40 |        49.40\n         |              |     Dream | 62 | 36.90 |   36.90 |        86.31\n         |              | Torgersen | 23 | 13.69 |   13.69 |       100.00\n         |              |      &lt;NA&gt; |  0 |  0.00 |    &lt;NA&gt; |         &lt;NA&gt;\n-------------------------------------------------------------------------\n\n\n\nCheckout the function reference of your favorite package in order to learn what’s on the shelf. For example, here’s the function reference site of datawizard, one of the packages in the easystats ecosystem.\n\n\n\n2.6.6 The easystats way\nThere are some packages, such as {easystats}, which provide comfortable access to basic statistics:\n\nlibrary(easystats)  # once per session\ndescribe_distribution(penguins)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nMin\nMax\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nrownames\n172.50000\n99.4484791\n172.500\n1.0\n344.0\n0.0000000\n-1.2000000\n344\n0\n\n\nbill_length_mm\n43.92193\n5.4595837\n9.300\n32.1\n59.6\n0.0531181\n-0.8760270\n342\n2\n\n\nbill_depth_mm\n17.15117\n1.9747932\n3.125\n13.1\n21.5\n-0.1434646\n-0.9068661\n342\n2\n\n\nflipper_length_mm\n200.91520\n14.0617137\n23.250\n172.0\n231.0\n0.3456818\n-0.9842729\n342\n2\n\n\nbody_mass_g\n4201.75439\n801.9545357\n1206.250\n2700.0\n6300.0\n0.4703293\n-0.7192219\n342\n2\n\n\nyear\n2008.02907\n0.8183559\n2.000\n2007.0\n2009.0\n-0.0537278\n-1.5049366\n344\n0\n\n\n\n\n\n\ndescribe_distribution provides us with an overview on typical descriptive summaries.\nHow to export to MS Office? Well, one simple approach is obviously to copy-paste. Checkout Section 2.9 for more advanced options.\nFor nominal variables, consider data_tabulate:\n\ndata_tabulate(penguins, select = c(\"sex\", \"island\")) |&gt; \n  print_md() \n\n\nFrequency Table\n\n\nVariable\nValue\nN\nRaw %\nValid %\nCumulative %\n\n\n\n\nsex\n(NA)\n11\n3.20\n3.20\n3.20\n\n\n\nfemale\n165\n47.97\n47.97\n51.16\n\n\n\nmale\n168\n48.84\n48.84\n100.00\n\n\n\n(NA)\n0\n0.00\n(NA)\n(NA)\n\n\n\n\n\n\n\n\n\n\nisland\nBiscoe\n168\n48.84\n48.84\n48.84\n\n\n\nDream\n124\n36.05\n36.05\n84.88\n\n\n\nTorgersen\n52\n15.12\n15.12\n100.00\n\n\n\n(NA)\n0\n0.00\n(NA)\n(NA)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that print_md helps to get a more visually pleasing (HTML) output, and not only raw command line style output.\nWe can also get grouped tabulations, which amounts to something similar to a contingency table:\n\npenguins %&gt;% \n  group_by(sex) %&gt;% \n  data_tabulate(select = \"island\", collapse = TRUE)\n\n# Frequency Table\n\nVariable |        Group |     Value |  N | Raw % | Valid % | Cumulative %\n---------+--------------+-----------+----+-------+---------+-------------\nisland   |       sex () |    Biscoe |  5 | 45.45 |   45.45 |        45.45\n         |              |     Dream |  1 |  9.09 |    9.09 |        54.55\n         |              | Torgersen |  5 | 45.45 |   45.45 |       100.00\n         |              |      &lt;NA&gt; |  0 |  0.00 |    &lt;NA&gt; |         &lt;NA&gt;\n---------+--------------+-----------+----+-------+---------+-------------\nisland   | sex (female) |    Biscoe | 80 | 48.48 |   48.48 |        48.48\n         |              |     Dream | 61 | 36.97 |   36.97 |        85.45\n         |              | Torgersen | 24 | 14.55 |   14.55 |       100.00\n         |              |      &lt;NA&gt; |  0 |  0.00 |    &lt;NA&gt; |         &lt;NA&gt;\n---------+--------------+-----------+----+-------+---------+-------------\nisland   |   sex (male) |    Biscoe | 83 | 49.40 |   49.40 |        49.40\n         |              |     Dream | 62 | 36.90 |   36.90 |        86.31\n         |              | Torgersen | 23 | 13.69 |   13.69 |       100.00\n         |              |      &lt;NA&gt; |  0 |  0.00 |    &lt;NA&gt; |         &lt;NA&gt;\n-------------------------------------------------------------------------\n\n\n\nCheckout the function reference of your favorite package in order to learn what’s on the shelf. For example, here’s the function reference site of datawizard, one of the packages in the easystats ecosystem.\n\n\n\n2.6.7 Lab\nTake your research data and prepare it using (at least) one of the “convenience” functions for data cleansing presented above.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "030-EDA.html#lab",
    "href": "030-EDA.html#lab",
    "title": "2  Exploratory Data Analysis",
    "section": "2.7 Lab",
    "text": "2.7 Lab\nTake your research data and prepare it using (at least) one of the “convenience” functions for data cleansing presented above.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "030-EDA.html#tidyverse",
    "href": "030-EDA.html#tidyverse",
    "title": "2  Exploratory Data Analysis",
    "section": "2.7 Tidyverse",
    "text": "2.7 Tidyverse\n\n\n\nEntering the tidyverse\n\n\n\n2.7.1 Intro to the tidyverse\nThe Tidyverse is probably the R thing with the most publicity. And it’s great. It’s a philosophy baked into an array of R packages. Perhaps central is the idea that a lot of little Lego pieces, if fitting nicely together, provides a simple yet flexible and thus powerful machinery.\nThere’s a lot of introductory material to the tidyverse around for instance, so I’m not repeating that here.\n\n\n2.7.2 More advanced tidyverse\n\n2.7.2.1 Repeat a function over many columns\nAt times, we would like to compute the same functions for many variables, ie columns for tidyverse applications.\nLet’s load the penguins data for illustration.\n\npenguins &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\n\nhead(penguins)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrownames\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n6\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\nSay, we would like to compute the mean value for each numeric variable in the data set:\n\npenguins %&gt;% \n  summarise(across(bill_length_mm:body_mass_g, mean, na.rm = TRUE))\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\n43.92193\n17.15117\n200.9152\n4201.754\n\n\n\n\n\n\nSynonymously, we could write:\n\npenguins %&gt;% \n  summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)))\n\n\n\n\n\n\n\n\n\n\n\n\n\nrownames\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nyear\n\n\n\n\n172.5\n43.92193\n17.15117\n200.9152\n4201.754\n2008.029\n\n\n\n\n\n\nSay, we would like to compute the z-value of each numeric variable.\nAddmittedly, easystats makes it quite simple:\n\npenguins %&gt;% \n  standardise(select = is.numeric) %&gt;% \n  head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrownames\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n-1.724511\nAdelie\nTorgersen\n-0.8832047\n0.7843001\n-1.4162715\n-0.5633167\nmale\n-1.257484\n\n\n-1.714456\nAdelie\nTorgersen\n-0.8099390\n0.1260033\n-1.0606961\n-0.5009690\nfemale\n-1.257484\n\n\n-1.704400\nAdelie\nTorgersen\n-0.6634077\n0.4298326\n-0.4206603\n-1.1867934\nfemale\n-1.257484\n\n\n-1.694345\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n-1.257484\n\n\n-1.684289\nAdelie\nTorgersen\n-1.3227986\n1.0881294\n-0.5628905\n-0.9374027\nfemale\n-1.257484\n\n\n-1.674234\nAdelie\nTorgersen\n-0.8465718\n1.7464261\n-0.7762357\n-0.6880121\nmale\n-1.257484\n\n\n\n\n\n\nSee the help page of standardise for mor details on how to select variables and on more options.\nBut for the purpose of illustration, let’s do it with more simple means, i.e. tidyverse only.\n\npenguins %&gt;% \n  transmute(across(bill_length_mm:body_mass_g, \n                .fns = ~ {(.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)},\n                .names = \"{.col}_z\")) %&gt;% \n  head()\n\n\n\n\n\nbill_length_mm_z\nbill_depth_mm_z\nflipper_length_mm_z\nbody_mass_g_z\n\n\n\n\n-0.8832047\n0.7843001\n-1.4162715\n-0.5633167\n\n\n-0.8099390\n0.1260033\n-1.0606961\n-0.5009690\n\n\n-0.6634077\n0.4298326\n-0.4206603\n-1.1867934\n\n\nNA\nNA\nNA\nNA\n\n\n-1.3227986\n1.0881294\n-0.5628905\n-0.9374027\n\n\n-0.8465718\n1.7464261\n-0.7762357\n-0.6880121\n\n\n\n\n\n\nIt’s maybe more succint to put the z-value computation in its function, and then just apply this function:\n\nz_stand &lt;- function(x){\n  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\n}\n\n\nd2 &lt;-\npenguins %&gt;% \n  mutate(across(bill_length_mm:body_mass_g, \n                .fns = z_stand))\n  \nd2 %&gt;% \n  glimpse()\n\nRows: 344\nColumns: 9\n$ rownames          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ bill_length_mm    &lt;dbl&gt; -0.8832047, -0.8099390, -0.6634077, NA, -1.3227986, …\n$ bill_depth_mm     &lt;dbl&gt; 0.78430007, 0.12600328, 0.42983257, NA, 1.08812936, …\n$ flipper_length_mm &lt;dbl&gt; -1.4162715, -1.0606961, -0.4206603, NA, -0.5628905, …\n$ body_mass_g       &lt;dbl&gt; -0.563316704, -0.500969030, -1.186793445, NA, -0.937…\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"f…\n$ year              &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n2.7.2.2 Rowwise operations\nFor technical reasons, it’s a bit cumbersome in (base) R to compute rowwise operations. The thing is, R’s dataframes are organized as vectors of columns so it’s much easier to do stuff columnwise.\nHowever, since recently, computing rowwise operations with the tidyverse has become simpler. Consider the following example. Say we would like to know the highest z-value for each variable we just computed, that is the highest values per individual, ie., by row in the data frame.\n\nd2 %&gt;% \n  drop_na() %&gt;% \n  rowwise() %&gt;% \n  mutate(max_z = max(c(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g))) %&gt;% \n  head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrownames\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\nmax_z\n\n\n\n\n1\nAdelie\nTorgersen\n-0.8832047\n0.7843001\n-1.4162715\n-0.5633167\nmale\n2007\n0.7843001\n\n\n2\nAdelie\nTorgersen\n-0.8099390\n0.1260033\n-1.0606961\n-0.5009690\nfemale\n2007\n0.1260033\n\n\n3\nAdelie\nTorgersen\n-0.6634077\n0.4298326\n-0.4206603\n-1.1867934\nfemale\n2007\n0.4298326\n\n\n5\nAdelie\nTorgersen\n-1.3227986\n1.0881294\n-0.5628905\n-0.9374027\nfemale\n2007\n1.0881294\n\n\n6\nAdelie\nTorgersen\n-0.8465718\n1.7464261\n-0.7762357\n-0.6880121\nmale\n2007\n1.7464261\n\n\n7\nAdelie\nTorgersen\n-0.9198375\n0.3285561\n-1.4162715\n-0.7191859\nfemale\n2007\n0.3285561\n\n\n\n\n\n\n\n\n\n2.7.3 Lab\nTake your research data and play around using the tidyverse functions shown above.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "030-EDA.html#data-visualization",
    "href": "030-EDA.html#data-visualization",
    "title": "2  Exploratory Data Analysis",
    "section": "2.8 Data visualization",
    "text": "2.8 Data visualization\nVisualize your data – plots can provide insights which can’t (easily) delivered by numbers alone. See Anscombe’s Quartet.\nThe star on the R visualization sky is called ggplot. It probably is one of the most advanced statistical visualization package on the planet.\nHowever, for a quick start, there are some nice wrappers on ggplot, levering the beauty of ggplot but making the making simple(r).\n\n2.8.1 ggpubr\nggpubr is a simple yet powerful way to visualize data.\n\nlibrary(ggpubr)  # must be installed\n\nLet’s visualize the association of bill length (see Figure 2.6) with body mass, see Figure 2.7.\n\n\n\n\n\n\nFigure 2.6: In case you did not know, that’s what is meant by bill length and depth of our penguins\n\n\n\n\nggscatter(penguins,\n          x = \"bill_length_mm\",\n          y = \"body_mass_g\",\n          color = \"sex\")\n\n\n\n\n\n\n\nFigure 2.7: A simple scatterplot of the penguins data set using ggscatter\n\n\n\n\n\nLet’s improve a bit on the plot, see Figure 2.8.\n\nggscatter(penguins,\n          x = \"bill_length_mm\",\n          y = \"body_mass_g\",\n          color = \"species\",\n          shape = \"species\",\n          add = \"reg.line\",\n          xlab = \"Bill length (mm)\",\n          ylab = \"Body maxx (g)\",\n          conf.int = TRUE,\n          ellipse = TRUE,\n          cor.coef = TRUE)\n\n\n\n\n\n\n\nFigure 2.8: A more polished scatterplot of the penguins data set using ggscatter\n\n\n\n\n\nCheck out the manual of the function for further notice on the options.\n\n\n2.8.2 ggstatsplot\nIn addtion, consider ggstatsplot)(https://indrajeetpatil.github.io/ggstatsplot/). Let’s use the mtcars dataset for a quick demonstration. For example, assume you would like to compare two group of cars, automatic vs. manual shifting, in terms of fuel economy. Here we go, Figure 2.9.\n\nlibrary(ggstatsplot)\nggbetweenstats(\n  data = penguins,\n  x = sex,\n  y = body_mass_g)\n\n\n\n\n\n\n\nFigure 2.9: ggstatsplot builds on ggplot, but is simpler to use\n\n\n\n\n\nCheckout the manual of the function to get a handle on all the options and details.\nWait, let’s drop all missing values (NA) before we plot using drop_na(), Figure 2.10.\n\npenguins_no_na &lt;-\npenguins |&gt; \n  drop_na()  # from package \"tidyverse\"\n\nggbetweenstats(\n  data = penguins_no_na,\n  x = sex,\n  y = body_mass_g)\n\n\n\n\n\n\n\nFigure 2.10: Comparing groups on a quantitative output variable, but dropping missing values before hand\n\n\n\n\n\n\n\n2.8.3 Lab\nTake your research data and play around using the visualization functions shown above.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "030-EDA.html#sec-office",
    "href": "030-EDA.html#sec-office",
    "title": "2  Exploratory Data Analysis",
    "section": "2.9 Exporting to Office",
    "text": "2.9 Exporting to Office\n\n2.9.1 Word\n\n2.9.1.1 Basic approach\nIt’s no big deal to get it into Word processors as well. Here’s one simple way:\n\nOpen a Quarto-Document in RStudio\nproduce some nice table\ncopy and paste it to Word\n\nHere’s an example of producing a nice table:\n\nlibrary(easystats)  # once per session only\ndescribe_distribution(penguins) |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nRange\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nrownames\n172.50\n99.45\n172.50\n(1.00, 344.00)\n0.00\n-1.20\n344\n0\n\n\nbill_length_mm\n43.92\n5.46\n9.30\n(32.10, 59.60)\n0.05\n-0.88\n342\n2\n\n\nbill_depth_mm\n17.15\n1.97\n3.12\n(13.10, 21.50)\n-0.14\n-0.91\n342\n2\n\n\nflipper_length_mm\n200.92\n14.06\n23.25\n(172.00, 231.00)\n0.35\n-0.98\n342\n2\n\n\nbody_mass_g\n4201.75\n801.95\n1206.25\n(2700.00, 6300.00)\n0.47\n-0.72\n342\n2\n\n\nyear\n2008.03\n0.82\n2.00\n(2007.00, 2009.00)\n-0.05\n-1.50\n344\n0\n\n\n\n\n\nNote that print_md() will “prettify” your output. Without this line, your output will look more command-line-nerdy style.\n\n\n2.9.1.2 Additional ways\nIn addition, one user gave the following recommendation on StackOverflow:\n\nAnother possible solution: The above strategy did not work for me when I had a similar issue, but it was resolved once I knitted the table1 object and opened the html in browser to copy the html table and successfully paste into word. Doing it within RStudio viewer would not work for me for some reason.\n\nLastly, there are options to export directly to Word or Powerpoint. The R package {flextable} provides functions for that purpose, see Table 2.11:\n\nlibrary(flextable)\nmy_flex_tab &lt;- flextable(head(penguins))\nmy_flex_tab\n\n\n\nTable 2.1: A table using the R package flextable\n\n\n\nrownamesspeciesislandbill_length_mmbill_depth_mmflipper_length_mmbody_mass_gsexyear1AdelieTorgersen39.118.71813,750male2,0072AdelieTorgersen39.517.41863,800female2,0073AdelieTorgersen40.318.01953,250female2,0074AdelieTorgersen2,0075AdelieTorgersen36.719.31933,450female2,0076AdelieTorgersen39.320.61903,650male2,007\n\n\n\n\n\nJust save it to the Office format of your choice:\n\nsave_as_docx(\"Table 1 \" = my_flex_tab, path = \"my_tab.docx\")\nsave_as_pptx(\"Table 1 \" = my_flex_tab, path = \"my_tab.pptx\")\n\n\n\n\n2.9.2 Excel\n\n2.9.2.1 Just save your data to disk\nThe most straightforward approach is to convince your EDA function to produce a data frame. Data frames can be written as CSV or XLSX to disk, and then easily imported to office packages.\neasystats and tidyverse are two examples where this happens.\n\nlibrary(easystats)   # must be installed\ndf1 &lt;- describe_distribution(mtcars)\n\nNow, df1 is a data frame:\n\nstr(df1)\n\nClasses 'parameters_distribution', 'see_parameters_distribution' and 'data.frame':  11 obs. of  10 variables:\n $ Variable : chr  \"mpg\" \"cyl\" \"disp\" \"hp\" ...\n $ Mean     : num  20.09 6.19 230.72 146.69 3.6 ...\n $ SD       : num  6.027 1.786 123.939 68.563 0.535 ...\n $ IQR      : num  7.53 4 221.53 84.5 0.84 ...\n $ Min      : num  10.4 4 71.1 52 2.76 ...\n $ Max      : num  33.9 8 472 335 4.93 ...\n $ Skewness : num  0.672 -0.192 0.42 0.799 0.293 ...\n $ Kurtosis : num  -0.022 -1.763 -1.068 0.275 -0.45 ...\n $ n        : int  32 32 32 32 32 32 32 32 32 32 ...\n $ n_Missing: int  0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"object_name\")= chr \"mtcars\"\n - attr(*, \"threshold\")= num 0.1\n\n\nLet’s export as XLSX (via the R package rio) and as CSV:\n\nlibrary(rio)  # must be installed\nexport(df1, file = \"df1.xlsx\")\nexport(df1, file = \"df1.csv\")\n\nFor exporting csv files we could alternatively use write_csv() from the tidyverse or write.csv from base R. Then, checkout the file in Excel. Note that it is possible to import CSV files to Excel.\n\ndf2 &lt;- data_tabulate(mtcars$am)\nstr(df2)\n\nClasses 'dw_data_tabulate' and 'data.frame':    3 obs. of  6 variables:\n $ Variable    : chr  \"mtcars$am\" \"mtcars$am\" \"mtcars$am\"\n $ Value       : Factor w/ 2 levels \"0\",\"1\": 1 2 NA\n $ N           : int  19 13 0\n $ Raw %       : num  59.4 40.6 0\n $ Valid %     : num  59.4 40.6 NA\n $ Cumulative %: num  59.4 100 NA\n - attr(*, \"type\")= chr \"numeric\"\n - attr(*, \"object\")= chr \"mtcars$am\"\n - attr(*, \"duplicate_varnames\")= logi [1:3] FALSE TRUE TRUE\n - attr(*, \"total_n\")= int 32\n - attr(*, \"valid_n\")= int 32\n\n\n\nexport(df2, file = \"df2.csv\")\n\n\n\n2.9.2.2 More advanced\nNote that if we use data_tabulate like this:\n\ndf3 &lt;- data_tabulate(mtcars, select = c(\"am\", \"vs\"))\n\ndf3 %&gt;% str()\n\nList of 2\n $ :Classes 'dw_data_tabulate' and 'data.frame':    3 obs. of  6 variables:\n  ..$ Variable    : chr [1:3] \"am\" \"am\" \"am\"\n  ..$ Value       : Factor w/ 2 levels \"0\",\"1\": 1 2 NA\n  ..$ N           : int [1:3] 19 13 0\n  ..$ Raw %       : num [1:3] 59.4 40.6 0\n  ..$ Valid %     : num [1:3] 59.4 40.6 NA\n  ..$ Cumulative %: num [1:3] 59.4 100 NA\n  ..- attr(*, \"type\")= chr \"numeric\"\n  ..- attr(*, \"varname\")= chr \"am\"\n  ..- attr(*, \"object\")= chr \"am\"\n  ..- attr(*, \"duplicate_varnames\")= logi [1:3] FALSE TRUE TRUE\n  ..- attr(*, \"total_n\")= int 32\n  ..- attr(*, \"valid_n\")= int 32\n $ :Classes 'dw_data_tabulate' and 'data.frame':    3 obs. of  6 variables:\n  ..$ Variable    : chr [1:3] \"vs\" \"vs\" \"vs\"\n  ..$ Value       : Factor w/ 2 levels \"0\",\"1\": 1 2 NA\n  ..$ N           : int [1:3] 18 14 0\n  ..$ Raw %       : num [1:3] 56.2 43.8 0\n  ..$ Valid %     : num [1:3] 56.2 43.8 NA\n  ..$ Cumulative %: num [1:3] 56.2 100 NA\n  ..- attr(*, \"type\")= chr \"numeric\"\n  ..- attr(*, \"varname\")= chr \"vs\"\n  ..- attr(*, \"object\")= chr \"vs\"\n  ..- attr(*, \"duplicate_varnames\")= logi [1:3] FALSE TRUE TRUE\n  ..- attr(*, \"total_n\")= int 32\n  ..- attr(*, \"valid_n\")= int 32\n - attr(*, \"class\")= chr [1:2] \"dw_data_tabulates\" \"list\"\n - attr(*, \"collapse\")= logi FALSE\n\n\nWe’ll get a list of two data frames.\nTo export either, we need to access each list list element:\n\nexport(df3[[1]], file = \"df3.csv\")\n\n\n\n\n2.9.3 Lab\nTake your research data, produce some nice tables and figures and import them into a word file.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "030-EDA.html#case-studies",
    "href": "030-EDA.html#case-studies",
    "title": "2  Exploratory Data Analysis",
    "section": "2.10 Case Studies",
    "text": "2.10 Case Studies\n\n2.10.1 Data Wrangling\n\n2.10.1.1 Penguins\n\n\n\nR package/dataset palmerpenguins\n\n\nExplore the palmerpenguins dataset, it’s a famous dataset made for learning data analysis.\nThere’s a great interactive course on EDA based on the penguins. Have a look, it’s great!\nGo penguins! Allez!\n\n\n\n2.10.2 More Case studies\n\nRebekka Barter’s introduction to the tidyverse\nCase study on the visualization of flight delays\nAdvanced case study on one hit wonders\nVisualization covid cases\nCase study on nominal data: Survival on the Titanic\nInspiration for own project: Visualize Covid-19 cases from this source.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "030-EDA.html#exercises",
    "href": "030-EDA.html#exercises",
    "title": "2  Exploratory Data Analysis",
    "section": "2.11 Exercises",
    "text": "2.11 Exercises\n\nGapminder\nPenguins Visualization\nmtcars\n\n\n🧑‍🎓 I want more!\n\n\n👨‍🏫 Check-out all exercises from Datenwerk with the tag datawrangling.\n\n\n\n\n\n\n\nTip\n\n\n\nUse the translation function of your browers to translate the webpage into your favorite language.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "030-EDA.html#cheatsheets",
    "href": "030-EDA.html#cheatsheets",
    "title": "2  Exploratory Data Analysis",
    "section": "2.12 Cheatsheets",
    "text": "2.12 Cheatsheets\nThere are a number of nice cheat sheets available on an array of topics related to EDA, made available by the folks at RStudio.\nConsider this collection:\n\n{dplyr}: data wrangling\n{tidyr}: data preparation\n{ggplot}: data visualization\n{gtsummary}: publication ready tables\n\nSo much great stuff! A bit too much to digest in one go, but definitely worthwhile if you plan to dig deeper in data analysis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "030-EDA.html#going-further",
    "href": "030-EDA.html#going-further",
    "title": "2  Exploratory Data Analysis",
    "section": "2.13 Going further",
    "text": "2.13 Going further\nWickham & Grolemund (2016) is an highly recommendable resource in order to get a thorough understanding of data analysis using R. Note that this source is focusing on the “how to”, not so much to theoretical foundations. Ismay & Kim (2020) is a gently introduction into many steps on the data journey, including EDA. For a gentle introduction to Data visualization, see ModernDive Chap. 2 (Ismay & Kim, 2020), and get the R code here. Similarly, for data wrangling, see ModernDive Chap. 3, and get the R code here.\n\n\n\n\nIsmay, C., & Kim, A. Y.-S. (2020). Statistical inference via data science: A ModernDive into R and the Tidyverse. CRC Press / Taylor & Francis Group. https://moderndive.com/\n\n\nWickham, H., & Grolemund, G. (2016). R for Data Science: Visualize, Model, Transform, Tidy, and Import Data. O’Reilly Media. https://r4ds.had.co.nz/index.html",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "040-inference.html",
    "href": "040-inference.html",
    "title": "3  Inference",
    "section": "",
    "text": "3.1 What is it?\nStatistical inference, according to Gelman et al. (2021), chap. 1.1, faces the challenge of generalizing from the particular to the general.\nIn more details, this amounts to generalizing from …",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "040-inference.html#what-is-it",
    "href": "040-inference.html#what-is-it",
    "title": "3  Inference",
    "section": "",
    "text": "a sample to a population\na treatment to a control group (i.e., causal inference)\nobserved measurement to the underlying (“latent”) construct of interest\n\n\n\n\n\n\n\nImportant\n\n\n\nStatistical inference is concerned with making general claims from particular data using mathematical tools.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "040-inference.html#population-and-sample",
    "href": "040-inference.html#population-and-sample",
    "title": "3  Inference",
    "section": "3.2 Population and sample",
    "text": "3.2 Population and sample\nWe want to have an estimate of some population value, for example the proportion of A.\nHowever, all we have is a subset, a sample of the populuation. Hence, we need to infer from the sample to the popluation. We do so by generalizing from the sample to the population, see Figure Figure 3.1.\n\n\n\n\n\n\n\n\n\n\n\n(a) Population\n\n\n\n\n\n\n\n\n\n\n\n(b) Sample\n\n\n\n\n\n\n\nFigure 3.1: Population vs. sample (Image credit: Karsten Luebke)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "040-inference.html#whats-not-inference",
    "href": "040-inference.html#whats-not-inference",
    "title": "3  Inference",
    "section": "3.3 What’s not inference?",
    "text": "3.3 What’s not inference?\nConsider fig. Figure 3.2 which epitomizes the difference between descriptive and inferential statistics.\n\n\n\n\n\n\n\n\nFigure 3.2: The difference between description and inference",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "040-inference.html#when-size-helps",
    "href": "040-inference.html#when-size-helps",
    "title": "3  Inference",
    "section": "3.4 When size helps",
    "text": "3.4 When size helps\nLarger samples allow for more precise estimations (ceteris paribus).\n\n\n\n\n\nSample size in motion, Image credit: Karsten Luebke",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "040-inference.html#what-flavors-are-available",
    "href": "040-inference.html#what-flavors-are-available",
    "title": "3  Inference",
    "section": "3.5 What flavors are available?",
    "text": "3.5 What flavors are available?\nTypically, when one hears “inference” one thinks of p-values and null hypothesis testing. Those procedures are examples of the school of Frequentist statistics.\nHowever, there’s a second flavor of statistics to be mentioned here: Bayesian statistics.\n\n3.5.1 Frequentist inference\nFrequentism is not concerned about the probability of your research hypothesis.\nFrequentism is all about controlling the long-term error. For illustration, suppose you are the CEO of a factory producing screws, and many of them. As the boss, you are not so much interested if a particular scree is in order (or faulty). Rather you are interested that the overall, long-term error rate of your production is low. One may add that your goal might not the minimize the long-term error, b ut to control it to a certain level - it may be to expensive to produce super high quality screws. Some decent, but cheap screws, might be more profitable.\n\n\n3.5.2 Bayes inference\nBayes inference is concerned about the probability of your research hypothesis.\nIt simply redistributes your beliefs based on new data (evidence) you observe, see Figure Figure 3.3.\n\n\n\n\n\n\nflowchart LR\n  A(prior belief) --&gt; B(new data) --&gt; C(posterior belief)\n\n\n\n\n\nFigure 3.3: Bayesian belief updating\n\n\n\n\n\nIn more detail, the posterior belief is formalized as the posterior probability. The Likelihood is the probability of the data given some hypothesis. The normalizing constant serves to give us a number between zero and one.\n\\[\\overbrace{\\Pr(\\color{blue}{H}|\\color{green}{D})}^\\text{posterior probability} = \\overbrace{Pr(\\color{blue}{H})}^\\text{prior} \\frac{\\overbrace{Pr(\\color{green}{D}|\\color{blue}{H})}^\\text{likelihood}}{\\underbrace{Pr(\\color{green}{D})}_{\\text{normalizing constant}}}\\]\nIn practice, the posterior probability of your hypothesis is, the average of your prior and the Likelihood of your data.\n\n\n\nPrior-Likelihood-Posterior\n\n\nCan you see that the posterior is some average of prior and likelihood?\nCheck out this great video on Bayes Theorem by 3b1b.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "040-inference.html#but-which-one-should-i-consume",
    "href": "040-inference.html#but-which-one-should-i-consume",
    "title": "3  Inference",
    "section": "3.6 But which one should I consume?",
    "text": "3.6 But which one should I consume?\nPRO Frequentist:\n\nYour supervisor and reviewers will be more familiar with it\nThe technical overhead is simpler compared to Bayes\n\nPRO Bayes:\n\nYou’ll probably want to have a posterior probability of your hypothesis\nYou may appear as a cool kid and an early adoptor of emering statistical methods\n\n\n\n\n\n\n\nTip\n\n\n\nYou’ll learn that the technical setup used for doing Bayes statistics is quite similar to doing frequentist statistics. Stay tuned.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "040-inference.html#lab",
    "href": "040-inference.html#lab",
    "title": "3  Inference",
    "section": "3.7 Lab",
    "text": "3.7 Lab\nConsider your (most pressing) research question. Assess whether it is more accessible via Frequentist or via Bayesian statistics. Explain your reasoning.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "040-inference.html#comment-from-xkcd",
    "href": "040-inference.html#comment-from-xkcd",
    "title": "3  Inference",
    "section": "3.8 Comment from xkcd",
    "text": "3.8 Comment from xkcd\n\n\n\n\n\n\n\n\n\nQuelle",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "040-inference.html#p-value",
    "href": "040-inference.html#p-value",
    "title": "3  Inference",
    "section": "3.9 p-value",
    "text": "3.9 p-value\nThe p-value has been used as the pivotal criterion to decide about whether or not a research hypothesis were to be “accepted” (a term forbidden in frequentist and Popperian langauge) or to be rejected. However, more recently, it is advised to use the p-value only as one indicator among multiple; see Wasserstein et al. (2019).\n\n\n\n\n\n\nImportant\n\n\n\nThe p-value is defined as the probability of obtaining the observed data (or more extreme) under the assumption of no effect.\n\n\nFigure Figure 3.4 visualizes the p-value.\n\n\n\n\n\n\n\n\nFigure 3.4: Visualization of the p-value",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "040-inference.html#some-confusion-remains-about-the-p-value",
    "href": "040-inference.html#some-confusion-remains-about-the-p-value",
    "title": "3  Inference",
    "section": "3.10 Some confusion remains about the p-value",
    "text": "3.10 Some confusion remains about the p-value\n\n\n\nSource: from ImgFlip Meme Generator",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "040-inference.html#exercises",
    "href": "040-inference.html#exercises",
    "title": "3  Inference",
    "section": "3.11 Exercises",
    "text": "3.11 Exercises\n\n👨‍🏫 Check-out all exercises from Datenwerk with the tag inference. For Bayesian inference, check out the tag bayes on the same website.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "040-inference.html#going-further",
    "href": "040-inference.html#going-further",
    "title": "3  Inference",
    "section": "3.12 Going further",
    "text": "3.12 Going further\nGoodman (2008) provides an entertaining overview on typical misconceptions of the p-value full text. Poldrack (2022) provides a fresh, accessible and sound introduction to statistical inference; in addition Cetinkaya-Rundel & Hardin (2021) is a worthwhile treatise.\n\n\n\n\nCetinkaya-Rundel, M., & Hardin, J. (2021). Introduction to Modern Statistics. https://openintro-ims.netlify.app/\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nGoodman, S. (2008). A dirty dozen: Twelve P-value misconceptions. Seminars in Hematology, 45(3), 135–140. https://doi.org/10.1053/j.seminhematol.2008.04.003\n\n\nPoldrack, R. (2022). Statistical Thinking for the 21st Century. https://statsthinking21.github.io/statsthinking21-core-site/index.html\n\n\nWasserstein, R. L., Schirm, A. L., & Lazar, N. A. (2019). Moving to a world beyond “ p p \\(&lt;\\) 0.05.” The American Statistician, 73, 1–19. https://doi.org/10.1080/00031305.2019.1583913",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "050-regression1.html",
    "href": "050-regression1.html",
    "title": "4  Modelling and regression",
    "section": "",
    "text": "4.1 R packages needed for this chapter\nlibrary(easystats)\nlibrary(tidyverse)\nlibrary(rstanarm)  # optional!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#whats-modelling",
    "href": "050-regression1.html#whats-modelling",
    "title": "4  Modelling and regression",
    "section": "4.2 What’s modelling?",
    "text": "4.2 What’s modelling?\nRead this great introduction by modelling by Russel Poldrack. Actually, the whole book is nice Poldrack (2022).\nAn epitome of modelling is this, let’s call it the fundamental modelling equation, a bit grandiose but at the point, see Equation 4.1.\nThe data can be separated in the model’s prediction and the rest (the “error”), i.e., what’s unaccounted for by the model.\n\\[\n\\text{data} = \\text{model} + \\text{error}\n\\tag{4.1}\\]\nA more visual account of our basic modelling equation is depicted in Figure 4.1.\n\n\n\n\n\n\nflowchart LR\n  X --&gt; Y\n  error --&gt; Y\n\n\n\n\nFigure 4.1: A more visual account of our basic modelling equation",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#regression-as-the-umbrella-tool-for-modelling",
    "href": "050-regression1.html#regression-as-the-umbrella-tool-for-modelling",
    "title": "4  Modelling and regression",
    "section": "4.3 Regression as the umbrella tool for modelling",
    "text": "4.3 Regression as the umbrella tool for modelling\n\n\n\nOne regression\n\n\nSource: Image Flip\nAlternatively, venture into the forest of statistical tests as outlined e.g. here, at Uni Muenster. Proceed at your own peril.\nYou may want to ponder on this image of a decision tree of which test to choose, see Figure Figure 4.2.\n\n\n\n\n\n\nFigure 4.2: Choose your test carefully\n\n\n\n\n4.3.1 Common statistical tests are linear models\nAs Jonas Kristoffer Lindeløv tells us, we can formulate most statistical tests as a linear model, ie., a regression.\n\n\n\nCommon statistical tests as linear models\n\n\n\n\n4.3.2 How to find the regression line\nIn the simplest case, regression analyses can be interpreted geometrically as a line in a 2D coordinate system, see Figure Figure 4.3.\n\n\n\n\n\n\nFigure 4.3: Least Square Regression\n\n\n\nSource: Orzetoo, CC-SA, Wikimedia\nPut simple, we are looking for the line which is in the “middle of the points”. More precisely, we place the line such that the squared distances from the line to the points is minimal, see Figre Figure 4.3.\nConsider Figure Figure 4.4, from this source by Roback & Legler (2021). It visualizes not only the notorious regression line, but also sheds light on regression assumptions, particularly on the error distribution.\n\n\n\n\n\n\nFigure 4.4: Regression and some of its assumptions\n\n\n\nAmong the assumptions of the linear model are:\n\nlinearity of the function\nvariance of \\(y\\) remains constant across range of \\(x\\)\nnormality of residuals\n\n\n\n4.3.3 The linear model\nHere’s the canonical form of the linear model.\nConsider a model with \\(k\\) predictors:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k + \\epsilon\\]\n\n\n4.3.4 Algebraic derivation\nFor the mathematical inclined, check out this derivation of the simple case regression model. Note that the article is written in German, but your browser can effortlessly translate into English. Here’s a similar English article from StackExchange.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#in-all-its-glory",
    "href": "050-regression1.html#in-all-its-glory",
    "title": "4  Modelling and regression",
    "section": "4.4 In all its glory",
    "text": "4.4 In all its glory\n\n\n\n\n\n\n\n\n\nLet’s depict the residuals, s. Figure 4.5.\n\n\n\n\n\n\n\n\nFigure 4.5: Residuals as deviations from the predicted value",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#first-model-one-metric-predictor",
    "href": "050-regression1.html#first-model-one-metric-predictor",
    "title": "4  Modelling and regression",
    "section": "4.5 First model: one metric predictor",
    "text": "4.5 First model: one metric predictor\nFirst, let’s load some data:\n\ndata(mtcars)\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\n4.5.1 Frequentist\nDefine and fit the model:\n\nlm1_freq &lt;- lm(mpg ~ hp, data = mtcars)\n\nGet the parameter values:\n\nparameters(lm1_freq)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n30.0988605\n1.6339210\n0.95\n26.7619488\n33.4357723\n18.421246\n30\n0e+00\n\n\nhp\n-0.0682283\n0.0101193\n0.95\n-0.0888947\n-0.0475619\n-6.742388\n30\n2e-07\n\n\n\n\n\n\nPlot the model parameters:\n\nplot(parameters(lm1_freq))\n\n\n\n\n\n\n\n\n\n\n4.5.2 Bayesian\n\nlm1_bayes &lt;- stan_glm(mpg ~ hp, data = mtcars)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.003812 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 38.12 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.066 seconds (Warm-up)\nChain 1:                0.063 seconds (Sampling)\nChain 1:                0.129 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.064 seconds (Warm-up)\nChain 2:                0.058 seconds (Sampling)\nChain 2:                0.122 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2.2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.066 seconds (Warm-up)\nChain 3:                0.06 seconds (Sampling)\nChain 3:                0.126 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.7e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.062 seconds (Warm-up)\nChain 4:                0.057 seconds (Sampling)\nChain 4:                0.119 seconds (Total)\nChain 4: \n\n\nActually, we want to suppress some overly verbose output of the sampling, so add the argument refresh = 0:\n\nlm1_bayes &lt;- stan_glm(mpg ~ hp, data = mtcars, refresh = 0)\n\nGet the parameter values:\n\nparameters(lm1_bayes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\nCI\nCI_low\nCI_high\npd\nRhat\nESS\nPrior_Distribution\nPrior_Location\nPrior_Scale\n\n\n\n\n(Intercept)\n30.0590424\n0.95\n26.7439165\n33.3124516\n1\n1.000771\n2982.289\nnormal\n20.09062\n15.0673701\n\n\nhp\n-0.0679057\n0.95\n-0.0882979\n-0.0472594\n1\n1.000275\n3533.338\nnormal\n0.00000\n0.2197599\n\n\n\n\n\n\nPlot the model parameters:\n\nplot(parameters(lm1_bayes))\n\n\n\n\n\n\n\n\n\n\n4.5.3 Model performance\n\nr2(lm1_freq)\n\n# R2 for Linear Regression\n       R2: 0.602\n  adj. R2: 0.589\n\n\n\nr2(lm1_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.583 (95% CI [0.379, 0.753])\n\n\n\n\n4.5.4 Model check\nHere’s a bunch of typical model checks in the Frequentist sense.\n\ncheck_model(lm1_freq)\n\n\n\n\n\n\n\n\nAnd here are some Bayesian flavored model checks.\n\ncheck_model(lm1_bayes)\n\n\n\n\n\n\n\n\n\n\n4.5.5 Get some predictions\n\nlm1_pred &lt;- estimate_relation(lm1_freq)\nlm1_pred\n\n\n\n\n\nhp\nPredicted\nSE\nCI_low\nCI_high\n\n\n\n\n52.00000\n26.550990\n1.1766139\n24.148024\n28.95396\n\n\n83.44444\n24.405590\n0.9358933\n22.494241\n26.31694\n\n\n114.88889\n22.260189\n0.7548971\n20.718484\n23.80190\n\n\n146.33333\n20.114789\n0.6828911\n18.720139\n21.50944\n\n\n177.77778\n17.969389\n0.7518697\n16.433866\n19.50491\n\n\n209.22222\n15.823989\n0.9310065\n13.922620\n17.72536\n\n\n240.66667\n13.678588\n1.1707841\n11.287528\n16.06965\n\n\n272.11111\n11.533188\n1.4412478\n8.589767\n14.47661\n\n\n303.55556\n9.387788\n1.7280486\n5.858642\n12.91693\n\n\n335.00000\n7.242387\n2.0242544\n3.108308\n11.37647\n\n\n\n\n\n\nMore details on the above function can be found on the respective page at the easystats site.\n\n\n4.5.6 Plot the model\n\nplot(lm1_pred)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#more-of-this",
    "href": "050-regression1.html#more-of-this",
    "title": "4  Modelling and regression",
    "section": "4.6 More of this",
    "text": "4.6 More of this\nMore technical details for gauging model performance and model quality, can be found on the site of the R package “performance at the easystats site.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#lab",
    "href": "050-regression1.html#lab",
    "title": "4  Modelling and regression",
    "section": "4.7 Lab",
    "text": "4.7 Lab\nRun a simple regression on your own research data. Present the results. Did you encounter any glitches?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#bayes-members-only",
    "href": "050-regression1.html#bayes-members-only",
    "title": "4  Modelling and regression",
    "section": "4.8 Bayes-members only",
    "text": "4.8 Bayes-members only\nBayes statistics provide a distribution as the result of the analysis, the posterior distribution, which provides us with quite some luxury.\nAs the posterior distribution manifests itself by a number of samples, we can easily filter and manipulate this sample distribution in order to ask some interesing questions.\nSee\n\nlm1_bayes_tibble &lt;- as_tibble(lm1_bayes)  # cast as a tibble (table)\n\nhead(lm1_bayes_tibble)  # show the first few rows\n\n\n\n\n\n(Intercept)\nhp\nsigma\n\n\n\n\n29.24798\n-0.0585108\n4.148574\n\n\n31.44328\n-0.0793732\n3.583081\n\n\n31.66962\n-0.0810650\n3.406428\n\n\n29.41583\n-0.0611019\n4.993273\n\n\n28.85437\n-0.0658251\n3.520692\n\n\n31.20567\n-0.0744505\n4.846585\n\n\n\n\n\n\n\n4.8.1 Asking for probabilites\nWhat’s the probability that the effect of hp is negative?\n\nlm1_bayes_tibble %&gt;% \n  count(hp &lt; 0)\n\n\n\n\n\nhp &lt; 0\nn\n\n\n\n\nTRUE\n4000\n\n\n\n\n\n\nFeel free to ask similar questions!\n\n\n4.8.2 Asking for quantiles\nWith a given probability of, say 90%, how large is the effect of hp?\n\nquantile(lm1_bayes_tibble$hp, .9)\n\n       90% \n-0.0547369 \n\n\nWhat’s the smallest 95% percent interval for the effect of hp?\n\nhdi(lm1_bayes)\n\n\n\n\n\nParameter\nCI\nCI_low\nCI_high\nEffects\nComponent\n\n\n\n\n(Intercept)\n0.95\n26.8254064\n33.3821880\nfixed\nconditional\n\n\nhp\n0.95\n-0.0874363\n-0.0469058\nfixed\nconditional\n\n\n\n\n\n\nIn case you prefer 89% intervals (I do!):\n\nhdi(lm1_bayes, ci = .89)\n\n\n\n\n\nParameter\nCI\nCI_low\nCI_high\nEffects\nComponent\n\n\n\n\n(Intercept)\n0.89\n27.4210227\n32.6653275\nfixed\nconditional\n\n\nhp\n0.89\n-0.0847388\n-0.0522295\nfixed\nconditional\n\n\n\n\n\n\n\n\n4.8.3 Model specification\nIn Bayes statistics, it is customary to specify the model in something like the following way:\n\\[\\begin{aligned}\ny_i &\\sim N(\\mu_i,\\sigma)\\\\\n\\mu_i &= \\beta_0 + \\beta_1 x_i\\\\\n\\beta_0, \\beta_1 &\\sim N(0, 1) \\\\\n\\sigma &\\sim E(1)\n\\end{aligned}\\]\nIn this specification, \\(N\\) refers to the normal distribution, and \\(E\\) to the exponential distribution. Furthermore, this model assumes that the \\(X\\) and \\(Y\\) are given in standard units.\n\n\n4.8.4 Prediction interval\nA prediction interval answers the following question\n\nHow large is the uncertainty in \\(y\\) associated with a given obersation? What interval of values should I expect for a randomly chosen observation?\n\nFor example, what’s the uncertainty attached to the fuel economy of a car with 100 hp?\n\nestimate_prediction(model = lm1_bayes, \n                    data = tibble(hp = 100) )\n\n\n\n\n\nhp\nPredicted\nSE\nCI_low\nCI_high\n\n\n\n\n100\n23.26178\n4.156918\n15.40819\n31.53782\n\n\n\n\n\n\n\n\n4.8.5 … And more\nWe could even ask intriguing questions such as\n\nGiven the model, and given two random observations, one from the experimental group and one from the control group, what is the probability that observation 1 has a higher value in \\(Y\\) than observation 2 has?\n\nNote that we are not only asking for “typical” observations as predicted by the model but we are also taking into account the uncertainty of the prediction for each group. Hence, this kind of questions is likely to yield more realistic (and less clear-cut) answers than just asking for the typical value. In other words, such analyses draw on the posterior predictive distribution.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#multiple-metric-predictors",
    "href": "050-regression1.html#multiple-metric-predictors",
    "title": "4  Modelling and regression",
    "section": "4.9 Multiple metric predictors",
    "text": "4.9 Multiple metric predictors\nAssume we have a theory that dictates that fuel economy is a (causal) function of horse power and engine displacement.\n\nlm2_freq &lt;- lm(mpg ~ hp + disp, data = mtcars)\nparameters(lm2_freq)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n30.7359042\n1.3315661\n0.95\n28.0125457\n33.4592628\n23.082522\n29\n0.0000000\n\n\nhp\n-0.0248401\n0.0133855\n0.95\n-0.0522165\n0.0025363\n-1.855746\n29\n0.0736791\n\n\ndisp\n-0.0303463\n0.0074049\n0.95\n-0.0454909\n-0.0152016\n-4.098159\n29\n0.0003063\n\n\n\n\n\n\nSimilarly for Bayes inference:\n\nlm2_bayes &lt;- stan_glm(mpg ~ hp + disp, data = mtcars)\n\nResults\n\nparameters(lm2_bayes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\nCI\nCI_low\nCI_high\npd\nRhat\nESS\nPrior_Distribution\nPrior_Location\nPrior_Scale\n\n\n\n\n(Intercept)\n30.7393959\n0.95\n27.9953578\n33.4954849\n1.000\n0.9999147\n4336.162\nnormal\n20.09062\n15.0673701\n\n\nhp\n-0.0243992\n0.95\n-0.0525050\n0.0036740\n0.957\n1.0021956\n2041.955\nnormal\n0.00000\n0.2197599\n\n\ndisp\n-0.0307138\n0.95\n-0.0458102\n-0.0153927\n1.000\n1.0014999\n2001.424\nnormal\n0.00000\n0.1215712\n\n\n\n\n\nplot(parameters(lm2_bayes))\n\n\n\n\n\n\n\nr2(lm2_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.733 (95% CI [0.580, 0.845])\n\n\nDepending on the value of disp the prediction of mpg from hp will vary:\n\nlm2_pred &lt;- estimate_relation(lm2_freq)\nplot(lm2_pred)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#one-nominal-predictor",
    "href": "050-regression1.html#one-nominal-predictor",
    "title": "4  Modelling and regression",
    "section": "4.10 One nominal predictor",
    "text": "4.10 One nominal predictor\n\nlm3a &lt;- lm(mpg ~ am, data = mtcars)\nparameters(lm3a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n17.147368\n1.124602\n0.95\n14.85062\n19.44411\n15.247492\n30\n0.000000\n\n\nam\n7.244939\n1.764422\n0.95\n3.64151\n10.84837\n4.106127\n30\n0.000285\n\n\n\n\n\n\n\nlm3a_means &lt;- estimate_means(lm3a, at = \"am = c(0, 1)\")\nlm3a_means \n\n\n\n\n\nam\nMean\nSE\nCI_low\nCI_high\n\n\n\n\n0\n17.14737\n1.124602\n14.85062\n19.44411\n\n\n1\n24.39231\n1.359578\n21.61568\n27.16894\n\n\n\n\n\n\nIf we were not to specify the values of am which we would like to get predictions for, the default of the function would select 10 values, spread across the range of am. For numeric variables, this is usually fine. However, for nominal variables - and am is in fact a nominally scaled variable - we insist that we want predictions for the levels of the variable only, that is for 0 and 1.\nHowever, unfortunately, the plot needs a nominal variable if we are to compare groups. In our case, am is considered a numeric variables, since it consists of numbers only. The plot does not work, malheureusement:\n\nplot(lm3a_means)\n\nError in `rlang::sym()`:\n! Can't convert a character vector to a symbol.\n\n\nWe need to transform am to a factor variable. That’s something like a string. If we hand over a factor() to the plotting function, everything will run smoothly. Computationwise, no big differences:\n\nmtcars2 &lt;-\n  mtcars %&gt;% \n  mutate(am_f = factor(am))\n\nlm3a &lt;- lm(mpg ~ am_f, data = mtcars2)\nparameters(lm3a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n17.147368\n1.124602\n0.95\n14.85062\n19.44411\n15.247492\n30\n0.000000\n\n\nam_f1\n7.244939\n1.764422\n0.95\n3.64151\n10.84837\n4.106127\n30\n0.000285\n\n\n\n\n\n\n\nlm3a_means &lt;- estimate_means(lm3a)\nplot(lm3a_means)\n\n\n\n\n\n\n\n\nNote that we should have converted am to a factor variable before fitting the model. Otherwise, the plot won’t work.\nHere’s a more hand-crafted version of the last plot, see Fig. Figure 4.6.\n\nggplot(mtcars2) +\n  aes(x = am_f, y = mpg) +\n  geom_violin() +\n  geom_jitter(width = .1, alpha = .5) +\n  geom_pointrange(data = lm3a_means,\n                  color = \"orange\",\n                  aes(ymin = CI_low, ymax = CI_high, y = Mean)) +\n  geom_line(data = lm3a_means, aes(y = Mean, group = 1))\n\n\n\n\n\n\n\nFigure 4.6: Means per level of am",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#one-metric-and-one-nominal-predictor",
    "href": "050-regression1.html#one-metric-and-one-nominal-predictor",
    "title": "4  Modelling and regression",
    "section": "4.11 One metric and one nominal predictor",
    "text": "4.11 One metric and one nominal predictor\n\nmtcars2 &lt;-\n  mtcars %&gt;% \n  mutate(cyl = factor(cyl))\n\nlm4 &lt;- lm(mpg ~ hp + cyl, data = mtcars2)\nparameters(lm4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n28.6501182\n1.5877870\n0.95\n25.3976840\n31.9025524\n18.044056\n28\n0.0000000\n\n\nhp\n-0.0240388\n0.0154079\n0.95\n-0.0556005\n0.0075228\n-1.560163\n28\n0.1299540\n\n\ncyl6\n-5.9676551\n1.6392776\n0.95\n-9.3255631\n-2.6097471\n-3.640418\n28\n0.0010921\n\n\ncyl8\n-8.5208508\n2.3260749\n0.95\n-13.2855993\n-3.7561022\n-3.663188\n28\n0.0010286\n\n\n\n\n\n\n\nlm4_pred &lt;- estimate_relation(lm4)\nplot(lm4_pred)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#watch-out-for-simpson",
    "href": "050-regression1.html#watch-out-for-simpson",
    "title": "4  Modelling and regression",
    "section": "4.12 Watch out for Simpson",
    "text": "4.12 Watch out for Simpson\nBeware! Model estimates can swing wildly if you add (or remove) some predictor from your model. See this post for an demonstration.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#what-about-correlation",
    "href": "050-regression1.html#what-about-correlation",
    "title": "4  Modelling and regression",
    "section": "4.13 What about correlation?",
    "text": "4.13 What about correlation?\nCorrelation is really a close cousin to regression. In fact, regression with standardized variables amounts to correlation.\nLet’s get the correlation matrix of the variables in involved in lm4.\n\nlm4_corr &lt;- \n  mtcars %&gt;% \n  select(mpg, hp, disp) %&gt;% \n  correlation()\n\nlm4_corr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\nt\ndf_error\np\nMethod\nn_Obs\n\n\n\n\nmpg\nhp\n-0.7761684\n0.95\n-0.8852686\n-0.5860994\n-6.742388\n30\n2e-07\nPearson correlation\n32\n\n\nmpg\ndisp\n-0.8475514\n0.95\n-0.9233594\n-0.7081376\n-8.747151\n30\n0e+00\nPearson correlation\n32\n\n\nhp\ndisp\n0.7909486\n0.95\n0.6106794\n0.8932775\n7.080122\n30\n1e-07\nPearson correlation\n32\n\n\n\n\n\n\n\nplot(summary(lm4_corr))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#exercises",
    "href": "050-regression1.html#exercises",
    "title": "4  Modelling and regression",
    "section": "4.14 Exercises",
    "text": "4.14 Exercises\n\nmtcars simple 1\nmtcars simple 2\nmtcars simple 3\n\n\n🧑‍🎓 I want more!\n\n\n👨‍🏫 Checkout all exercises tagged with “regression” on datenwerk. Pro-Tipp: Use the translation function of your browers to translate the webpage into your favorite language.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#case-study",
    "href": "050-regression1.html#case-study",
    "title": "4  Modelling and regression",
    "section": "4.15 Case study",
    "text": "4.15 Case study\n\nPrices of Boston houses, first part\nModeling movie succes, first part",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#lab-1",
    "href": "050-regression1.html#lab-1",
    "title": "4  Modelling and regression",
    "section": "4.16 Lab",
    "text": "4.16 Lab\nGet your own data, and build a simple model reflecting your research hypothesis. If you are lacking data (or hypothesis) get something close to it.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#going-further",
    "href": "050-regression1.html#going-further",
    "title": "4  Modelling and regression",
    "section": "4.17 Going further",
    "text": "4.17 Going further\nAn accessible treatment of regression is provided by Ismay & Kim (2020).\nRoback & Legler (2021) provide a more than introductory account of regression while being accessible. A recent but already classic book (if this is possible) is the book by Gelman et al. (2021). You may also benefit from Poldrack (2022) (open access).\nFor a gentle introduction to the basics of modelling, see ModernDive Chap. 5.0 (Ismay & Kim, 2020), and get the R code here. For slightly more advanced topics on linear regression such as multiple regression and interaction, see ModernDive Chap. 6, and get the R code here.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "050-regression1.html#debrief",
    "href": "050-regression1.html#debrief",
    "title": "4  Modelling and regression",
    "section": "4.18 Debrief",
    "text": "4.18 Debrief\nScience!\n\n\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nIsmay, C., & Kim, A. Y.-S. (2020). Statistical inference via data science: A ModernDive into R and the Tidyverse. CRC Press / Taylor & Francis Group. https://moderndive.com/\n\n\nPoldrack, R. (2022). Statistical thinking for the 21st century. https://statsthinking21.github.io/statsthinking21-core-site/index.html\n\n\nRoback, P., & Legler, J. (2021). Beyond multiple linear regression: Applied generalized linear models and multilevel models in (1st ed.). CRC Press.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modelling and regression</span>"
    ]
  },
  {
    "objectID": "060-regression2.html",
    "href": "060-regression2.html",
    "title": "5  More linear models",
    "section": "",
    "text": "5.1 R-packages needed",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>More linear models</span>"
    ]
  },
  {
    "objectID": "060-regression2.html#r-packages-needed-for-this-chapter",
    "href": "060-regression2.html#r-packages-needed-for-this-chapter",
    "title": "5  More linear models",
    "section": "5.2 R packages needed for this chapter",
    "text": "5.2 R packages needed for this chapter\n\nlibrary(easystats)\nlibrary(tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>More linear models</span>"
    ]
  },
  {
    "objectID": "060-regression2.html#multiplicative-associations",
    "href": "060-regression2.html#multiplicative-associations",
    "title": "5  More linear models",
    "section": "5.3 Multiplicative associations",
    "text": "5.3 Multiplicative associations\n\n5.3.1 The Log-Y model\nConsider again the linear model, in a simple form:\n\\[\\hat{y} = \\beta_0 + \\beta_1 x_1 +  \\ldots + b_kx_k +\\] Surprisingly, we can use this linear model to describe multiplicative assocations:\n\\(\\hat{y} = e^{b_0 + b_1x_1 + b_2x_2 + \\ldots + b_kx_k}\\)\n(I wrote b instead of \\(\\beta\\) just to show that both has its meaning, but are separate things.)\nExponentiate both sides to get:\n\\(log (\\hat{y}) = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_kx_k\\)\nFor simplicity, let’s drop the subscripts in the following without loss of generality and keep it short:\n\\(y = e^{x}\\), with \\(e \\approx 2.71...\\)\nExponentiate both sides to get:\n\\(log(y) = x\\)\nThis association is called multiplicative, because if x increases by 1, y increased by a constant factor.\n\n\n\n\n\n\nNote\n\n\n\nThe logarithm is not defined for negative (input) values. And \\(log(0) = -\\infty\\).\n\n\nA side-effect of modelling log_y instead of y is that the distribution shape of the outcome variable changes. This can be useful at times.\nLog-Y Regression can usefully be employed for modelling growth, among other, see Example 5.1.\n\nExample 5.1 (Bacteria growth) Some bacteria dish grows with at a fixed proportional rate, that is it doubles its population size in a fixed period of time. This is what is called exponential growth. For concreteness, say, the bacteriae double each two days, starting with 1 unit of bacteria.\nAfter about three weeks, i.e., 10 doubling periods (20 days), we’ll have \\(y\\) units of bacteriae:\n\ne &lt;- 2.7178\ny &lt;- e^10\ny\n\n[1] 21987.45\n\n\nQuite a bit! More than 20 thousand times more than before.\n\n\n\n5.3.2 Exercise\n\nEffect of education on income\nEffect of log-y transformation on the distribution, an example\n\n\n\n\n\n\n\nNote\n\n\n\nThe exercises are written in German Language. Don’t fret. Browsers are able to translate websites instantaneously. Alternatively, go to sites such as Google Translate and enter the URL of the website to be translated. Also check out the webstor of your favorite browser to get an extention such as this one for Google Chrome.\n\n\n\n\n5.3.3 Visualizing Log Transformation\nCheck out this post for an example of a log-y regression visualized.\nThis post puts some more weight to the argument that a log-y transformation is useful (if you want to model multiplicative relations).\n\n\n5.3.4 Further reading\nCheck out this great essay by Kenneth Benoit on different log-variants in regression. Also Gelman et al. (2021), chapter 12 (and others), is useful.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>More linear models</span>"
    ]
  },
  {
    "objectID": "060-regression2.html#interaction",
    "href": "060-regression2.html#interaction",
    "title": "5  More linear models",
    "section": "5.4 Interaction",
    "text": "5.4 Interaction\n\n5.4.1 Multiple predictors, no interaction\nRegression analyses can be used with more than one predictor, see Figure Figure 5.1.\n\n\n\n\n\n\nflowchart LR\nX --&gt; Y1\n\nX1 --&gt; Y2\nX2 --&gt; Y2\n\n\n\n\nFigure 5.1: One predictor (X) vs. two predictors (X1, X2)\n\n\n\n\n\ngiven by Figure Figure 5.2, where a 3D account of a regression is given. 3D means to input variables, and (which is always the case) one output variable.\n\n\n\n\n\n\nNote\n\n\n\nNote that the slope in linear in both axis (X1 and X2).\n\n\nA different perspective is shown here,\nwhere a 3D account of a regression is given. 3D means to input variables, and (which is always the case) one output variable.\n\n\n\n\n\n\nImportant\n\n\n\nIf the slope for one predictor is the same for all values of the other predictor, then we say that no interaction is taking place.\n\n\nHere’s a visualization of a 3D regression plane (not line) without interaction: constant slope in one axis, see the following figure, Figure 5.2. The three cubes show the same data, just turned by different degrees (along the z axis).\n\n\n\n\n\n\n\n\n\n\n\n(a) seen from angle 1\n\n\n\n\n\n\n\n\n\n\n\n(b) seen from angle 2\n\n\n\n\n\n\n\n\n\n\n\n(c) seen from angle 3\n\n\n\n\n\n\n\nFigure 5.2: 3D regression plane (not a line) without interaction\n\n\n\nNote that in the above figure, the slope in each predictor axis equals 1, boringly. Hence the according 2D plots are boring, too.\nFor the sake of an example, consider this linear model:\n\\(mpg \\sim hp + disp\\)\nOr, in more regression like terms:\n\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\), where x1 is hp and x2 is disp in the mtcars dataset.\nIn R terms:\n\nlm3d &lt;- lm(mpg ~ hp + disp, data = mtcars)\n\nThe 3D plot is shown in Figure Figure 5.3.\n\n\n\n\n\n\n\n\nFigure 5.3: mpg ~ hp + disp\n\n\n\n\n\nHere are the two corresponding 2d (1 predictor) regression models:\nlm1 &lt;- lm(mpg ~ hp, data = mtcars)\nplot(estimate_relation(lm1))\nlm2 &lt;- lm(mpg ~ disp, data = mtcars)\nplot(estimate_relation(lm2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheckout this post for a visually slightly more appealing 3d regression plane.\n\n\n5.4.2 Interaction\nFor interaction to happen we relax the assumption that the slope of predictor 1 must be constant for all values of predictor 2.\nIn R, we specify an interaction model like this:\n\nlm3d_interact &lt;- lm(mpg ~ hp + disp + hp:disp, data = mtcars)\n\nThe symbol hp:disp can be read as “the interaction effect of hp and disp”.\nHere’s a visual account, see Figure Figure 5.4.\n\n\n\n\n\n\n\n\nFigure 5.4: mpg ~ hp + disp\n\n\n\n\n\nCompare Figure 5.4 and Figure 5.3. The difference is the interaction effect.\nIn Figure 5.4 you’ll see that the lines along the Y axis are not parallel anymore. Similarly, the lines along the X axis are not parallel anymore.\n\n\n\n\n\n\nImportant\n\n\n\nIf the regression lines (indicating different values of one predictor) are not parallel, we say that an interaction effect is taking place.\n\n\nHowever, the difference or change between two adjacent values (lines) is constant. This value is the size the regression effect.\n\n\n5.4.3 Interaction made simple\nIf you find that two sophisticated, consider the following simple case.\nFirst, we mutate am to be a factor variable, in order to make things simpler (without loss of generality).\n\nmtcars2 &lt;-\n  mtcars %&gt;% \n  mutate(am_f = factor(am))\n\nNow we use this new variable for a simple regression model:\n\nlm_interact_simple &lt;- lm(mpg ~ disp + am_f + disp:am_f, data = mtcars2)\n\nHere’s the plot, Figure Figure 5.5.\n\nplot(estimate_relation(lm_interact_simple))\n\n\n\n\n\n\n\nFigure 5.5: A simple interaction model\n\n\n\n\n\nIn this picture, we see that the two regression lines are not parallel, and hence there is evidence of an interaction effect.\nThe interaction effect amounts to the difference in slops in Figure 5.5.\nOne might be inclined to interpret Figure Figure 5.5 as an 3D image, where the one (reddish) line is in the foreground and the blueish line in the background (or vice versa, as you like). Given a 3D image (and hence 2 predictors), we are where we started further above.\nFor completeness, here are the parameters of the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(28)\np\n\n\n\n\n(Intercept)\n25.16\n1.93\n(21.21, 29.10)\n13.07\n&lt; .001\n\n\ndisp\n-0.03\n6.22e-03\n(-0.04, -0.01)\n-4.44\n&lt; .001\n\n\nam f (1)\n7.71\n2.50\n(2.58, 12.84)\n3.08\n0.005\n\n\ndisp × am f (1)\n-0.03\n0.01\n(-0.05, -7.99e-03)\n-2.75\n0.010\n\n\n\n\n\n\n\n5.4.4 Centering variables\nThe effect of of am_f must be interpreted when disp is zero, which does not make much sense.\nTherefore it simplifies the interpretation of regression coefficients to center all input variables, by subtrating the mean value (“demeaning” or “centering”):\n\\[x' = x - \\bar{x}\\] In R, this can be achieved e.g,. in this way:\n\nmtcars3 &lt;- \nmtcars2 %&gt;% \n  mutate(disp_c = disp - mean(disp))\n\n\nlm_interact_simple2 &lt;- lm(mpg ~ disp_c + am_f + disp_c:am_f, data = mtcars3)\nparameters(lm_interact_simple2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n18.7929250\n0.7631321\n0.95\n17.2297199\n20.3561302\n24.6260457\n28\n0.0000000\n\n\ndisp_c\n-0.0275836\n0.0062190\n0.95\n-0.0403225\n-0.0148447\n-4.4354101\n28\n0.0001295\n\n\nam_f1\n0.4517578\n1.3915089\n0.95\n-2.3986189\n3.3021346\n0.3246532\n28\n0.7478567\n\n\ndisp_c:am_f1\n-0.0314548\n0.0114574\n0.95\n-0.0549242\n-0.0079855\n-2.7453781\n28\n0.0104373",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>More linear models</span>"
    ]
  },
  {
    "objectID": "060-regression2.html#predictor-relevance",
    "href": "060-regression2.html#predictor-relevance",
    "title": "5  More linear models",
    "section": "5.5 Predictor relevance",
    "text": "5.5 Predictor relevance\nGiven a model, we might want to know which predictor has the strongest association with the outcome?\nIn order to answer this question, all predictor must have the same scale. Otherwise the importance of a predictor would increase by 1000, if we multiply each of the observations’ values by the same factor. However, this multiplication should not change the relevance of a predictor.\nA simple solution is to standardize all predictors to the same scale (sd=1).\n\nmtcars4 &lt;-\n  mtcars %&gt;% \n  standardize(select = c(\"disp\", \"hp\", \"cyl\"))\n\nBy the way, “standardizing” centers the variable by default to a mean value of zero (by demeaning).\nSee:\n\nhead(mtcars4$disp)\n\n[1] -0.57061982 -0.57061982 -0.99018209  0.22009369  1.04308123 -0.04616698\n\nhead(mtcars$disp)\n\n[1] 160 160 108 258 360 225\n\n\nHere’s the SD:\n\nsd(mtcars4$disp)\n\n[1] 1\n\nsd(mtcars$disp)\n\n[1] 123.9387\n\n\nAnd here’s the mean value:\n\nmean(mtcars4$disp)\n\n[1] -9.084937e-17\n\nmean(mtcars$disp)\n\n[1] 230.7219\n\n\nNow we are in a position to decide which predictor is more important:\n\nm &lt;- lm(mpg ~ disp + hp + cyl, data = mtcars4)\nparameters(m)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n20.090625\n0.5400989\n0.95\n18.984282\n21.1969675\n37.198045\n28\n0.0000000\n\n\ndisp\n-2.334768\n1.2894201\n0.95\n-4.976025\n0.3064896\n-1.810711\n28\n0.0809290\n\n\nhp\n-1.006457\n1.0045056\n0.95\n-3.064094\n1.0511791\n-1.001943\n28\n0.3249519\n\n\ncyl\n-2.192076\n1.4238730\n0.95\n-5.108747\n0.7245958\n-1.539516\n28\n0.1349044",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>More linear models</span>"
    ]
  },
  {
    "objectID": "060-regression2.html#exercises",
    "href": "060-regression2.html#exercises",
    "title": "5  More linear models",
    "section": "5.6 Exercises",
    "text": "5.6 Exercises\n\nPredictor relevance\nAdjusting\nAdjusting 2\nInterpreting Regression coefficients",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>More linear models</span>"
    ]
  },
  {
    "objectID": "060-regression2.html#case-studies",
    "href": "060-regression2.html#case-studies",
    "title": "5  More linear models",
    "section": "5.7 Case studies",
    "text": "5.7 Case studies\n\nPrices of Boston houses, second part\nModeling movie succes, second part\nModeling flight delays",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>More linear models</span>"
    ]
  },
  {
    "objectID": "060-regression2.html#lab",
    "href": "060-regression2.html#lab",
    "title": "5  More linear models",
    "section": "5.8 Lab",
    "text": "5.8 Lab\nGet your own data, and build a simple model reflecting your research hypothesis based on the topics covered in this chapter. If you are lacking data (or hypothesis) get something close to it.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>More linear models</span>"
    ]
  },
  {
    "objectID": "060-regression2.html#glimpse-on-parameter-estimation",
    "href": "060-regression2.html#glimpse-on-parameter-estimation",
    "title": "5  More linear models",
    "section": "5.9 Glimpse on parameter estimation",
    "text": "5.9 Glimpse on parameter estimation\nAn elegant yet simple explanation of the math of parameter estimation can be found at “go data driven”. A similar approach is presented here.\nConsider this geometric interpretation of the least square method in Figure Figure 5.6.\n\n\n\n\n\n\n\n\nFigure 5.6: Geometric interpretation of the least square method. Source: Oleg Alexandrov on Wikimedia",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>More linear models</span>"
    ]
  },
  {
    "objectID": "060-regression2.html#going-further",
    "href": "060-regression2.html#going-further",
    "title": "5  More linear models",
    "section": "5.10 Going further",
    "text": "5.10 Going further\nA recent but already classic book on regression and inference (if this is possible) is the book by Gelman et al. (2021). A great textbook on statistical modelling (with a Bayesian flavor) was written by McElreath (2020); it’s suitable for PhD level.\nMathematical foundations can be found in Deisenroth et al. (2020). Here’s a collection of online resources tapping into statistics and machine learning.\nRegression asks the Log: What you got to do here, an introduction.\n\n\n\n\nDeisenroth, M. P., Faisal, A. A., & Ong, C. S. (2020). Mathematics for machine learning. Cambridge University Press. https://mml-book.github.io/\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other stories. Cambridge University Press.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd ed.). Taylor and Francis, CRC Press.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>More linear models</span>"
    ]
  },
  {
    "objectID": "070-causality.html",
    "href": "070-causality.html",
    "title": "6  Causality",
    "section": "",
    "text": "6.1 R packages needed for this chapter\nlibrary(tidyverse)\nlibrary(ggdag)  # optional",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Causality</span>"
    ]
  },
  {
    "objectID": "070-causality.html#intro-to-causality",
    "href": "070-causality.html#intro-to-causality",
    "title": "6  Causality",
    "section": "6.2 Intro to causality",
    "text": "6.2 Intro to causality\nCheck out this talk.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Causality</span>"
    ]
  },
  {
    "objectID": "070-causality.html#literature",
    "href": "070-causality.html#literature",
    "title": "6  Causality",
    "section": "6.3 Literature",
    "text": "6.3 Literature\nRohrer (2018) provides an accessible introduction to causal inference. Slightly more advanced is the introduction by one of the leading figures of the Field, Judea Pearl, Pearl et al. (2016). If your after a text book on modelling that covers causal inference, and if you like Bayesian statistics, than you should definitely check out McElreath (2020).\n\n\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd ed.). Taylor and Francis, CRC Press.\n\n\nPearl, J., Glymour, M., & Jewell, N. P. (2016). Causal inference in statistics: A primer. Wiley.\n\n\nRohrer, J. M. (2018). Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data. Advances in Methods and Practices in Psychological Science, 1(1), 27–42. https://doi.org/10.1177/2515245917745629",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Causality</span>"
    ]
  },
  {
    "objectID": "080-outro.html",
    "href": "080-outro.html",
    "title": "7  Going further",
    "section": "",
    "text": "7.1 I want to dig deeper\nCheck-out this course on statistical inference using Bayes statistics. If you are interested in predictive modeling, check-out this couse.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Going further</span>"
    ]
  },
  {
    "objectID": "080-outro.html#more-exercises",
    "href": "080-outro.html#more-exercises",
    "title": "7  Going further",
    "section": "7.2 More exercises",
    "text": "7.2 More exercises\nThere’s a ton of exercises available at the Datenwerk.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Going further</span>"
    ]
  },
  {
    "objectID": "080-outro.html#case-studies",
    "href": "080-outro.html#case-studies",
    "title": "7  Going further",
    "section": "7.3 Case studies",
    "text": "7.3 Case studies\n\n7.3.1 Case studies on data visualization\n\nFallstudien – NUR Datenvisualisierung\n\nvis-gapminder\nvis-penguins\nvis-mtcars\nAufgabe zur Datenvisualisierung des Diamantenpreises\n\n\n\n\n7.3.2 Case studies on explorative data analysis\n\nFALLSTUDIEN - NUR EXPLORATIVE DATENANALYSE\n\nLouise E. Sinks: TidyTuesday Week 18: Portal Project\nLouise E. Sinks: TidyTuesday Week 17: London Marathon\nLouise E. Sinks: TidyTuesday Week 16: Neolithic Founder Crops\nDatenjudo mit Pinguinen\nData-Wrangling-Aufgaben zur Lebenserwartung\nCase study: data vizualization on flight delays using tidyverse tools\nFallstudie Flugverspätungen - EDA\nFallstudie zur EDA: Top-Gear\nFallstudie zur EDA: OECD-Wellbeing-Studie\nFallstudie zur EDA: Movie Rating\nFallstudie zur EDA: Women in Parliament\nFinde den Tag mit den meisten Flugverspätungen, Datensatz ‘nycflights13’\nCleaning and visualizing genomic data: a case study in tidy analysis\nTidyverse Case Study: Exploring the Billboard Charts\nAnalyse einiger RKI-Coronadaten: Eine reproduzierbare Fallstudie\nOpenCaseStudies - Health Expenditure\nOpen Case Studies: School Shootings in the United States - includes dashboards\nOpen Case Studies: Disparities in Youth Disconnection\nYACSDA Seitensprünge\nThe Open Case Study Search provides a nice collection of helpful case studies.\nifes@FOM Fallstudienseite",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Going further</span>"
    ]
  },
  {
    "objectID": "080-outro.html#case-studies-on-linear-models",
    "href": "080-outro.html#case-studies-on-linear-models",
    "title": "7  Going further",
    "section": "7.4 Case studies on linear models",
    "text": "7.4 Case studies on linear models\n\nFALLSTUDIEN - NUR LINEARE MODELLE\n\nBeispiel für Prognosemodellierung 1, grundlegender Anspruch, Video\nBeispiel für Ihre Prognosemodellierung 2, mittlerer Anspruch\nBeispiel für Ihre Prognosemodellierung 3, hoher Anspruch\nFallstudie: Modellierung von Flugverspätungen\nModelling movie successes: linear regression\nMovies\nFallstudie Einfache lineare Regression in Base-R, Anfängerniveau, Kaggle-Competition TMDB\nFallstudie Sprit sparen\nFallstudie zum Beitrag verschiedener Werbeformate zum Umsatz; eine Fallstudie in Python, aber mit etwas Erfahrung wird man den Code einfach in R umsetzen können (wenn man nicht in Python schreiben will)\nPractical Linear Regression with R: A case study on diamond prices\nCase Study: Italian restaurants in NYC\nVorhersage-Modellierung des Preises von Diamanten\nModellierung Diamantenpreis 2\n\n\n\n7.4.1 Case studies on machine learning using tidymodels\n\nFALLSTUDIEN - MASCHINELLES LERNEN MIT TIDYMODELS\n\nLouise E. Sinks: One Class SVM\nLouise E. Sinks: Credit Card Fraud: A Tidymodels Tutorial - inklusive Workflow-Sets\nExperimenting with machine learning in R with tidymodels and the Kaggle titanic dataset\nTutorial on tidymodels for Machine Learning\nClassification with Tidymodels, Workflows and Recipes\nA (mostly!) tidyverse tour of the Titanic\nPersonalised Medicine - EDA with tidy R\nTidy TitaRnic\nFallstudie Seegurken\nSehr einfache Fallstudie zur Modellierung einer Regression mit tidymodels\nFallstudie zur linearen Regression mit Tidymodels\nAnalyse zum Verlauf von Covid-Fällen\nFallstudie zur Modellierung einer logististischen Regression mit tidymodels\nFallstudie zu Vulkanausbrüchen (Resampling and kein Tuning)\nFallstudie Himalaya (Resampling and kein Tuning)\nFallstudien zu Studiengebühren\n1. Modell der Fallstudie Hotel Bookings\nAufgaben zur logistischen Regression, PDF\nFallstudie Oregon Schools\nFallstudie Windturbinen\nFallstudie Churn\nEinfache Durchführung eines Modellierung mit XGBoost\nFallstudie Oregon Schools\nFallstudie Churn\nFallstudie Ikea\nFallstudie Wasserquellen in Sierra Leone\nFallstudie Bäume in San Francisco: Random Forest tunen\nFallstudie Vulkanausbrüche\nFallstudie Brettspiele mit XGBoost\nFallstudie Serie The Office: Lasso tunen\nFallstudie NBER Papers\nFallstudie Einfache lineare Regression mit Tidymodels, Kaggle-Competition TMDB\nFallstudie Einfaches Random-Forest-Modell mit Tidymodels, Kaggle-Competition TMDB\nFallstudie Workflow-Set mit Tidymodels, Kaggle-Competition TMDB\nFallstudie Titanic mit Tidymodels bei Kaggle\nEinfache Fallstudie mit Tidymodels bei Kaggle\nExploring the Star Wars “Prequel Renaissance” Using tidymodels and workflowsets\nClassification modelling workflow using tidymodels, Konstantinos Patelis\nTune xgboost models with early stopping to predict shelter animal status\nPredicting injuries for Chicago traffic crashes: Resampling BAG Tree\nUsing tidymodels to Predict Health Insurance Cost: Resmapling",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Going further</span>"
    ]
  },
  {
    "objectID": "080-outro.html#recommened-reading",
    "href": "080-outro.html#recommened-reading",
    "title": "7  Going further",
    "section": "7.5 Recommened reading",
    "text": "7.5 Recommened reading\nCheck-out the references pointed out in this course.\nIn addition, here’s a Zotero group with recommended resources on empirical research and statistical modeling. Note that the resources are of introductory nature.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Going further</span>"
    ]
  },
  {
    "objectID": "080-outro.html#have-fun",
    "href": "080-outro.html#have-fun",
    "title": "7  Going further",
    "section": "7.6 Have fun",
    "text": "7.6 Have fun\nAnd while we are talking about it, follow a strong purpose.\nBest of luck!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Going further</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cetinkaya-Rundel, M., & Hardin, J. (2021). Introduction to\nModern Statistics. https://openintro-ims.netlify.app/\n\n\nDeisenroth, M. P., Faisal, A. A., & Ong, C. S. (2020).\nMathematics for machine learning. Cambridge University Press.\nhttps://mml-book.github.io/\n\n\nGelman, A., Hill, J., & Vehtari, A. (2021). Regression and other\nstories. Cambridge University Press.\n\n\nGoodman, S. (2008). A dirty dozen: Twelve\nP-value misconceptions. Seminars in Hematology,\n45(3), 135–140. https://doi.org/10.1053/j.seminhematol.2008.04.003\n\n\nHernán, M. A., Hsu, J., & Healy, B. (2019). A second chance to get\ncausal inference right: A classification of data science\ntasks. Chance, 32(1), 42–49. https://doi.org/10.1080/09332480.2019.1579578\n\n\nIsmay, C., & Kim, A. Y.-S. (2020). Statistical inference via\ndata science: A ModernDive into R and the\nTidyverse. CRC Press / Taylor & Francis Group. https://moderndive.com/\n\n\nMacKay, R. J., & Oldford, R. W. (2000). Scientific method,\nstatistical method and the speed of light. Statistical Science,\n15(3), 254–278. https://doi.org/10.1214/ss/1009212817\n\n\nMcElreath, R. (2020). Statistical rethinking: A\nBayesian course with examples in R and\nStan (2nd ed.). Taylor and Francis, CRC\nPress.\n\n\nPearl, J., Glymour, M., & Jewell, N. P. (2016). Causal inference\nin statistics: A primer. Wiley.\n\n\nPoldrack, R. (2022a). Statistical thinking for the 21st\ncentury. https://statsthinking21.github.io/statsthinking21-core-site/index.html\n\n\nPoldrack, R. (2022b). Statistical Thinking for the 21st\nCentury. https://statsthinking21.github.io/statsthinking21-core-site/index.html\n\n\nRoback, P., & Legler, J. (2021). Beyond multiple linear\nregression: Applied generalized linear models and multilevel models\nin (1st ed.). CRC Press.\n\n\nRohrer, J. M. (2018). Thinking Clearly About Correlations\nand Causation: Graphical Causal Models for\nObservational Data. Advances in Methods and Practices\nin Psychological Science, 1(1), 27–42. https://doi.org/10.1177/2515245917745629\n\n\nSauer, S. (2019). Moderne Datenanalyse mit\nR: Daten einlesen, aufbereiten, visualisieren\nund modellieren (1. Auflage 2019). Springer. https://www.springer.com/de/book/9783658215866\n\n\nWasserstein, R. L., Schirm, A. L., & Lazar, N. A. (2019). Moving to\na world beyond “ p p &lt; 0.05.” The American\nStatistician, 73, 1–19. https://doi.org/10.1080/00031305.2019.1583913\n\n\nWickham, H., & Grolemund, G. (2016). R for Data\nScience: Visualize, Model,\nTransform, Tidy, and Import\nData. O’Reilly Media. https://r4ds.had.co.nz/index.html\n\n\nWild, C. J., & Pfannkuch, M. (1999). Statistical thinking in\nempirical enquiry. International Statistical Review,\n67(3), 223–248.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "030-EDA.html#footnotes",
    "href": "030-EDA.html#footnotes",
    "title": "2  Exploratory Data Analysis",
    "section": "",
    "text": "Don’t forget to install it before you use it.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "020-basics.html#exercises",
    "href": "020-basics.html#exercises",
    "title": "1  Basics",
    "section": "1.8 Exercises",
    "text": "1.8 Exercises\n\n🧑‍🎓 I need exercises on R!\n\n\n👨‍🏫 Checkout all exercises tagged with “R” on datenwerk. Pro-Tipp: Use the translation function of your browers to translate the webpage into your favorite language.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics</span>"
    ]
  }
]