
# Advanced Regression


![](img/stern.png){width=5%}

## R-packages needed

For this chapter, the following R packages are needed.

```{r}
#| message: false
library(tidyverse)
library(easystats)
```




## Multiplicative associations


### The Log-Y model

Consider again the linear model, in a simple form:




$$\hat{y} = \beta_0 + \beta_1 x_1 +  \ldots + b_kx_k +$$
Surprisingly, we can use this *linear* model to describe *multiplicative* assocations:


$\hat{y} = e^{b_0 + b_1x_1 + b_2x_2 + \ldots + b_kx_k}$

(I wrote `b` instead of $\beta$ just to show that both has its meaning, but are separate things.)

Exponentiate both sides to get:

$log (\hat{y}) = b_0 + b_1x_1 + b_2x_2 + \ldots + b_kx_k$

For simplicity, let's drop the subscripts in the following without loss of generality and keep it short:



$y = e^{x}$, with $\approx  2.71...$

Exponentiate both sides to get:

$log(y) = x$


This association is called multiplicative, because if x increases by 1, y increased by a *constant factor*.




:::{.callout-note}
The logarithm is not defined for negative (input) values. And $log(0) = -\infty$.
:::


A side-effect of modelling `log_y` instead of `y` is that the distribution shape of the outcome variable changes. 
This can be useful in times.



### Exercise

- [Effect of education on income](https://datenwerk.netlify.app/post/log-y-regr1/log-y-regr1/)
- [Effect of log-y transformation on the distribution, an example](https://datenwerk.netlify.app/post/log-y-regr2/log-y-regr2/)


:::{.callout-note}
The exercises are written in German Language. Don't fret. Browsers are able to translate websites instantaneously. Alternatively, go to sites such as [Google Translate](https://translate.google.de/?sl=de&tl=en&op=websites) and enter the URL of the website to be translated. Also check out the webstor of your favorite browser to get an extention [such as this one for Google Chrome](https://chrome.google.com/webstore/detail/google-translate).
:::


### Visualizing Log Transformation

Check out [this post](https://data-se.netlify.app/2022/01/14/visualizing-a-log-y-regression-model/) for an example of a log-y regression visualized.

[This post](https://data-se.netlify.app/2021/06/17/ein-beispiel-zum-nutzen-einer-log-transformation/) puts some more weight to the argument that a log-y transformation is useful (if you want to model multiplicative relations).

### Further reading

Check out [this great essay by Kenneth Benoit](https://kenbenoit.net/assets/courses/ME104/logmodels2.pdf) on different log-variants in regression. 
Also @gelman_regression_2021, chapter 12 (and others), is useful.





## Interaction




### Multiple predictors, no interaction


Regression analyses can be used with more than one predictor, see Figure @fig-multregr.


```{mermaid}
%%| fig-width: 6.5
%%| label: fig-multregr
%%| fig-cap: One predictor (X) vs. two predictors (X1, X2)

flowchart LR
X --> Y1

X1 --> Y2
X2 --> Y2
```



A different perspective is given by Figure @fig-3dregr, where a 3D account of a regression is given. 3D means to input variables, and (which is always the case) one output variable.

:::{.callout-note}
Note that the slope in linear in both axis (X1 and X2).
:::




![Multiple Regression. Image source: Cfbaf, Public Domain](https://upload.wikimedia.org/wikipedia/commons/a/ae/2d_multiple_linear_regression.gif?20161014061355){#fig-3dregr}



:::{.callout-important}
If the slope for one predictor is the same for all values of the other predictor, then we say that no interaction is taking place.
:::


Here's a visualization of a 3D regression plane (not line) *without interaction*: constant slope in one axis, see Figure @fig-3dregr2.


```{r}
#| out-width: "33%"
#| echo: false
#| results: hold
#| label: fig-3dregr2
plane <- function(x, y) {
  x+y
}

x <- y <- seq(-1, 1, length = 30)
z <- outer(x,y, plane)

persp(x,y,z, theta = 30)
persp(x,y,z, theta = 90)
persp(x,y,z, theta = 150)
```


Note that the slope in each predictor axis equals 1, boringly. 
Hence the according 2D plots are boring, too, see Figure @3d-2dregr.


For the sake of an example,
consider this linear model:


$mpg \sim hp + disp$

Or, in more regression like terms:


$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$, where x1 is `hp` and `x2` is `disp` in the `mtcars` dataset.

In R terms:

```{r}
lm3d <- lm(mpg ~ hp + disp, data = mtcars)
```


The 3D plot is shown in Figure @fig-mtcars3d.

```{r}
#| echo: false
#| label: fig-mtcars3d
#| fig-cap: "mpg ~ hp + disp"
x <- seq(min(mtcars$hp), max(mtcars$hp), length.out = 25)
y <- seq(min(mtcars$disp), max(mtcars$disp), length.out = 25)
z <- outer(x, y, function(x,y) {predict(lm3d, data.frame(hp = x, disp = y))})
persp(x,y,z , theta = c(45, 135, 225, 315), col = "lightblue")
```


Here are the two corresponding 2d (1 predictor) regression models:


```{r}
lm1 <- lm(mpg ~ hp, data = mtcars)
plot(estimate_relation(lm1))

lm2 <- lm(mpg ~ disp, data = mtcars)
plot(estimate_relation(lm2))
```





Checkout [this post](https://data-se.netlify.app/2022/04/19/3d-regression-plane-with-scatter-plot/) for a visually slightly more appealing 3d regression plane.



### Interaction

For interaction to happen we relax the assumption that the slope of predictor 1 must be constant for all values of predictor 2.

In R, we specify an interaction model like this:

```{r}
lm3d_interact <- lm(mpg ~ hp + disp + hp:disp, data = mtcars)
```

The symbol `hp:disp` can be read as "the interaction effect of `hp` and `disp`".

Here's a visual account, see Figure @fig-mtcars3d-interact.



```{r}
#| echo: false
#| label: fig-mtcars3d-interact
#| fig-cap: "mpg ~ hp + disp"
x <- seq(min(mtcars$hp), max(mtcars$hp), length.out = 25)
y <- seq(min(mtcars$disp), max(mtcars$disp), length.out = 25)
z <- outer(x, y, function(x,y) {predict(lm3d_interact, data.frame(hp = x, disp = y))})
persp(x,y,z , theta = c(45, 135, 225, 315), col = "lightblue")
```

Compare @fig-mtcars3d-interact and @fig-mtcars3d.

In @fig-mtcars3d-interact you'll see that the lines along the Y axis are not parallel anymore.
Similarly, the lines along the X axis are not parallel anymore.


:::{.callout-important}
If the regression lines (indicating different values of one predictor) are *not* parallel, we say that an interaction effect is taking place.
:::


However, the *difference* or *change* between two adjacent values (lines) is constant. This value is the size the regression effect.


### Interaction made simple


If you find that two sophisticated, consider the following simple case.


First, we mutate `am` to be a factor variable, in order to make things simpler (without loss of generality).

```{r}
mtcars2 <-
  mtcars %>% 
  mutate(am_f = factor(am))
```

Now we use this new variable for a simple regression model:

```{r}
lm_interact_simple <- lm(mpg ~ disp + am_f + disp:am_f, data = mtcars2)
```


Here's the plot, Figure @fig-interact-simple.

```{r}
#| label: fig-interact-simple
#| fig-cap: A simple interaction model
plot(estimate_relation(lm_interact_simple))
```


In this picture, we see that the two regression lines are *not* parallel, 
and hence there is evidence of an interaction effect.


The interaction effect amounts to the *difference* in slops in Figure @fig-interact-simple.

One might be inclined to interpret  Figure @fig-interact-simple as an 3D image, where the one (reddish) line is in the foreground and the blueish line in the background (or vice versa, as you like).
Given a 3D image (and hence 2 predictors), we are where we started further above.

For completeness, here are the parameters of the model.


```{r}
#| echo: false
display(parameters(lm_interact_simple))
```


### Centering variables

The effect of of `am_f` must be interpreted when `disp` is zero, which does not make much sense.

Therefore it simplifies the interpretation of regression coefficients to *center* all input variables, by subtrating the mean value ("demeaning" or "centering"):

$$x' = x - \bar{x}$$
In R, this can be achieved e.g,. in this way:

```{r}
mtcars3 <- 
mtcars2 %>% 
  mutate(disp_c = disp - mean(disp))
```


```{r}
lm_interact_simple2 <- lm(mpg ~ disp_c + am_f + disp_c:am_f, data = mtcars3)
parameters(lm_interact_simple2)
```





## Predictor relevance


Given a model, we might want to know which predictor has the strongest association with the outcome?

In order to answer this question, all predictor must have the same scale.
Otherwise the importance of a predictor would increase by 1000, if we multiply each of the observations' values by the same factor.
However, this multiplication should not change the relevance of a predictor.

A simple solution is to standardize all predictors to the same scale (sd=1).


```{r}
mtcars4 <-
  mtcars %>% 
  standardize(select = c("disp", "hp", "cyl"))
```


By the way, "standardizing" centers the variable by default to a mean value of zero (by demeaning).

See:

```{r}
head(mtcars4$disp)
head(mtcars$disp)
```


Here's the SD:

```{r}
sd(mtcars4$disp)
sd(mtcars$disp)
```

And here's the mean value:


```{r}
mean(mtcars4$disp)
mean(mtcars$disp)
```



Now we are in a position to decide which predictor is more important:

```{r}
m <- lm(mpg ~ disp + hp + cyl, data = mtcars4)
parameters(m)
```




## Exercises


- [Predictor relevance](https://datenwerk.netlify.app/post/log-y-regr3/log-y-regr3/)
- [Adjusting](https://datenwerk.netlify.app/post/adjustieren1/adjustieren1/)
- [Adjusting 2](https://datenwerk.netlify.app/post/adjustieren2/adjustieren2/)
- [Interpreting Regression coefficients](https://datenwerk.netlify.app/post/interpret-koeff/interpret-koeff/)




## Lab

Get your own data, and build a simple model reflecting your research hypothesis based on the topics covered in this chapter. If you are lacking data (or hypothesis) get something close to it.





## Glimpse on parameter estimation

An elegant yet simple explanation of the math of parameter estimation can be found [at "go data driven"](https://godatadriven.com/blog/the-linear-algebra-behind-linear-regression/).
A similar approach is presented [here](https://shainarace.github.io/LinearAlgebra/leastsquares.html).

Here's the essence of a geometric interpretation of the least square method, see  Figure @fig-leastsq.



![Geometric interpretation of the least square method. Source: Oleg Alexandrov on Wikimedia](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Linear_least_squares_geometric_interpretation.png/543px-Linear_least_squares_geometric_interpretation.png?20080406011122){#fig-leastsq}




## Further Reading

Mathematical foundations can be found in @deisenroth_mathematics_2020.
[Here's](https://data-se.netlify.app/2022/06/13/free-resources-for-aspiring-data-adepts/) a collection of online resources tapping into statistics and machine learning.


