{
  "hash": "276051ed75244edb713ce1ee7bd62f8a",
  "result": {
    "markdown": "# Modelling and regression\n\n\n![](img/stern.png){width=5%}\n\n\n\n## What's modelling?\n\n[Read this great introduction by modelling by Russel Poldrack](https://statsthinking21.github.io/statsthinking21-core-site/fitting-models.html#what-is-a-model).\n\n\n\n\n\n\n\n\n\n## Regression as the umbrella tool for modelling\n\n![One regression](https://memegenerator.net/img/instances/86435221.jpg){width=\"50%\"}\n\n\nAlternatively, \nventure into the forest of statistical tests as [oultined eg here, at Uni Muenster](https://web.archive.org/web/20091029162244/http://www.wiwi.uni-muenster.de/ioeb/en/organisation/pfaff/stat_overview_table.html).\n\n\nYou may want to ponder on this image of a decision tree of which test to choose, see Figure @fig-choose-test.\n\n![Choose your test carefully](img/choose-test.png){#fig-choose-test}\n\n\n\n\n### Common statistical tests are linear models\n\n\nAs Jonas Kristoffer Lindeløv tells us,\nwe can formulate most statistical tests as a linear model, ie., a regression.\n\n\n![Common statistical tests as linear models](https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.png)\n\n### How to find the regression line\n\nIn the simplest case, regression analyses can be interpreted geometrically as a line in a 2D coordinate system, see Figre @fig-regr1.\n\n\n\n![Least Square Regression](https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Coefficient_of_Determination.svg/800px-Coefficient_of_Determination.svg.png?20100906105829){#fig-regr1}\n\n\nPut simple, we are looking for the line which is in the \"middle of the points\". More precisely, we place the line such that the squared distances from the line to the points is minimal, see Figre @fig-regr1.\n\n\nConsider Figure @fig-regr2, from [this source](https://bookdown.org/roback/bookdown-BeyondMLR/ch-MLRreview.html#assumptions-for-linear-least-squares-regression) by @roback_beyond_2021. \nIt visualizes not only the notorious regression line,\nbut also sheds light on regression assumptions,\nparticularly on the error distribution.\n\n\n![Regression and some of its assumptions](https://bookdown.org/roback/bookdown-BeyondMLR/bookdown-BeyondMLR_files/figure-html/OLSassumptions-1.png){#fig-regr2}\n\n\n\n### The linear model \n\n\nHere's the canonical form of the linear model.\n\nConsider a model with $k$ predictors:\n\n$$y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k + \\epsilon$$\n\n\n\n### Algebraic derivation \n\nFor the mathematical inclined, check out [this derivation](https://data-se.netlify.app/2022/05/23/ableitung-der-koeffizienten-der-einfachen-regression/) of the simple case regression model.\nNote that the article is written in German, but your browser can effortlessly translate into English. \nHere's a [similar English article from StackExchange](https://math.stackexchange.com/questions/716826/derivation-of-simple-linear-regression-parameters).\n\n\n\n## R-packages needed\n\nFor this chapter, the following R packages are needed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstanarm)\nlibrary(tidyverse)\nlibrary(easystats)\n```\n:::\n\n\n\n\n\n## In all its glory\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression1_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n## First model: one metric predictor\n\nFirst, let's load some data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(mtcars)\nglimpse(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 32\nColumns: 11\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n```\n:::\n:::\n\n\n\n### Frequentist\n\nDefine and fit the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm1_freq <- lm(mpg ~ hp, data = mtcars)\n```\n:::\n\n\n\nGet the parameter values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparameters(lm1_freq)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter   | Coefficient |   SE |         95% CI | t(30) |      p\n------------------------------------------------------------------\n(Intercept) |       30.10 | 1.63 | [26.76, 33.44] | 18.42 | < .001\nhp          |       -0.07 | 0.01 | [-0.09, -0.05] | -6.74 | < .001\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n```\n:::\n:::\n\n\nPlot the model parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(parameters(lm1_freq))\n```\n\n::: {.cell-output-display}\n![](regression1_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n### Bayesian\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm1_bayes <- stan_glm(mpg ~ hp, data = mtcars, refresh = 0)\n```\n:::\n\n\nActually, we want to suppress some overly verbose output, using `refresh = 0`:\n\n::: {.cell}\n\n```{.r .cell-code}\nlm1_bayes <- stan_glm(mpg ~ hp, data = mtcars, refresh = 0)\n```\n:::\n\n\n\n\nGet the parameter values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparameters(lm1_bayes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter   | Median |         95% CI |   pd | % in ROPE |  Rhat |     ESS |                   Prior\n----------------------------------------------------------------------------------------------------\n(Intercept) |  30.08 | [26.83, 33.37] | 100% |        0% | 1.000 | 3329.00 | Normal (20.09 +- 15.07)\nhp          |  -0.07 | [-0.09, -0.05] | 100% |      100% | 1.000 | 3679.00 |   Normal (0.00 +- 0.22)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n```\n:::\n:::\n\n\nPlot the model parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(parameters(lm1_bayes))\n```\n\n::: {.cell-output-display}\n![](regression1_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n### Model performance\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr2(lm1_freq)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# R2 for Linear Regression\n       R2: 0.602\n  adj. R2: 0.589\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nr2(lm1_bayes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.589 (95% CI [0.384, 0.749])\n```\n:::\n:::\n\n\n\n\n### Model check\n\n\n::: {.cell fit-width='10' fig.asp='1'}\n\n```{.r .cell-code}\ncheck_model(lm1_freq)\n```\n\n::: {.cell-output-display}\n![](regression1_files/figure-html/unnamed-chunk-13-1.png){width=100%}\n:::\n:::\n\n::: {.cell fit-width='10' fig.asp='1'}\n\n```{.r .cell-code}\ncheck_model(lm1_bayes)\n```\n\n::: {.cell-output-display}\n![](regression1_files/figure-html/unnamed-chunk-14-1.png){width=100%}\n:::\n:::\n\n\n\n### Get some predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm1_pred <- estimate_relation(lm1_freq)\nlm1_pred\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel-based Expectation\n\nhp     | Predicted |   SE |         95% CI\n------------------------------------------\n52.00  |     26.55 | 1.18 | [24.15, 28.95]\n83.44  |     24.41 | 0.94 | [22.49, 26.32]\n114.89 |     22.26 | 0.75 | [20.72, 23.80]\n146.33 |     20.11 | 0.68 | [18.72, 21.51]\n177.78 |     17.97 | 0.75 | [16.43, 19.50]\n209.22 |     15.82 | 0.93 | [13.92, 17.73]\n240.67 |     13.68 | 1.17 | [11.29, 16.07]\n272.11 |     11.53 | 1.44 | [ 8.59, 14.48]\n303.56 |      9.39 | 1.73 | [ 5.86, 12.92]\n335.00 |      7.24 | 2.02 | [ 3.11, 11.38]\n\nVariable predicted: mpg\nPredictors modulated: hp\n```\n:::\n:::\n\n\n\nMore details on the above function can be found on the [respective page at the easystats site](https://easystats.github.io/modelbased/reference/estimate_expectation.html#functions-for-estimating-predicted-values-and-uncertainty).\n\n\n\n### Plot the model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(lm1_pred)\n```\n\n::: {.cell-output-display}\n![](regression1_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## More of this\n\nMore technical details for gauging model performance and model quality,\ncan be found on the site of [the R package \"performance](https://easystats.github.io/performance/) at the easystats site.\n\n\n\n\n## Bayes-members only\n\n\nBayes statistics provide a distribution as the result of the analysis,\nthe posterior distribution, which provides us with quite some luxury.\n\n\nAs the posterior distribution manifests itself by a number of samples,\nwe can easily filter and manipulate this sample distribution in order to ask some interesing questions.\n\nSee:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm1_bayes %>% \n  as_tibble() %>% \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  `(Intercept)`      hp sigma\n          <dbl>   <dbl> <dbl>\n1          29.0 -0.0530  3.73\n2          32.3 -0.0856  4.54\n3          28.3 -0.0519  3.46\n4          27.8 -0.0510  3.94\n5          30.8 -0.0659  4.13\n6          32.2 -0.0704  4.20\n```\n:::\n:::\n\n\n\n### Asking for probabilites\n\n\n*What's the probability that the effect of hp is negative?*\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm1_bayes %>% \n  as_tibble() %>% \n  count(hp < 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  `hp < 0`     n\n  <lgl>    <int>\n1 TRUE      4000\n```\n:::\n:::\n\n\nFeel free to ask similar questions!\n\n\n### Asking for quantiles\n\n\n*With a given probability of, say 90%, how large is the effect of hp?*\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm1_bayes %>% \n  as_tibble() %>% \n  summarise(q_90 = quantile(hp, .9))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n     q_90\n    <dbl>\n1 -0.0552\n```\n:::\n:::\n\n\n*What's the smallest 95% percent interval for the effect of hp?*\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhdi(lm1_bayes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHighest Density Interval\n\nParameter   |        95% HDI\n----------------------------\n(Intercept) | [27.12, 33.60]\nhp          | [-0.09, -0.05]\n```\n:::\n:::\n\n\nIn case you prefer 89% intervals (I do!):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhdi(lm1_bayes, ci = .89)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHighest Density Interval\n\nParameter   |        89% HDI\n----------------------------\n(Intercept) | [27.57, 32.90]\nhp          | [-0.08, -0.05]\n```\n:::\n:::\n\n\n\n\n## Multiple metric predictors\n\nAssume we have a theory that dictates that fuel economy is a (causal) function of horse power and engine displacement.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm2_freq <- lm(mpg ~ hp + disp, data = mtcars)\nparameters(lm2_freq)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter   | Coefficient |       SE |         95% CI | t(29) |      p\n----------------------------------------------------------------------\n(Intercept) |       30.74 |     1.33 | [28.01, 33.46] | 23.08 | < .001\nhp          |       -0.02 |     0.01 | [-0.05,  0.00] | -1.86 | 0.074 \ndisp        |       -0.03 | 7.40e-03 | [-0.05, -0.02] | -4.10 | < .001\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n```\n:::\n:::\n\n\n\nSimilarly for Bayes inference:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm2_bayes <- stan_glm(mpg ~ hp + disp, data = mtcars)\n```\n:::\n\n\nResults\n\n::: {.cell}\n\n```{.r .cell-code}\nparameters(lm2_bayes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter   | Median |         95% CI |     pd | % in ROPE |  Rhat |     ESS |                   Prior\n------------------------------------------------------------------------------------------------------\n(Intercept) |  30.78 | [28.15, 33.40] |   100% |        0% | 1.001 | 3828.00 | Normal (20.09 +- 15.07)\nhp          |  -0.03 | [-0.05,  0.00] | 97.02% |      100% | 1.000 | 1887.00 |   Normal (0.00 +- 0.22)\ndisp        |  -0.03 | [-0.04, -0.02] |   100% |      100% | 1.000 | 2208.00 |   Normal (0.00 +- 0.12)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n```\n:::\n\n```{.r .cell-code}\nplot(parameters(lm2_bayes))\n```\n\n::: {.cell-output-display}\n![](regression1_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n\n```{.r .cell-code}\nr2(lm2_bayes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.732 (95% CI [0.576, 0.843])\n```\n:::\n:::\n\n\n\n\nDepending on the value of `disp` the prediction of `mpg` from `hp` will vary:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm2_pred <- estimate_relation(lm2_freq)\nplot(lm2_pred)\n```\n\n::: {.cell-output-display}\n![](regression1_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\n\n## One nominal predictor\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars2 <-\n  mtcars %>% \n  mutate(am_f = factor(am))\n\nlm3a <- lm(mpg ~ am_f, data = mtcars2)\nparameters(lm3a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter   | Coefficient |   SE |         95% CI | t(30) |      p\n------------------------------------------------------------------\n(Intercept) |       17.15 | 1.12 | [14.85, 19.44] | 15.25 | < .001\nam f [1]    |        7.24 | 1.76 | [ 3.64, 10.85] |  4.11 | < .001\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm3a_means <- estimate_means(lm3a, at = \"am_f\")\nlm3a_means \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEstimated Marginal Means\n\nam_f |  Mean |   SE |         95% CI\n------------------------------------\n0    | 17.15 | 1.12 | [14.85, 19.44]\n1    | 24.39 | 1.36 | [21.62, 27.17]\n\nMarginal means estimated at am_f\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(lm3a_means)\n```\n\n::: {.cell-output-display}\n![](regression1_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\nNote that we should have converted `am` to a factor variable before fitting the model.\nOtherwise, the plot won't work.\n\n\nHere's a more hand-crafted version of the last plot:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mtcars2) +\n  aes(x = am_f, y = mpg) +\n  geom_violin() +\n  geom_jitter(width = .1, alpha = .5) +\n  geom_pointrange(data = lm3a_means,\n                  color = \"orange\",\n                  aes(ymin = CI_low, ymax = CI_high, y = Mean)) +\n  geom_line(data = lm3a_means, aes(y = Mean, group = 1))\n```\n\n::: {.cell-output-display}\n![](regression1_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n\n## One metric and one nominal predictor\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars2 <-\n  mtcars %>% \n  mutate(cyl = factor(cyl))\n\nlm4 <- lm(mpg ~ hp + cyl, data = mtcars2)\nparameters(lm4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter   | Coefficient |   SE |          95% CI | t(28) |      p\n-------------------------------------------------------------------\n(Intercept) |       28.65 | 1.59 | [ 25.40, 31.90] | 18.04 | < .001\nhp          |       -0.02 | 0.02 | [ -0.06,  0.01] | -1.56 | 0.130 \ncyl [6]     |       -5.97 | 1.64 | [ -9.33, -2.61] | -3.64 | 0.001 \ncyl [8]     |       -8.52 | 2.33 | [-13.29, -3.76] | -3.66 | 0.001 \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm4_pred <- estimate_relation(lm4)\nplot(lm4_pred)\n```\n\n::: {.cell-output-display}\n![](regression1_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n\n## What about correlation?\n\n\nCorrelation is really a close cousin to regression. In fact, regression with standardized variables amounts to correlation.\n\nLet's get the correlation matrix of the variables in involved in `lm4`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm4_corr <- \n  mtcars %>% \n  select(mpg, hp, disp) %>% \n  correlation()\n\nlm4_corr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |     r |         95% CI | t(30) |         p\n--------------------------------------------------------------------\nmpg        |         hp | -0.78 | [-0.89, -0.59] | -6.74 | < .001***\nmpg        |       disp | -0.85 | [-0.92, -0.71] | -8.75 | < .001***\nhp         |       disp |  0.79 | [ 0.61,  0.89] |  7.08 | < .001***\n\np-value adjustment method: Holm (1979)\nObservations: 32\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(summary(lm4_corr))\n```\n\n::: {.cell-output-display}\n![](regression1_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Exercises\n\n1. [mtcars simple 1](https://datenwerk.netlify.app/post/mtcars-simple1/mtcars-simple1/)\n1. [mtcars simple 2](https://datenwerk.netlify.app/post/mtcars-simple2/mtcars-simple2/)\n1. [mtcars simple 3](https://datenwerk.netlify.app/post/mtcars-simple3/mtcars-simple3/)\n\n\n## Lab\n\nGet your own data, and build a simple model reflecting your research hypothesis. If you are lacking data (or hypothesis) get something close to it.\n\n\n\n## Further reading\n\n\n@roback_beyond_2021 provide and more than introductory account of regression while being accessible. \nA recent but still classic book (if this is possible) is the book by @gelman_regression_2021.\n\n\n\n## Debrief\n\n\n\n\n![Science. Via Giphy.](https://media.giphy.com/media/141amBdjqs9Vvy/giphy.gif)\n\n\n\n\n\n\n",
    "supporting": [
      "regression1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}