[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "stats-nutshell",
    "section": "",
    "text": "Desktop version, not the server↩︎\npossibly you need also a Fortran compiler, but maybe that’s optional↩︎"
  },
  {
    "objectID": "goals.html#overview",
    "href": "goals.html#overview",
    "title": "1  Goals in statistics",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nMany stories to be told. Here’s one, on the goals pursued in statistics (and related fields), see Figure Figure 1.1.\n\n\n\n\n\nflowchart LR\n  A{Goals} --> B(describe)\n  A --> C(predict)\n  A --> D(explain)\n  B --> E(distribution)\n  B --> F(assocation)\n  B --> G(extrapolation)\n  C --> H(point estimate)\n  C --> I(interval)\n  D --> J(causal inference)\n  D --> K(population)\n  D --> L(latent construct)\n\n\n\n\n\n\nFigure 1.1: A taxonomy of statistical goals\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that “goals” do not exist in the world. We make them up in our heads. Hence, they have no ontological existence, they are epistemological beasts. This entails that we are free to devise goals as we wish, provided we can convince ourselves and other souls of the utility of our creativity."
  },
  {
    "objectID": "goals.html#further-reading",
    "href": "goals.html#further-reading",
    "title": "1  Goals in statistics",
    "section": "1.2 Further reading",
    "text": "1.2 Further reading\nHernán, Hsu, and Healy (2019) distinguish:\nHernán et al. (2019) distinguish:\n\nDescription: “How can women aged 60–80 years with stroke history be partitioned in classes defined by their characteristics?”\nPrediction: “What is the probability of having a stroke next year for women with certain characteristics?”\nCausal inference: “Will starting a statin reduce, on average, the risk of stroke in women with certain characteristics?”\n\nGelman, Hill, and Vehtari (2021), chap. 1.1 proposes three “challenges” of statistical inference."
  },
  {
    "objectID": "goals.html#if-nothing-else-helps",
    "href": "goals.html#if-nothing-else-helps",
    "title": "1  Goals in statistics",
    "section": "1.3 If nothing else helps",
    "text": "1.3 If nothing else helps\nStay calm and behold the infinity.\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge: Cambridge University Press.\n\n\nHernán, Miguel A., John Hsu, and Brian Healy. 2019. “A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks.” Chance 32 (1): 42–49. https://doi.org/10.1080/09332480.2019.1579578."
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "2  Basics",
    "section": "",
    "text": "The PPDAC Model is a methodological framework (aka a model) for applying the scientific method to any analytical or research question, or at least it is applicable to quite a few (MacKay and Oldford 2000). It is not meant to be a rigid sequence, but rather a cycle that may turn a number of rounds like a spiral. Statistician Chris Wild puts the PPDAC cycle in the following figure, see Figure Figure 2.1. In this short essay, he summaries his ideas on how to use the PPDAC as a tool for data analysis in problem solving.\n\n\n\nFigure 2.1: PPDAC cycle. Image source: Chris Wild\n\n\nWild and Pfannkuch (1999) give a more systematic overview on how a quantitative research question - applied or basic - can be tackled and conceived. For example, in their paper the authors enumarate some dispositions that researcher should embrace in order to fruitfully engage in empirical research:\n\nScepticism\nImagination\nCuriosity\nOpennness\nA propsenisty to seek deeper menaing\nBeing logical\nEngagement\nPerseverance\n\nWild and Pfannkuch (1999) further note that variation is one of the essential characteristics of data. They discern to types of variation however, see Figure Figure 2.2.\n\n\n\nFigure 2.2: Two types of variartion. Image source: Chris Wild"
  },
  {
    "objectID": "basics.html#r-basics",
    "href": "basics.html#r-basics",
    "title": "2  Basics",
    "section": "2.2 R Basics",
    "text": "2.2 R Basics\nCheck out chapter 1 in ModernDive for an accessible introduction to getting started with R and RStudio.\nPlease also note that R and RStudio should be installed before starting (this course)."
  },
  {
    "objectID": "basics.html#initial-quiz",
    "href": "basics.html#initial-quiz",
    "title": "2  Basics",
    "section": "2.3 Initial quiz",
    "text": "2.3 Initial quiz\nTo get an idea whether you have digested some R basics, consider the following quiz.\n\nExercise 2.1 (Define a variable) Define in R the variable age and assign the value 42.1\n\n\nExercise 2.2 (Define a variable as a string) Define in R the variable name and assign the value me.2\n\n\nExercise 2.3 (Define a variable by another variable) Define in R the variable name and assign the variable age.3\n\n\nExercise 2.4 (Call a function) Ask R what today’s date() is, that is, call a function.4\n\n\nExercise 2.5 (Define a vector) Define in R a vector x with the values 1,2,3 .5\n\n\nExercise 2.6 (Vector wise computation) Square each value in the vector x.6\n\n\nExercise 2.7 (Vector wise computation 2) Square each value in the vector x and sum up the values.7\n\n\nExercise 2.8 (Vector wise computation 3) Square each value in the vector x, sum up the values, and divide by 3.8\n\n\nExercise 2.9 (Compute the variance) Compute the variance of x using basic arithmetic.910\n\n\nExercise 2.10 (Work with NA) Define the vector y with the values 1,2,NA. Compute the mean. Explain the results.11"
  },
  {
    "objectID": "basics.html#data-import",
    "href": "basics.html#data-import",
    "title": "2  Basics",
    "section": "2.4 Data import",
    "text": "2.4 Data import\nCheck out chapter 4 in ModernDive on how to import data into RStudio and for some basic concepts about “tidy data”.\nSpoiler: There’s a button in RStudio in the “Environment” Pane saying “Import Dataset”. Just click it, and things should work out."
  },
  {
    "objectID": "basics.html#sec-blitz-data",
    "href": "basics.html#sec-blitz-data",
    "title": "2  Basics",
    "section": "2.5 Blitz start with data",
    "text": "2.5 Blitz start with data\nTo blitz start with data, type the following in R:\n\ndata(mtcars)\n\nAnd the data set mtcars will be available.\nTo get help for the data set, type help(mtcars)\nA bit more advanced, but it’s a nice data set, try the Palmer Penguins data set:\n\nd <- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\n\nhead(d)  # see the first few rows, the \"head\" of the table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n6\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\nHere’s some documentation (code book) for this data set."
  },
  {
    "objectID": "basics.html#more-data-set",
    "href": "basics.html#more-data-set",
    "title": "2  Basics",
    "section": "2.6 More data set",
    "text": "2.6 More data set\nCheck out this curated list of data sets useful for learning and practicing your data skills."
  },
  {
    "objectID": "basics.html#literature",
    "href": "basics.html#literature",
    "title": "2  Basics",
    "section": "2.7 Literature",
    "text": "2.7 Literature\nIsmay and Kim (2020) is a helpful start into the first steps in R.\n\n\n\n\nIsmay, Chester, and Albert Young-Sun Kim. 2020. Statistical Inference via Data Science: A ModernDive into r and the Tidyverse. Chapman & Hall/CRC the r Series. Boca Raton: CRC Press / Taylor & Francis Group. https://moderndive.com/.\n\n\nMacKay, R. J., and R. W. Oldford. 2000. “Scientific Method, Statistical Method and the Speed of Light.” Statistical Science 15 (3): 254–78. https://doi.org/10.1214/ss/1009212817.\n\n\nWild, Chris J, and Maxine Pfannkuch. 1999. “Statistical Thinking in Empirical Enquiry.” International Statistical Review 67 (3): 223–48."
  },
  {
    "objectID": "EDA.html#r-packages-needed-for-this-chapter",
    "href": "EDA.html#r-packages-needed-for-this-chapter",
    "title": "3  Exploratory Data Analysis",
    "section": "3.1 R packages needed for this chapter",
    "text": "3.1 R packages needed for this chapter\n\nlibrary(easystats)\nlibrary(tidyverse)\nlibrary(rstanarm)  # optional!"
  },
  {
    "objectID": "EDA.html#whats-eda",
    "href": "EDA.html#whats-eda",
    "title": "3  Exploratory Data Analysis",
    "section": "3.2 What’s EDA?",
    "text": "3.2 What’s EDA?\nExploratory Data Analysis (EDA) is a procedure to scrutinize a dataset at hand in order learn about it. EDA comprises descriptive statistics, data visualization and data transformation techniques (such as dimension reduction).\nIt’s not so mathematical deep as modelling, but in practice it’s really important.\nThere’s this famous saying:\n\nIn Data Science, 80% of time spent prepare data, 20% of time spent complain about the need to prepare data.\n\nEDA can roughly be said to comprise the following parts:\n\nImporting (and exporting) data\nData cleansing (such as deal with missing data etc)\nData transformation or “wrangling” (such as long to wide format)\nComputing desriptive statistics (such as the notorious mean)\nAnalyzing distributions (is it normal?)\nFinding patterns in data (aka data mining)\nMore complex data transformation techniques (such as factor analysis)"
  },
  {
    "objectID": "EDA.html#data-journey",
    "href": "EDA.html#data-journey",
    "title": "3  Exploratory Data Analysis",
    "section": "3.3 Data journey",
    "text": "3.3 Data journey\nWickham and Grolemund (2016) present a visual sketch of what could be called the “data journey”, i.e., the steps we are taking in order to learn from data, seen from an hands-on angle, see Figure 3.1.\n\n\n\nFigure 3.1: The data journey"
  },
  {
    "objectID": "EDA.html#blitz-data",
    "href": "EDA.html#blitz-data",
    "title": "3  Exploratory Data Analysis",
    "section": "3.4 Blitz data",
    "text": "3.4 Blitz data\nSee Section 2.5 for some data sets suitable to get going."
  },
  {
    "objectID": "EDA.html#data-explorer",
    "href": "EDA.html#data-explorer",
    "title": "3  Exploratory Data Analysis",
    "section": "3.5 Data Explorer",
    "text": "3.5 Data Explorer\nThere are many systems and approaches to explore data. One particular interesting system is the R-package DataExplorer.\n\n\n\nR-package DataExplorer\n\n\nCheck it out on its Githup page."
  },
  {
    "objectID": "EDA.html#vtree",
    "href": "EDA.html#vtree",
    "title": "3  Exploratory Data Analysis",
    "section": "3.6 vtree",
    "text": "3.6 vtree\nA bit similar to {DataExplorer}, the R package {vtree} helps to explore visually datasets.\n\n\n\nvtree is used to generate variable trees, like the one above."
  },
  {
    "objectID": "EDA.html#tidyverse",
    "href": "EDA.html#tidyverse",
    "title": "3  Exploratory Data Analysis",
    "section": "3.7 Tidyverse",
    "text": "3.7 Tidyverse\nThe Tidyverse is probably the R thing with the most publicity. And it’s great. It’s a philosophy baken into an array of R packages. Perhaps central is the idea that a lot of little lego pieces, if fitting nicely together, provides a simple yet flexibel and thus powerful machinery."
  },
  {
    "objectID": "EDA.html#janitor",
    "href": "EDA.html#janitor",
    "title": "3  Exploratory Data Analysis",
    "section": "3.8 janitor",
    "text": "3.8 janitor\nThe R package {janitor} provides some nice stuff for data cleansing. Check out this case study."
  },
  {
    "objectID": "EDA.html#the-easystats-way",
    "href": "EDA.html#the-easystats-way",
    "title": "3  Exploratory Data Analysis",
    "section": "3.9 The easystats way",
    "text": "3.9 The easystats way\nThere are some packages, such as {easystats}, which provide comfortable access to basic statistics:\n\nlibrary(easystats)  # once per session\ndescribe_distribution(mtcars)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nMin\nMax\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nmpg\n20.090625\n6.0269481\n7.52500\n10.400\n33.900\n0.6723771\n-0.0220063\n32\n0\n\n\ncyl\n6.187500\n1.7859216\n4.00000\n4.000\n8.000\n-0.1922609\n-1.7627939\n32\n0\n\n\ndisp\n230.721875\n123.9386938\n221.52500\n71.100\n472.000\n0.4202331\n-1.0675234\n32\n0\n\n\nhp\n146.687500\n68.5628685\n84.50000\n52.000\n335.000\n0.7994067\n0.2752116\n32\n0\n\n\ndrat\n3.596563\n0.5346787\n0.84000\n2.760\n4.930\n0.2927802\n-0.4504325\n32\n0\n\n\nwt\n3.217250\n0.9784574\n1.18625\n1.513\n5.424\n0.4659161\n0.4165947\n32\n0\n\n\nqsec\n17.848750\n1.7869432\n2.02250\n14.500\n22.900\n0.4063466\n0.8649307\n32\n0\n\n\nvs\n0.437500\n0.5040161\n1.00000\n0.000\n1.000\n0.2645418\n-2.0632731\n32\n0\n\n\nam\n0.406250\n0.4989909\n1.00000\n0.000\n1.000\n0.4008089\n-1.9665503\n32\n0\n\n\ngear\n3.687500\n0.7378041\n1.00000\n3.000\n5.000\n0.5823086\n-0.8952916\n32\n0\n\n\ncarb\n2.812500\n1.6152000\n2.00000\n1.000\n8.000\n1.1570911\n2.0200593\n32\n0\n\n\n\n\n\n\ndescribe_distribution provides us with an overview on typical descriptive summaries.\nFor nominal variables, consider data_tabulate:\n\ndata_tabulate(mtcars, select = c(\"am\", \"vs\"))\n\nam (am) <numeric>\n# total N=32 valid N=32\n\nValue |  N | Raw % | Valid % | Cumulative %\n------+----+-------+---------+-------------\n0     | 19 | 59.38 |   59.38 |        59.38\n1     | 13 | 40.62 |   40.62 |       100.00\n<NA>  |  0 |  0.00 |    <NA> |         <NA>\n\nvs (vs) <numeric>\n# total N=32 valid N=32\n\nValue |  N | Raw % | Valid % | Cumulative %\n------+----+-------+---------+-------------\n0     | 18 | 56.25 |   56.25 |        56.25\n1     | 14 | 43.75 |   43.75 |       100.00\n<NA>  |  0 |  0.00 |    <NA> |         <NA>\n\n\nWe can also get grouped tabulations, which amounts to something similar to a contingency table:\n\nmtcars %>% \n  group_by(am) %>% \n  data_tabulate(select = \"vs\", collapse = TRUE)\n\n# Frequency Table\n\nVariable |  Group | Value |  N | Raw % | Valid % | Cumulative %\n---------+--------+-------+----+-------+---------+-------------\nvs       | am (0) |     0 | 12 | 63.16 |   63.16 |        63.16\n         |        |     1 |  7 | 36.84 |   36.84 |       100.00\n         |        |  <NA> |  0 |  0.00 |    <NA> |         <NA>\n---------+--------+-------+----+-------+---------+-------------\nvs       | am (1) |     0 |  6 | 46.15 |   46.15 |        46.15\n         |        |     1 |  7 | 53.85 |   53.85 |       100.00\n         |        |  <NA> |  0 |  0.00 |    <NA> |         <NA>\n---------------------------------------------------------------\n\n\n\nCheckout the function reference of your favorite package in order to learn what’s on the shelf. For example, here’s the function reference site of datawizard, one of the packages in the easystats ecosystem."
  },
  {
    "objectID": "EDA.html#case-study",
    "href": "EDA.html#case-study",
    "title": "3  Exploratory Data Analysis",
    "section": "3.10 Case Study",
    "text": "3.10 Case Study\n\n\n\nR package/dataset palmerpenguins\n\n\nExplore the palmerpenguins dataset, it’s a famous dataset made for learning data analysis.\nThere’s a great interactive course on EDA based on the penguins. Have a look, it’s great!\nGo penguins! Allez!"
  },
  {
    "objectID": "EDA.html#cheatsheets",
    "href": "EDA.html#cheatsheets",
    "title": "3  Exploratory Data Analysis",
    "section": "3.11 Cheatsheets",
    "text": "3.11 Cheatsheets\nThere are a number of nice cheat sheets available on an array of topics related to EDA, made available by the folks at RStudio.\nConsider this collection:\n\n{dplyr}: data wrangling\n{tidyr}: data preparation\n{ggplot}: data visualization\n{gtsummary}: publication ready tables\n\nSo much great stuff! A bit too much to digest in one go, but definitely worthwhile if you plan to dig deeper in data analysis."
  },
  {
    "objectID": "EDA.html#literature",
    "href": "EDA.html#literature",
    "title": "3  Exploratory Data Analysis",
    "section": "3.12 Literature",
    "text": "3.12 Literature\nWickham and Grolemund (2016) is an highly recommendable resource in order to get a thorough understanding of data analysis using R. Note that this source is focusing on the “how to”, not so much to theoretical foundations. Ismay and Kim (2020) is a gently introduction into many steps on the data journey, including EDA.\n\n\n\n\nIsmay, Chester, and Albert Young-Sun Kim. 2020. Statistical Inference via Data Science: A ModernDive into r and the Tidyverse. Chapman & Hall/CRC the r Series. Boca Raton: CRC Press / Taylor & Francis Group. https://moderndive.com/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Visualize, Model, Transform, Tidy, and Import Data. O’Reilly Media. https://r4ds.had.co.nz/index.html."
  },
  {
    "objectID": "inference.html#what-is-it",
    "href": "inference.html#what-is-it",
    "title": "4  Inference",
    "section": "4.1 What is it?",
    "text": "4.1 What is it?\nStatistical inference, according to Gelman, Hill, and Vehtari (2021), chap. 1.1, faces the challenge of generalizing from the particular to the general.\nIn more details, this amounts to generalizing from …\n\na sample to a population\na treatment to a control group (i.e., causal inference)\nobserved measurement to the underlying (“latent”) construct of interest\n\n\n\n\n\n\n\nImportant\n\n\n\nStatistical inference is concerned with making general claims from particular data using mathematical tools."
  },
  {
    "objectID": "inference.html#population-and-sample",
    "href": "inference.html#population-and-sample",
    "title": "4  Inference",
    "section": "4.2 Population and sample",
    "text": "4.2 Population and sample\nWe want to have an estimate of some population value, for example the proportion of A.\nHowever, all we have is a subset, a sample of the populuation. Hence, we need to infer from the sample to the popluation. We do so by generalizing from the sample to the population, see Figure Figure 4.1.\n\n\n\n\n\n\n\n(a) Population\n\n\n\n\n\n\n\n(b) Sample\n\n\n\n\nFigure 4.1: Population vs. sample (Image credit: Karsten Luebke)"
  },
  {
    "objectID": "inference.html#whats-not-inference",
    "href": "inference.html#whats-not-inference",
    "title": "4  Inference",
    "section": "4.3 What’s not inference?",
    "text": "4.3 What’s not inference?\nConsider fig. Figure 4.2 which epitomizes the difference between descriptive and inferential statistics.\n\n\n\n\n\nFigure 4.2: The difference between description and inference"
  },
  {
    "objectID": "inference.html#when-size-helps",
    "href": "inference.html#when-size-helps",
    "title": "4  Inference",
    "section": "4.4 When size helps",
    "text": "4.4 When size helps\nLarger samples allow for more precise estimations (ceteris paribus).\n\nknitr::include_graphics(\"img/Estimate.gif\")\n\n\n\n\nSample size in motion, Image credit: Karsten Luebke"
  },
  {
    "objectID": "inference.html#what-flavors-are-available",
    "href": "inference.html#what-flavors-are-available",
    "title": "4  Inference",
    "section": "4.5 What flavors are available?",
    "text": "4.5 What flavors are available?\nTypically, when one hears “inference” one thinks of p-values and null hypothesis testing. Those procedures are examples of the school of Frequentist statistics.\nHowever, there’s a second flavor of statistics to be mentioned here: Bayesian statistics.\n\n4.5.1 Frequentist inference\nFrequentism is not concerned about the probability of your research hypothesis.\nFrequentism is all about controlling the long-term error. For illustration, suppose you are the CEO of a factory producing screws, and many of them. As the boss, you are not so much interested if a particular scree is in order (or faulty). Rather you are interested that the overall, long-term error rate of your production is low. One may add that your goal might not the minimize the long-term error, b ut to control it to a certain level - it may be to expensive to produce super high quality screws. Some decent, but cheap screws, might be more profitable.\n\n\n4.5.2 Bayes inference\nBayes inference is concerned about the probability of your research hypothesis.\nIt simply redistributes your beliefs based on new data (evidence) you observe, see Figure Figure 4.3.\n\n\n\n\n\nflowchart LR\n  A(prior belief) --> B(new data) --> C(posterior belief)\n\n\n\n\n\n\nFigure 4.3: Bayesian belief updating\n\n\n\n\nIn more detail, the posterior belief is formalized as the posterior probability. The Likelihood is the probability of the data given some hypothesis. The normalizing constant serves to give us a number between zero and one.\n\\[\\overbrace{\\Pr(\\color{blue}{H}|\\color{green}{D})}^\\text{posterior probability} = \\overbrace{Pr(\\color{blue}{H})}^\\text{prior} \\frac{\\overbrace{Pr(\\color{green}{D}|\\color{blue}{H})}^\\text{likelihood}}{\\underbrace{Pr(\\color{green}{D})}_{\\text{normalizing constant}}}\\]\nIn practice, the posterior probability of your hypothesis is, the average of your prior and the Likelihood of your data.\n\n\n\nPrior-Likelihood-Posterior\n\n\nCan you see that the posterior is some average of prior and likelihood?\nCheck out this great video on Bayes Theorem by 3b1b."
  },
  {
    "objectID": "inference.html#but-which-one-should-i-consume",
    "href": "inference.html#but-which-one-should-i-consume",
    "title": "4  Inference",
    "section": "4.6 But which one should I consume?",
    "text": "4.6 But which one should I consume?\nPRO Frequentist:\n\nYour supervisor and reviewers will be more familiar with it\nThe technical overhead is simpler compared to Bayes\n\nPRO Bayes:\n\nYou’ll probably want to have a posterior probability of your hypothesis\nYou may appear as a cool kid and an early adoptor of emering statistical methods\n\n\n\n\n\n\n\nTip\n\n\n\nYou’ll learn that the technical setup used for doing Bayes statistics is quite similar to doing frequentist statistics. Stay tuned."
  },
  {
    "objectID": "inference.html#comment-from-xkcd",
    "href": "inference.html#comment-from-xkcd",
    "title": "4  Inference",
    "section": "4.7 Comment from xkcd",
    "text": "4.7 Comment from xkcd\n\n\n\n\n\n\n\n\n\nQuelle"
  },
  {
    "objectID": "inference.html#p-value",
    "href": "inference.html#p-value",
    "title": "4  Inference",
    "section": "4.8 p-value",
    "text": "4.8 p-value\nThe p-value has been used as the pivotal criterion to decide about whether or not a research hypothesis were to be “accepted” (a term forbidden in frequentist and Popperian langauge) or to be rejected. However, more recently, it is advised to use the p-value only as one indicator among multiple; see Wasserstein and Lazar (2016) and Wasserstein, Schirm, and Lazar (2019).\n\n\n\n\n\n\nImportant\n\n\n\nThe p-value is defined as the probability of obtaining the observed data (or more extreme) under the assumption of no effect.\n\n\nFigure Figure 4.4 visualizes the p-value.\n\n\n\n\n\nFigure 4.4: Visualization of the p-value"
  },
  {
    "objectID": "inference.html#some-confusion-remains-about-the-p-value",
    "href": "inference.html#some-confusion-remains-about-the-p-value",
    "title": "4  Inference",
    "section": "4.9 Some confusion remains about the p-value",
    "text": "4.9 Some confusion remains about the p-value\n\n\n\nSource: from ImgFlip Meme Generator\n\n\nGoodman (2008) provides an entertaining overview on typical misconceptions of the p-value full text.\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge: Cambridge University Press.\n\n\nGoodman, Steven. 2008. “A Dirty Dozen: Twelve p-Value Misconceptions.” Seminars in Hematology, Interpretation of quantitative research, 45 (3): 135–40. https://doi.org/10.1053/j.seminhematol.2008.04.003.\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA’s Statement on p-Values: Context, Process, and Purpose.” The American Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108.\n\n\nWasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. 2019. “Moving to a World Beyond ‘ p</i> p < 0.05’.” The American Statistician 73 (March): 1–19. https://doi.org/10.1080/00031305.2019.1583913."
  },
  {
    "objectID": "regression1.html#r-packages-needed-for-this-chapter",
    "href": "regression1.html#r-packages-needed-for-this-chapter",
    "title": "5  Modelling and regression",
    "section": "5.1 R packages needed for this chapter",
    "text": "5.1 R packages needed for this chapter\n\nlibrary(easystats)\nlibrary(tidyverse)\nlibrary(rstanarm)  # optional!"
  },
  {
    "objectID": "regression1.html#whats-modelling",
    "href": "regression1.html#whats-modelling",
    "title": "5  Modelling and regression",
    "section": "5.2 What’s modelling?",
    "text": "5.2 What’s modelling?\nRead this great introduction by modelling by Russel Poldrack. Actually, the whole book is nice Poldrack (2022).\nAn epitome of modelling is this, let’s call it the fundamental modelling equation, a bit grandiose but at the point, see Equation 5.1.\nThe data can be separated in the model’s prediction and the rest (the “error”), i.e., what’s unaccounted for by the model.\n\\[\n\\text{data} = \\text{model} + \\text{error}\n\\tag{5.1}\\]\nA more visual account of our basic modelling equation is depicted in Figure 5.1.\n\n\n\n\n\nflowchart LR\n  X --> Y\n  error --> Y\n\n\n\n\n\nFigure 5.1: A more visual account of our basic modelling equation"
  },
  {
    "objectID": "regression1.html#regression-as-the-umbrella-tool-for-modelling",
    "href": "regression1.html#regression-as-the-umbrella-tool-for-modelling",
    "title": "5  Modelling and regression",
    "section": "5.3 Regression as the umbrella tool for modelling",
    "text": "5.3 Regression as the umbrella tool for modelling\n Source: Image Flip\nAlternatively, venture into the forest of statistical tests as outlined e.g. here, at Uni Muenster.\nYou may want to ponder on this image of a decision tree of which test to choose, see Figure Figure 5.2.\n\n\n\nFigure 5.2: Choose your test carefully\n\n\n\n5.3.1 Common statistical tests are linear models\nAs Jonas Kristoffer Lindeløv tells us, we can formulate most statistical tests as a linear model, ie., a regression.\n\n\n\nCommon statistical tests as linear models\n\n\n\n\n5.3.2 How to find the regression line\nIn the simplest case, regression analyses can be interpreted geometrically as a line in a 2D coordinate system, see Figure Figure 5.3.\n\n\n\nFigure 5.3: Least Square Regression\n\n\nSource: Orzetoo, CC-SA, Wikimedia\nPut simple, we are looking for the line which is in the “middle of the points”. More precisely, we place the line such that the squared distances from the line to the points is minimal, see Figre Figure 5.3.\nConsider Figure Figure 5.4, from this source by Roback and Legler (2021). It visualizes not only the notorious regression line, but also sheds light on regression assumptions, particularly on the error distribution.\n\n\n\nFigure 5.4: Regression and some of its assumptions\n\n\n\n\n5.3.3 The linear model\nHere’s the canonical form of the linear model.\nConsider a model with \\(k\\) predictors:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k + \\epsilon\\]\n\n\n5.3.4 Algebraic derivation\nFor the mathematical inclined, check out this derivation of the simple case regression model. Note that the article is written in German, but your browser can effortlessly translate into English. Here’s a similar English article from StackExchange."
  },
  {
    "objectID": "regression1.html#in-all-its-glory",
    "href": "regression1.html#in-all-its-glory",
    "title": "5  Modelling and regression",
    "section": "5.4 In all its glory",
    "text": "5.4 In all its glory"
  },
  {
    "objectID": "regression1.html#first-model-one-metric-predictor",
    "href": "regression1.html#first-model-one-metric-predictor",
    "title": "5  Modelling and regression",
    "section": "5.5 First model: one metric predictor",
    "text": "5.5 First model: one metric predictor\nFirst, let’s load some data:\n\ndata(mtcars)\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\n5.5.1 Frequentist\nDefine and fit the model:\n\nlm1_freq <- lm(mpg ~ hp, data = mtcars)\n\nGet the parameter values:\n\nparameters(lm1_freq)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n30.0988605\n1.6339210\n0.95\n26.7619488\n33.4357723\n18.421246\n30\n0e+00\n\n\nhp\n-0.0682283\n0.0101193\n0.95\n-0.0888947\n-0.0475619\n-6.742388\n30\n2e-07\n\n\n\n\n\n\nPlot the model parameters:\n\nplot(parameters(lm1_freq))\n\n\n\n\n\n\n5.5.2 Bayesian\n\nlm1_bayes <- stan_glm(mpg ~ hp, data = mtcars)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 7.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.74 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.034904 seconds (Warm-up)\nChain 1:                0.035594 seconds (Sampling)\nChain 1:                0.070498 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.7e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.036974 seconds (Warm-up)\nChain 2:                0.030925 seconds (Sampling)\nChain 2:                0.067899 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.8e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.033843 seconds (Warm-up)\nChain 3:                0.031322 seconds (Sampling)\nChain 3:                0.065165 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.033838 seconds (Warm-up)\nChain 4:                0.030412 seconds (Sampling)\nChain 4:                0.06425 seconds (Total)\nChain 4: \n\n\nActually, we want to suppress some overly verbose output of the sampling, so add the argument refresh = 0:\n\nlm1_bayes <- stan_glm(mpg ~ hp, data = mtcars, refresh = 0)\n\nGet the parameter values:\n\nparameters(lm1_bayes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\nCI\nCI_low\nCI_high\npd\nROPE_Percentage\nRhat\nESS\nPrior_Distribution\nPrior_Location\nPrior_Scale\n\n\n\n\n(Intercept)\n30.0458334\n0.95\n26.744874\n33.3780301\n1\n0\n0.9999552\n3816.002\nnormal\n20.09062\n15.0673701\n\n\nhp\n-0.0683247\n0.95\n-0.088284\n-0.0476861\n1\n1\n1.0002417\n3851.764\nnormal\n0.00000\n0.2197599\n\n\n\n\n\n\nPlot the model parameters:\n\nplot(parameters(lm1_bayes))\n\n\n\n\n\n\n5.5.3 Model performance\n\nr2(lm1_freq)\n\n# R2 for Linear Regression\n       R2: 0.602\n  adj. R2: 0.589\n\n\n\nr2(lm1_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.584 (95% CI [0.378, 0.749])\n\n\n\n\n5.5.4 Model check\nHere’s a bunch of typical model checks in the Frequentist sense.\n\ncheck_model(lm1_freq)\n\n\n\n\nAnd here are some Bayesian flavored model checks.\n\ncheck_model(lm1_bayes)\n\n\n\n\n\n\n5.5.5 Get some predictions\n\nlm1_pred <- estimate_relation(lm1_freq)\nlm1_pred\n\n\n\n\n\nhp\nPredicted\nSE\nCI_low\nCI_high\n\n\n\n\n52.00000\n26.550990\n1.1766139\n24.148024\n28.95396\n\n\n83.44444\n24.405590\n0.9358933\n22.494241\n26.31694\n\n\n114.88889\n22.260189\n0.7548971\n20.718484\n23.80190\n\n\n146.33333\n20.114789\n0.6828911\n18.720139\n21.50944\n\n\n177.77778\n17.969389\n0.7518697\n16.433866\n19.50491\n\n\n209.22222\n15.823989\n0.9310065\n13.922620\n17.72536\n\n\n240.66667\n13.678588\n1.1707841\n11.287528\n16.06965\n\n\n272.11111\n11.533188\n1.4412478\n8.589767\n14.47661\n\n\n303.55556\n9.387788\n1.7280486\n5.858642\n12.91693\n\n\n335.00000\n7.242387\n2.0242544\n3.108308\n11.37647\n\n\n\n\n\n\nMore details on the above function can be found on the respective page at the easystats site.\n\n\n5.5.6 Plot the model\n\nplot(lm1_pred)"
  },
  {
    "objectID": "regression1.html#more-of-this",
    "href": "regression1.html#more-of-this",
    "title": "5  Modelling and regression",
    "section": "5.6 More of this",
    "text": "5.6 More of this\nMore technical details for gauging model performance and model quality, can be found on the site of the R package “performance at the easystats site."
  },
  {
    "objectID": "regression1.html#bayes-members-only",
    "href": "regression1.html#bayes-members-only",
    "title": "5  Modelling and regression",
    "section": "5.7 Bayes-members only",
    "text": "5.7 Bayes-members only\nBayes statistics provide a distribution as the result of the analysis, the posterior distribution, which provides us with quite some luxury.\nAs the posterior distribution manifests itself by a number of samples, we can easily filter and manipulate this sample distribution in order to ask some interesing questions.\nSee:\n\nlm1_bayes %>% \n  as_tibble() %>% \n  head()\n\n\n\n\n\n(Intercept)\nhp\nsigma\n\n\n\n\n31.31765\n-0.0801401\n4.248651\n\n\n33.89991\n-0.0824994\n4.802076\n\n\n30.73198\n-0.0680882\n3.826181\n\n\n30.09580\n-0.0701588\n4.905716\n\n\n30.35285\n-0.0656387\n3.295531\n\n\n31.50666\n-0.0706685\n4.168542\n\n\n\n\n\n\n\n5.7.1 Asking for probabilites\nWhat’s the probability that the effect of hp is negative?\n\nlm1_bayes %>% \n  as_tibble() %>% \n  count(hp < 0)\n\n\n\n\n\nhp < 0\nn\n\n\n\n\nTRUE\n4000\n\n\n\n\n\n\nFeel free to ask similar questions!\n\n\n5.7.2 Asking for quantiles\nWith a given probability of, say 90%, how large is the effect of hp?\n\nlm1_bayes %>% \n  as_tibble() %>% \n  summarise(q_90 = quantile(hp, .9))\n\n\n\n\n\nq_90\n\n\n\n\n-0.0547491\n\n\n\n\n\n\nWhat’s the smallest 95% percent interval for the effect of hp?\n\nhdi(lm1_bayes)\n\n\n\n\n\nParameter\nCI\nCI_low\nCI_high\nEffects\nComponent\n\n\n\n\n(Intercept)\n0.95\n26.8722745\n33.4599590\nfixed\nconditional\n\n\nhp\n0.95\n-0.0884446\n-0.0478163\nfixed\nconditional\n\n\n\n\n\n\nIn case you prefer 89% intervals (I do!):\n\nhdi(lm1_bayes, ci = .89)\n\n\n\n\n\nParameter\nCI\nCI_low\nCI_high\nEffects\nComponent\n\n\n\n\n(Intercept)\n0.89\n27.3645122\n32.6733738\nfixed\nconditional\n\n\nhp\n0.89\n-0.0850095\n-0.0523753\nfixed\nconditional"
  },
  {
    "objectID": "regression1.html#multiple-metric-predictors",
    "href": "regression1.html#multiple-metric-predictors",
    "title": "5  Modelling and regression",
    "section": "5.8 Multiple metric predictors",
    "text": "5.8 Multiple metric predictors\nAssume we have a theory that dictates that fuel economy is a (causal) function of horse power and engine displacement.\n\nlm2_freq <- lm(mpg ~ hp + disp, data = mtcars)\nparameters(lm2_freq)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n30.7359042\n1.3315661\n0.95\n28.0125457\n33.4592628\n23.082522\n29\n0.0000000\n\n\nhp\n-0.0248401\n0.0133855\n0.95\n-0.0522165\n0.0025363\n-1.855746\n29\n0.0736791\n\n\ndisp\n-0.0303463\n0.0074049\n0.95\n-0.0454909\n-0.0152016\n-4.098159\n29\n0.0003063\n\n\n\n\n\n\nSimilarly for Bayes inference:\n\nlm2_bayes <- stan_glm(mpg ~ hp + disp, data = mtcars)\n\nResults\n\nparameters(lm2_bayes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\nCI\nCI_low\nCI_high\npd\nROPE_Percentage\nRhat\nESS\nPrior_Distribution\nPrior_Location\nPrior_Scale\n\n\n\n\n(Intercept)\n30.7631742\n0.95\n27.9592857\n33.4523213\n1.00000\n0\n1.000854\n4605.880\nnormal\n20.09062\n15.0673701\n\n\nhp\n-0.0252340\n0.95\n-0.0538511\n0.0032994\n0.95825\n1\n1.001172\n2080.083\nnormal\n0.00000\n0.2197599\n\n\ndisp\n-0.0304116\n0.95\n-0.0456015\n-0.0144810\n0.99900\n1\n1.000431\n1957.573\nnormal\n0.00000\n0.1215712\n\n\n\n\n\nplot(parameters(lm2_bayes))\n\n\n\nr2(lm2_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.731 (95% CI [0.580, 0.844])\n\n\nDepending on the value of disp the prediction of mpg from hp will vary:\n\nlm2_pred <- estimate_relation(lm2_freq)\nplot(lm2_pred)"
  },
  {
    "objectID": "regression1.html#one-nominal-predictor",
    "href": "regression1.html#one-nominal-predictor",
    "title": "5  Modelling and regression",
    "section": "5.9 One nominal predictor",
    "text": "5.9 One nominal predictor\n\nlm3a <- lm(mpg ~ am, data = mtcars)\nparameters(lm3a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n17.147368\n1.124602\n0.95\n14.85062\n19.44411\n15.247492\n30\n0.000000\n\n\nam\n7.244939\n1.764422\n0.95\n3.64151\n10.84837\n4.106127\n30\n0.000285\n\n\n\n\n\n\n\nlm3a_means <- estimate_means(lm3a, at = \"am = c(0, 1)\")\nlm3a_means \n\n\n\n\n\nam\nMean\nSE\nCI_low\nCI_high\n\n\n\n\n0\n17.14737\n1.124602\n14.85062\n19.44411\n\n\n1\n24.39231\n1.359578\n21.61568\n27.16894\n\n\n\n\n\n\nIf we were not to specify the values of am which we would like to get predictions for, the default of the function would select 10 values, spreaded across the range of am. For numeric variables, this is usually fine. However, for nominal variables - and am is in fact a nominally scaled variable - we insist that we want predictions for the levels of the variable only, that is for 0 and 1.\nHowever, unfortunately, the plot needs a nominal variable if we are to compare groups. In our case, am is considered a numeric variables, since it consists of numbers only. The plot does not work, malheureusement:\n\nplot(lm3a_means)\n\n\n\n\nWe need to transform am to a factor variable. That’s something like a string. If we hand over a factor() to the plotting function, everything will run smoothly. Computationwise, no big differences:\n\nmtcars2 <-\n  mtcars %>% \n  mutate(am_f = factor(am))\n\nlm3a <- lm(mpg ~ am_f, data = mtcars2)\nparameters(lm3a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n17.147368\n1.124602\n0.95\n14.85062\n19.44411\n15.247492\n30\n0.000000\n\n\nam_f1\n7.244939\n1.764422\n0.95\n3.64151\n10.84837\n4.106127\n30\n0.000285\n\n\n\n\n\n\n\nlm3a_means <- estimate_means(lm3a)\nplot(lm3a_means)\n\n\n\n\nNote that we should have converted am to a factor variable before fitting the model. Otherwise, the plot won’t work.\nHere’s a more hand-crafted version of the last plot, see Fig. Figure 5.5.\n\nggplot(mtcars2) +\n  aes(x = am_f, y = mpg) +\n  geom_violin() +\n  geom_jitter(width = .1, alpha = .5) +\n  geom_pointrange(data = lm3a_means,\n                  color = \"orange\",\n                  aes(ymin = CI_low, ymax = CI_high, y = Mean)) +\n  geom_line(data = lm3a_means, aes(y = Mean, group = 1))\n\n\n\n\nFigure 5.5: ?(caption)"
  },
  {
    "objectID": "regression1.html#one-metric-and-one-nominal-predictor",
    "href": "regression1.html#one-metric-and-one-nominal-predictor",
    "title": "5  Modelling and regression",
    "section": "5.10 One metric and one nominal predictor",
    "text": "5.10 One metric and one nominal predictor\n\nmtcars2 <-\n  mtcars %>% \n  mutate(cyl = factor(cyl))\n\nlm4 <- lm(mpg ~ hp + cyl, data = mtcars2)\nparameters(lm4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n28.6501182\n1.5877870\n0.95\n25.3976840\n31.9025524\n18.044056\n28\n0.0000000\n\n\nhp\n-0.0240388\n0.0154079\n0.95\n-0.0556005\n0.0075228\n-1.560163\n28\n0.1299540\n\n\ncyl6\n-5.9676551\n1.6392776\n0.95\n-9.3255631\n-2.6097471\n-3.640418\n28\n0.0010921\n\n\ncyl8\n-8.5208508\n2.3260749\n0.95\n-13.2855993\n-3.7561022\n-3.663188\n28\n0.0010286\n\n\n\n\n\n\n\nlm4_pred <- estimate_relation(lm4)\nplot(lm4_pred)"
  },
  {
    "objectID": "regression1.html#watch-out-for-simpson",
    "href": "regression1.html#watch-out-for-simpson",
    "title": "5  Modelling and regression",
    "section": "5.11 Watch out for Simpson",
    "text": "5.11 Watch out for Simpson\nBeware! Model estimates can swing wildly if you add (or remove) some predictor from your model. See this post for an demonstration."
  },
  {
    "objectID": "regression1.html#what-about-correlation",
    "href": "regression1.html#what-about-correlation",
    "title": "5  Modelling and regression",
    "section": "5.12 What about correlation?",
    "text": "5.12 What about correlation?\nCorrelation is really a close cousin to regression. In fact, regression with standardized variables amounts to correlation.\nLet’s get the correlation matrix of the variables in involved in lm4.\n\nlm4_corr <- \n  mtcars %>% \n  select(mpg, hp, disp) %>% \n  correlation()\n\nlm4_corr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\nt\ndf_error\np\nMethod\nn_Obs\n\n\n\n\nmpg\nhp\n-0.7761684\n0.95\n-0.8852686\n-0.5860994\n-6.742388\n30\n2e-07\nPearson correlation\n32\n\n\nmpg\ndisp\n-0.8475514\n0.95\n-0.9233594\n-0.7081376\n-8.747151\n30\n0e+00\nPearson correlation\n32\n\n\nhp\ndisp\n0.7909486\n0.95\n0.6106794\n0.8932775\n7.080122\n30\n1e-07\nPearson correlation\n32\n\n\n\n\n\n\n\nplot(summary(lm4_corr))"
  },
  {
    "objectID": "regression1.html#exercises",
    "href": "regression1.html#exercises",
    "title": "5  Modelling and regression",
    "section": "5.13 Exercises",
    "text": "5.13 Exercises\n\nmtcars simple 1\nmtcars simple 2\nmtcars simple 3"
  },
  {
    "objectID": "regression1.html#lab",
    "href": "regression1.html#lab",
    "title": "5  Modelling and regression",
    "section": "5.14 Lab",
    "text": "5.14 Lab\nGet your own data, and build a simple model reflecting your research hypothesis. If you are lacking data (or hypothesis) get something close to it."
  },
  {
    "objectID": "regression1.html#literature",
    "href": "regression1.html#literature",
    "title": "5  Modelling and regression",
    "section": "5.15 Literature",
    "text": "5.15 Literature\nAn accessible treatment of regression is provided by Ismay and Kim (2020).\nRoback and Legler (2021) provide a more than introductory account of regression while being accessible. A recent but already classic book (if this is possible) is the book by Gelman, Hill, and Vehtari (2021). You may also benefit from Poldrack (2022) (open access)."
  },
  {
    "objectID": "regression1.html#debrief",
    "href": "regression1.html#debrief",
    "title": "5  Modelling and regression",
    "section": "5.16 Debrief",
    "text": "5.16 Debrief\nScience!\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge: Cambridge University Press.\n\n\nIsmay, Chester, and Albert Young-Sun Kim. 2020. Statistical Inference via Data Science: A ModernDive into r and the Tidyverse. Chapman & Hall/CRC the r Series. Boca Raton: CRC Press / Taylor & Francis Group. https://moderndive.com/.\n\n\nPoldrack, Russell. 2022. Statistical Thinking for the 21st Century. https://statsthinking21.github.io/statsthinking21-core-site/index.html.\n\n\nRoback, Paul, and Julie Legler. 2021. Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in. 1st ed. Chapman and Hall Texts in Statistical Science. Boca Raton: CRC Press."
  },
  {
    "objectID": "regression2.html#r-packages-needed",
    "href": "regression2.html#r-packages-needed",
    "title": "6  More lineare models",
    "section": "6.1 R-packages needed",
    "text": "6.1 R-packages needed"
  },
  {
    "objectID": "regression2.html#r-packages-needed-for-this-chapter",
    "href": "regression2.html#r-packages-needed-for-this-chapter",
    "title": "6  More lineare models",
    "section": "6.2 R packages needed for this chapter",
    "text": "6.2 R packages needed for this chapter\n\nlibrary(easystats)\nlibrary(tidyverse)"
  },
  {
    "objectID": "regression2.html#multiplicative-associations",
    "href": "regression2.html#multiplicative-associations",
    "title": "6  More lineare models",
    "section": "6.3 Multiplicative associations",
    "text": "6.3 Multiplicative associations\n\n6.3.1 The Log-Y model\nConsider again the linear model, in a simple form:\n\\[\\hat{y} = \\beta_0 + \\beta_1 x_1 +  \\ldots + b_kx_k +\\] Surprisingly, we can use this linear model to describe multiplicative assocations:\n\\(\\hat{y} = e^{b_0 + b_1x_1 + b_2x_2 + \\ldots + b_kx_k}\\)\n(I wrote b instead of \\(\\beta\\) just to show that both has its meaning, but are separate things.)\nExponentiate both sides to get:\n\\(log (\\hat{y}) = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_kx_k\\)\nFor simplicity, let’s drop the subscripts in the following without loss of generality and keep it short:\n\\(y = e^{x}\\), with \\(e \\approx 2.71...\\)\nExponentiate both sides to get:\n\\(log(y) = x\\)\nThis association is called multiplicative, because if x increases by 1, y increased by a constant factor.\n\n\n\n\n\n\nNote\n\n\n\nThe logarithm is not defined for negative (input) values. And \\(log(0) = -\\infty\\).\n\n\nA side-effect of modelling log_y instead of y is that the distribution shape of the outcome variable changes. This can be useful at times.\nLog-Y Regression can usefully be employed for modelling growth, among othrs, see Example 6.1.\n\nExample 6.1 (Bacteria growth) Some bacteria dish grows with at a fixed proportional rate, that is it doubles its population size in a fixed period of time. This is what is called exponential growth. For concreteness, say, the bacteriae double each two days, starting with 1 unit of bacteria.\nAfter about three weeks, we’ll have this number (of units) of bacteriae:\n\ne <- 2.7178\ne^10\n\n[1] 21987.45\n\n\n\n\n\n6.3.2 Exercise\n\nEffect of education on income\nEffect of log-y transformation on the distribution, an example\n\n\n\n\n\n\n\nNote\n\n\n\nThe exercises are written in German Language. Don’t fret. Browsers are able to translate websites instantaneously. Alternatively, go to sites such as Google Translate and enter the URL of the website to be translated. Also check out the webstor of your favorite browser to get an extention such as this one for Google Chrome.\n\n\n\n\n6.3.3 Visualizing Log Transformation\nCheck out this post for an example of a log-y regression visualized.\nThis post puts some more weight to the argument that a log-y transformation is useful (if you want to model multiplicative relations).\n\n\n6.3.4 Further reading\nCheck out this great essay by Kenneth Benoit on different log-variants in regression. Also Gelman, Hill, and Vehtari (2021), chapter 12 (and others), is useful."
  },
  {
    "objectID": "regression2.html#interaction",
    "href": "regression2.html#interaction",
    "title": "6  More lineare models",
    "section": "6.4 Interaction",
    "text": "6.4 Interaction\n\n6.4.1 Multiple predictors, no interaction\nRegression analyses can be used with more than one predictor, see Figure Figure 6.1.\n\n\n\n\n\nflowchart LR\nX --> Y1\n\nX1 --> Y2\nX2 --> Y2\n\n\n\n\n\nFigure 6.1: One predictor (X) vs. two predictors (X1, X2)\n\n\n\n\ngiven by Figure ?fig-3dregr, where a 3D account of a regression is given. 3D means to input variables, and (which is always the case) one output variable.\n\n\n\n\n\n\nNote\n\n\n\nNote that the slope in linear in both axis (X1 and X2).\n\n\nA different perspective is shown here,\nwhere a 3D account of a regression is given. 3D means to input variables, and (which is always the case) one output variable.\n\n\n\n\n\n\nImportant\n\n\n\nIf the slope for one predictor is the same for all values of the other predictor, then we say that no interaction is taking place.\n\n\nHere’s a visualization of a 3D regression plane (not line) without interaction: constant slope in one axis, see the following figure, ?fig-3dregr2.\n\n\n\n\n\nFigure 6.2: 3D regression plane (not line) without interaction\n\n\n\n\n\n\n\nFigure 6.3: 3D regression plane (not line) without interaction\n\n\n\n\n\n\n\nFigure 6.4: 3D regression plane (not line) without interaction\n\n\n\n\nNote that in the above figure, the slope in each predictor axis equals 1, boringly. Hence the according 2D plots are boring, too.\nFor the sake of an example, consider this linear model:\n\\(mpg \\sim hp + disp\\)\nOr, in more regression like terms:\n\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\), where x1 is hp and x2 is disp in the mtcars dataset.\nIn R terms:\n\nlm3d <- lm(mpg ~ hp + disp, data = mtcars)\n\nThe 3D plot is shown in Figure Figure 6.5.\n\n\n\n\n\nFigure 6.5: mpg ~ hp + disp\n\n\n\n\nHere are the two corresponding 2d (1 predictor) regression models:\n\nlm1 <- lm(mpg ~ hp, data = mtcars)\nplot(estimate_relation(lm1))\n\n\n\nlm2 <- lm(mpg ~ disp, data = mtcars)\nplot(estimate_relation(lm2))\n\n\n\n\nCheckout this post for a visually slightly more appealing 3d regression plane.\n\n\n6.4.2 Interaction\nFor interaction to happen we relax the assumption that the slope of predictor 1 must be constant for all values of predictor 2.\nIn R, we specify an interaction model like this:\n\nlm3d_interact <- lm(mpg ~ hp + disp + hp:disp, data = mtcars)\n\nThe symbol hp:disp can be read as “the interaction effect of hp and disp”.\nHere’s a visual account, see Figure Figure 6.6.\n\n\n\n\n\nFigure 6.6: mpg ~ hp + disp\n\n\n\n\nCompare Figure 6.6 and Figure 6.5.\nIn Figure 6.6 you’ll see that the lines along the Y axis are not parallel anymore. Similarly, the lines along the X axis are not parallel anymore.\n\n\n\n\n\n\nImportant\n\n\n\nIf the regression lines (indicating different values of one predictor) are not parallel, we say that an interaction effect is taking place.\n\n\nHowever, the difference or change between two adjacent values (lines) is constant. This value is the size the regression effect.\n\n\n6.4.3 Interaction made simple\nIf you find that two sophisticated, consider the following simple case.\nFirst, we mutate am to be a factor variable, in order to make things simpler (without loss of generality).\n\nmtcars2 <-\n  mtcars %>% \n  mutate(am_f = factor(am))\n\nNow we use this new variable for a simple regression model:\n\nlm_interact_simple <- lm(mpg ~ disp + am_f + disp:am_f, data = mtcars2)\n\nHere’s the plot, Figure Figure 6.7.\n\nplot(estimate_relation(lm_interact_simple))\n\n\n\n\nFigure 6.7: A simple interaction model\n\n\n\n\nIn this picture, we see that the two regression lines are not parallel, and hence there is evidence of an interaction effect.\nThe interaction effect amounts to the difference in slops in Figure Figure 6.7.\nOne might be inclined to interpret Figure Figure 6.7 as an 3D image, where the one (reddish) line is in the foreground and the blueish line in the background (or vice versa, as you like). Given a 3D image (and hence 2 predictors), we are where we started further above.\nFor completeness, here are the parameters of the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(28)\np\n\n\n\n\n(Intercept)\n25.16\n1.93\n(21.21, 29.10)\n13.07\n< .001\n\n\ndisp\n-0.03\n6.22e-03\n(-0.04, -0.01)\n-4.44\n< .001\n\n\nam f (1)\n7.71\n2.50\n(2.58, 12.84)\n3.08\n0.005\n\n\ndisp * am f (1)\n-0.03\n0.01\n(-0.05, -7.99e-03)\n-2.75\n0.010\n\n\n\n\n\n\n\n6.4.4 Centering variables\nThe effect of of am_f must be interpreted when disp is zero, which does not make much sense.\nTherefore it simplifies the interpretation of regression coefficients to center all input variables, by subtrating the mean value (“demeaning” or “centering”):\n\\[x' = x - \\bar{x}\\] In R, this can be achieved e.g,. in this way:\n\nmtcars3 <- \nmtcars2 %>% \n  mutate(disp_c = disp - mean(disp))\n\n\nlm_interact_simple2 <- lm(mpg ~ disp_c + am_f + disp_c:am_f, data = mtcars3)\nparameters(lm_interact_simple2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n18.7929250\n0.7631321\n0.95\n17.2297199\n20.3561302\n24.6260457\n28\n0.0000000\n\n\ndisp_c\n-0.0275836\n0.0062190\n0.95\n-0.0403225\n-0.0148447\n-4.4354101\n28\n0.0001295\n\n\nam_f1\n0.4517578\n1.3915089\n0.95\n-2.3986189\n3.3021346\n0.3246532\n28\n0.7478567\n\n\ndisp_c:am_f1\n-0.0314548\n0.0114574\n0.95\n-0.0549242\n-0.0079855\n-2.7453781\n28\n0.0104373"
  },
  {
    "objectID": "regression2.html#predictor-relevance",
    "href": "regression2.html#predictor-relevance",
    "title": "6  More lineare models",
    "section": "6.5 Predictor relevance",
    "text": "6.5 Predictor relevance\nGiven a model, we might want to know which predictor has the strongest association with the outcome?\nIn order to answer this question, all predictor must have the same scale. Otherwise the importance of a predictor would increase by 1000, if we multiply each of the observations’ values by the same factor. However, this multiplication should not change the relevance of a predictor.\nA simple solution is to standardize all predictors to the same scale (sd=1).\n\nmtcars4 <-\n  mtcars %>% \n  standardize(select = c(\"disp\", \"hp\", \"cyl\"))\n\nBy the way, “standardizing” centers the variable by default to a mean value of zero (by demeaning).\nSee:\n\nhead(mtcars4$disp)\n\n[1] -0.57061982 -0.57061982 -0.99018209  0.22009369  1.04308123 -0.04616698\n\nhead(mtcars$disp)\n\n[1] 160 160 108 258 360 225\n\n\nHere’s the SD:\n\nsd(mtcars4$disp)\n\n[1] 1\n\nsd(mtcars$disp)\n\n[1] 123.9387\n\n\nAnd here’s the mean value:\n\nmean(mtcars4$disp)\n\n[1] -9.084937e-17\n\nmean(mtcars$disp)\n\n[1] 230.7219\n\n\nNow we are in a position to decide which predictor is more important:\n\nm <- lm(mpg ~ disp + hp + cyl, data = mtcars4)\nparameters(m)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n20.090625\n0.5400989\n0.95\n18.984282\n21.1969675\n37.198045\n28\n0.0000000\n\n\ndisp\n-2.334768\n1.2894201\n0.95\n-4.976025\n0.3064896\n-1.810711\n28\n0.0809290\n\n\nhp\n-1.006457\n1.0045056\n0.95\n-3.064094\n1.0511791\n-1.001943\n28\n0.3249519\n\n\ncyl\n-2.192076\n1.4238730\n0.95\n-5.108747\n0.7245958\n-1.539516\n28\n0.1349044"
  },
  {
    "objectID": "regression2.html#exercises",
    "href": "regression2.html#exercises",
    "title": "6  More lineare models",
    "section": "6.6 Exercises",
    "text": "6.6 Exercises\n\nPredictor relevance\nAdjusting\nAdjusting 2\nInterpreting Regression coefficients"
  },
  {
    "objectID": "regression2.html#lab",
    "href": "regression2.html#lab",
    "title": "6  More lineare models",
    "section": "6.7 Lab",
    "text": "6.7 Lab\nGet your own data, and build a simple model reflecting your research hypothesis based on the topics covered in this chapter. If you are lacking data (or hypothesis) get something close to it."
  },
  {
    "objectID": "regression2.html#glimpse-on-parameter-estimation",
    "href": "regression2.html#glimpse-on-parameter-estimation",
    "title": "6  More lineare models",
    "section": "6.8 Glimpse on parameter estimation",
    "text": "6.8 Glimpse on parameter estimation\nAn elegant yet simple explanation of the math of parameter estimation can be found at “go data driven”. A similar approach is presented here.\nConsider this geometric interpretation of the least square method in Figure Figure 6.8.\n\n\n\n\n\nFigure 6.8: Geometric interpretation of the least square method. Source: Oleg Alexandrov on Wikimedia"
  },
  {
    "objectID": "regression2.html#literatur",
    "href": "regression2.html#literatur",
    "title": "6  More lineare models",
    "section": "6.9 Literatur",
    "text": "6.9 Literatur\nA recent but already classic book on regression and inference (if this is possible) is the book by Gelman, Hill, and Vehtari (2021). A great textbook on statistical modelling (with a Bayesian flavor) was written by McElreath (2020); it’s suitable for PhD level.\nMathematical foundations can be found in Deisenroth, Faisal, and Ong (2020). Here’s a collection of online resources tapping into statistics and machine learning.\n\n\n\n\nDeisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020. Mathematics for Machine Learning. Cambridge ; New York, NY: Cambridge University Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge: Cambridge University Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. 2nd ed. CRC Texts in Statistical Science. Boca Raton: Taylor; Francis, CRC Press."
  },
  {
    "objectID": "causality.html#r-packages-needed-for-this-chapter",
    "href": "causality.html#r-packages-needed-for-this-chapter",
    "title": "7  Causality",
    "section": "7.1 R packages needed for this chapter",
    "text": "7.1 R packages needed for this chapter\n\nlibrary(tidyverse)\nlibrary(ggdag)  # optional"
  },
  {
    "objectID": "causality.html#intro-to-causality",
    "href": "causality.html#intro-to-causality",
    "title": "7  Causality",
    "section": "7.2 Intro to causality",
    "text": "7.2 Intro to causality\nCheck out this talk."
  },
  {
    "objectID": "causality.html#literature",
    "href": "causality.html#literature",
    "title": "7  Causality",
    "section": "7.3 Literature",
    "text": "7.3 Literature\nRohrer (2018) provides an accessible introduction to causal inference. Slightly more advanced is the introduction by one of the leading figures of the Field, Judea Pearl, Pearl, Glymour, and Jewell (2016). If your after a text book on modelling that covers causal inference, and if you like Bayesian statistics, than you should definitely check out McElreath (2020).\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. 2nd ed. CRC Texts in Statistical Science. Boca Raton: Taylor; Francis, CRC Press.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. Chichester, West Sussex: Wiley.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629."
  },
  {
    "objectID": "casestudies.html",
    "href": "casestudies.html",
    "title": "8  Case studies",
    "section": "",
    "text": "FALLSTUDIEN - NUR EXPLORATIVE DATENANALYSE\n\nDatenjudo mit Pinguinen\nData-Wranglinng-Aufgaben zur Lebenserwartung\nCase study: data vizualization on flight delays using tidyverse tools\nAufgabe zur Datenvisualisierung des Diamantenpreises\nFallstudie Flugverspätungen - EDA\nFallstudie zur EDA: Top-Gear\nFallstudie zur EDA: OECD-Wellbeing-Studie\nFallstudie zur EDA: Movie Rating\nFallstudie zur EDA: Women in Parliament\nFinde den Tag mit den meisten Flugverspätungen, Datensatz ‘nycflights13’\nCleaning and visualizing genomic data: a case study in tidy analysis\nTidyverse Case Study: Exploring the Billboard Charts\nAnalyse einiger RKI-Coronadaten: Eine reproduzierbare Fallstudie\nOpenCaseStudies - Health Expenditure\nOpen Case Studies: School Shootings in the United States - includes dashboards\nOpen Case Studies: Disparities in Youth Disconnection\nYACSDA Seitensprünge\nThe Open Case Study Search provides a nice collection of helpful case studies.\nifes@FOM Fallstudienseite"
  },
  {
    "objectID": "casestudies.html#case-studies-on-linear-modesl",
    "href": "casestudies.html#case-studies-on-linear-modesl",
    "title": "8  Case studies",
    "section": "8.2 Case studies on linear modesl",
    "text": "8.2 Case studies on linear modesl\n\nFALLSTUDIEN - NUR LINEARE MODELLE\n\nBeispiel für Prognosemodellierung 1, grundlegender Anspruch, Video\nBeispiel für Ihre Prognosemodellierung 2, mittlerer Anspruch\nBeispiel für Ihre Prognosemodellierung 3, hoher Anspruch\nFallstudie: Modellierung von Flugverspätungen\nModelling movie successes: linear regression\nMovies\nFallstudie Einfache lineare Regression in Base-R, Anfängerniveau, Kaggle-Competition TMDB\nFallstudie Sprit sparen\nFallstudie zum Beitrag verschiedener Werbeformate zum Umsatz; eine Fallstudie in Python, aber mit etwas Erfahrung wird man den Code einfach in R umsetzen können (wenn man nicht in Python schreiben will)\nPractical Linear Regression with R: A case study on diamond prices\nCase Study: Italian restaurants in NYC\nVorhersage-Modellierung des Preises von Diamanten\nModellierung Diamantenpreis 2"
  },
  {
    "objectID": "casestudies.html#case-studies-on-machine-learning-using-tidymodels",
    "href": "casestudies.html#case-studies-on-machine-learning-using-tidymodels",
    "title": "8  Case studies",
    "section": "8.3 Case studies on machine learning using tidymodels",
    "text": "8.3 Case studies on machine learning using tidymodels\n\nFALLSTUDIEN - MASCHINELLES LERNEN MIT TIDYMODELS\n\nExperimenting with machine learning in R with tidymodels and the Kaggle titanic dataset\nTutorial on tidymodels for Machine Learning\nClassification with Tidymodels, Workflows and Recipes\nA (mostly!) tidyverse tour of the Titanic\nPersonalised Medicine - EDA with tidy R\nTidy TitaRnic\nFallstudie Seegurken\nSehr einfache Fallstudie zur Modellierung einer Regression mit tidymodels\nFallstudie zur linearen Regression mit Tidymodels\nAnalyse zum Verlauf von Covid-Fällen\nFallstudie zur Modellierung einer logististischen Regression mit tidymodels\nFallstudie zu Vulkanausbrüchen\nFallstudie Himalaya\nFallstudien zu Studiengebühren\n1. Modell der Fallstudie Hotel Bookings\nAufgaben zur logistischen Regression, PDF\nFallstudie Oregon Schools\nFallstudie Windturbinen\nFallstudie Churn\nEinfache Durchführung eines Modellierung mit XGBoost\nFallstudie Oregon Schools\nFallstudie Churn\nFallstudie Ikea\nFallstudie Wasserquellen in Sierra Leone\nFallstudie Bäume in San Francisco\nFallstudie Vulkanausbrüche\nFallstudie Brettspiele mit XGBoost\nFallstudie Serie The Office\nFallstudie NBER Papers\nFallstudie Einfache lineare Regression mit Tidymodels, Kaggle-Competition TMDB\nFallstudie Einfaches Random-Forest-Modell mit Tidymodels, Kaggle-Competition TMDB\nFallstudie Workflow-Set mit Tidymodels, Kaggle-Competition TMDB\nFallstudie Titanic mit Tidymodels bei Kaggle\nEinfache Fallstudie mit Tidymodels bei Kaggle\nExploring the Star Wars “Prequel Renaissance” Using tidymodels and workflowsets"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Deisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. 2020.\nMathematics for Machine Learning. Cambridge ; New York,\nNY: Cambridge University Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and\nOther Stories. Analytical Methods for Social Research. Cambridge:\nCambridge University Press.\n\n\nGoodman, Steven. 2008. “A Dirty Dozen: Twelve p-Value\nMisconceptions.” Seminars in Hematology, Interpretation\nof quantitative research, 45 (3): 135–40. https://doi.org/10.1053/j.seminhematol.2008.04.003.\n\n\nHernán, Miguel A., John Hsu, and Brian Healy. 2019. “A Second\nChance to Get Causal Inference Right: A Classification of Data Science\nTasks.” Chance 32 (1): 42–49. https://doi.org/10.1080/09332480.2019.1579578.\n\n\nIsmay, Chester, and Albert Young-Sun Kim. 2020. Statistical\nInference via Data Science: A ModernDive into r and the\nTidyverse. Chapman & Hall/CRC the r Series. Boca\nRaton: CRC Press / Taylor & Francis Group. https://moderndive.com/.\n\n\nMacKay, R. J., and R. W. Oldford. 2000. “Scientific Method,\nStatistical Method and the Speed of Light.” Statistical\nScience 15 (3): 254–78. https://doi.org/10.1214/ss/1009212817.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in r and Stan. 2nd ed. CRC Texts in\nStatistical Science. Boca Raton: Taylor; Francis, CRC\nPress.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal\nInference in Statistics: A Primer. Chichester, West Sussex: Wiley.\n\n\nPoldrack, Russell. 2022. Statistical Thinking for the 21st\nCentury. https://statsthinking21.github.io/statsthinking21-core-site/index.html.\n\n\nRoback, Paul, and Julie Legler. 2021. Beyond Multiple Linear\nRegression: Applied Generalized Linear Models and Multilevel Models\nin. 1st ed. Chapman and Hall Texts in Statistical Science. Boca\nRaton: CRC Press.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and\nCausation: Graphical Causal Models for Observational Data.”\nAdvances in Methods and Practices in Psychological Science 1\n(1): 27–42. https://doi.org/10.1177/2515245917745629.\n\n\nSauer, Sebastian. 2019. Moderne Datenanalyse Mit r: Daten Einlesen,\nAufbereiten, Visualisieren Und Modellieren. 1. Auflage 2019.\nFOM-Edition. Wiesbaden: Springer. https://www.springer.com/de/book/9783658215866.\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The\nASA’s Statement on p-Values: Context, Process, and\nPurpose.” The American Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108.\n\n\nWasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. 2019.\n“Moving to a World Beyond ‘\np</i> p <\n0.05’.” The American Statistician 73 (March):\n1–19. https://doi.org/10.1080/00031305.2019.1583913.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science:\nVisualize, Model, Transform, Tidy, and Import Data. O’Reilly Media.\nhttps://r4ds.had.co.nz/index.html.\n\n\nWild, Chris J, and Maxine Pfannkuch. 1999. “Statistical Thinking\nin Empirical Enquiry.” International Statistical Review\n67 (3): 223–48."
  }
]